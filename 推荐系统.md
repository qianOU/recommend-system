# 组成

![image-20220310124847419](.\figs\f43.jpg)

# 札记

1. 较好的深度学习模型通常要求模型参数具有稀疏性也就是权重要倾向于0，是一个较小的值。

1. l1正则有助于稀疏性，l2正则有助于防止过拟合

1. 当训练数据正负样本不平衡时，比如我们经常会遇到正样本很少，负样本很多的情况，此时LogLoss会倾向于偏向负样本一方。 而AUC评估不会受很大影响，具体和AUC的计算原理有关.

1. 对于Tensorflow而已，神经网络的静态计算图一旦建成就会加载到内存中，不会重复构建计算图。（因此在利用函数式API构建神经网络模型的时候，层结构也可以定义在函数体内，虽然定义在函数体内是局部变量，但是图一旦构建完成，则计算流程已经全部加载到内存中，后续调用函数不会重复构建层结构）

   ```python
   def layer(inputs):
       DNN = layers.Dense(1, name='inner_layer') # 将DNN定义在函数体内
       return DNN(inputs)
   
   def main():
       inputs = keras.Input(shape=(4,), name='input_layer')
       outputs = layer(inputs)
       model = keras.Model(inputs=inputs, outputs=outputs)
       return model
   
   main().summary()
   
   # outputs
   Model: "model_1"
   _________________________________________________________________
    Layer (type)                Output Shape              Param #   
   =================================================================
    input_layer (InputLayer)    [(None, 4)]               0         
                                                                    
    inner_layer (Dense)         (None, 1)                 5         
                                                                    
   =================================================================
   Total params: 5
   Trainable params: 5
   Non-trainable params: 0
   _________________________________________________________________
   ```

4. add_loss相关坑点

   在许多的场景中，我们不仅仅只计算最终输出和目标的loss来更新整个网络的权重，也会在某层加入一些loss来更好的训练某层的表达（诸如：对某个层的参数使用正则化来防止过拟合——l2以及l1损失，或者是在DIEN中加入auxillary loss 来使得GRU序列能学习到用户的兴趣迁移情况）。

   ```python
   add_loss 可以在继承了 layers.Layer 或者 keras.Model 的子类中的 call 方法中实现。
   当实例化的自定义对象成为计算图的一部分的时候（可以通过model.summary()来查看自定义的层是否已经被添加入计算图中），在最终计算损失的时候会带上我们自定义的add_loss值。（注意：add_loss中的值应该是标量）
   ```

   1. 能够正常添加入自定义loss的案例

      ```python
      class mylayer(layers.Layer):
          def __init__(self):
              super().__init__()
              self.dnn = layers.Dense(1, name='inner_layer')
          
          def call(self, x):
              y = self.dnn(x)
              # 如果 mylayer 的子类对象被添加到计算图中则添加自定义的l1损失
              self.add_loss(lambda: tf.abs(tf.reduce_mean(self.dnn.kernel))) 
              return y
          
      def main():
          inputs = keras.Input(shape=(4,), name='input_layer')
          layer = mylayer() # 实例化 mylayer 对象
          outputs = layer(inputs) # 将自定义的 layer 加入计算图中
          model = keras.Model(inputs=inputs, outputs=outputs)
          return model
      
      main().summary()
      
      # outputs
      # 成功加入 mylayer对象到计算图中，自定义的l1损失生效
      Model: "model_6"
      _________________________________________________________________
       Layer (type)                Output Shape              Param #   
      =================================================================
       input_layer (InputLayer)    [(None, 4)]               0         
                                                                       
       mylayer_8 (mylayer)         (None, 1)                 5         
                                                                       
      =================================================================
      Total params: 5
      Trainable params: 5
      Non-trainable params: 0
      _________________________________________________________________
      ```

   2. 不能正常添加入自定义loss的案例

      ```python
      class mylayer(layers.Layer):
          def __init__(self):
              super().__init__()
              self.dnn = layers.Dense(1, name='inner_layer')
          
          def call(self, x):
              y = self.dnn(x)
              # 如果 mylayer 的子类对象被添加到计算图中则添加自定义的l1损失
              self.add_loss(lambda: tf.abs(tf.reduce_mean(self.dnn.kernel))) 
              return y
          
          def build_model(self): 
              """
              采用这种方式方式建立模型，由于没有使用到 mylayer 的子类对象，只是调用了其中的call方法， 
              mylayer的子类对象没有被加入到计算图中,所以最终 最终自定义的 add_loss 损失值没能成功加入到模型最终的损失中
              """
              inputs = keras.Input(shape=(4,), name='input_layer')
              model = keras.Model(inputs=inputs,
                                 outputs=self.call(inputs))
              return model
      
      mylayer().build_model().summary()
      
      # outputs
      # 自定义的myloss没有加入到计算图中，所以自定义的l1 loss 未能生效
      Model: "model_5"
      _________________________________________________________________
       Layer (type)                Output Shape              Param #   
      =================================================================
       input_layer (InputLayer)    [(None, 4)]               0         
                                                                       
       inner_layer (Dense)         (None, 1)                 5         
                                                                       
      =================================================================
      Total params: 5
      Trainable params: 5
      Non-trainable params: 0
      _________________________________________________________________
      ```

   3. 推荐系统中的几个改进方向:

      1. 不断尝试特征交叉组合的方式：PNN, NeuralFM, NFM, DeepFM，DCN
      2. 引入特征重要度的角度：AFM, DIN, DIEN（通过Attention机制），FIBINET（SENT和双线性交互方式）
      3.  采用用户行为序列：DIN, DIEN

      ![img](C:\Users\Admin\Desktop\学习\figs\f61.jpg)

   4. 特征交互的几种方式：

      1. PNN的内积和外积形式
      2. NFM的哈达玛积
      3. FIBINET的双线性交互

   5. 考虑特征重要性程度

      1. Attention
         1. 内积
         2. MLP结构（DIN 中的 local attention unit）
      2. SENET（来自CV的方法）

# Embedding

1. 在使用embedding技术的时候，如果需要使用pad_sequences进行填充（设置value=0）则embedidnng之前的label_encode操作应该从1开始编码，即将值为0的embedding向量作为单独无意义的一个向量，并且后续可以通过传递mask掩码数组，来忽略padding的值对loss的影响。
1. embedding的size一般采用个经验值，假如embedding对应的原始feature的取值数量为 n ，那么一般会采用 log2(n)或者 k * pow(n, 1/4)来做初始的size，其中 k <= 16。

具体案例可见DIN的实现部分（牵涉到不同用户具有不同长度的用户行为序列的处理）。

# 序列模型

### LSTM

![查看源图像](C:\Users\Admin\Desktop\学习\figs\f60.jpg)

### GRU

![](C:\Users\Admin\Desktop\学习\figs\f52.jpg)

### BILSTM

![img](C:\Users\Admin\Desktop\学习\figs\f90.jpg)

BILSTM 实质就是两个LSTM，一个LSTM正向处理序列，一个LSTM反向处理序列（trick: **将输入反转，输入LSTM中，实现中通过传入 go_backwards=True, 则LSTM自动会将输入序列进行反转，在输出的时候需要手动进行反转回转正确的顺序**），之后将得到的某个时刻的隐状态进行相加。

![](C:\Users\Admin\Desktop\学习\figs\f91.jpg)

灵活的实现BILSTM，可以指定融合形式，以及残差连接的层数。

```python
class BiLSTM(Layer):
    def __init__(self, units, layers=2, res_layers=0, dropout_rate=0.2, merge_mode='ave'):
        """
        units： 隐状态的维数
        layer: BILSTM 的层数
        res_layers: 指名最后的多少层使用残差连接，防止梯度消失
        dropout_rate: 在LSTM计算过程中使用dropout防止过拟合 
        merge_mode: 指名正向LSTM和反向LSTM输出的隐状态融合的方式
        """
        super(BiLSTM, self).__init__()
        self.units = units
        self.layers = layers
        self.res_layers = res_layers
        self.dropout_rate = dropout_rate
        self.merge_mode = merge_mode
    
    # 这里要构建正向的LSTM和反向的LSTM， 因为我们是要两者的计算结果最后加和，所以这里需要分别计算
    def build(self, input_shape):
        """
        input_shape: (None, sess_max_count, embed_dim)
        """
        self.fw_lstm = []
        self.bw_lstm = []
        for _ in range(self.layers):
            self.fw_lstm.append(
                LSTM(self.units, dropout=self.dropout_rate, bias_initializer='ones', return_sequences=True, unroll=True)
            )
            # go_backwards 如果为真，则反向处理输入序列并返回相反的序列
            # unroll 布尔(默认错误)。如果为真，则网络将展开，否则使用符号循环。展开可以提高RNN的速度，尽管它往往会占用更多的内存。展开只适用于较短的序列。
            self.bw_lstm.append(
                LSTM(self.units, dropout=self.dropout_rate, bias_initializer='ones', return_sequences=True, go_backwards=True, unroll=True)
            )
        super(BiLSTM, self).build(input_shape)
    
    def call(self, inputs):
        
        input_fw = inputs
        input_bw = inputs
        for i in range(self.layers):
            output_fw = self.fw_lstm[i](input_fw)
            output_bw = self.bw_lstm[i](input_bw)
            output_bw = Lambda(lambda x: K.reverse(x, 1), mask=lambda inputs, mask:mask)(output_bw) # 将反向计算的LSTM输出再次反转回正确的顺序，与fw_output可以直接结合
            
            if i >= self.layers - self.res_layers:
                output_fw += input_fw
                output_bw += input_bw
            
            input_fw = output_fw
            input_bw = output_bw
        
        if self.merge_mode == "fw":
            output = output_fw
        elif self.merge_mode == "bw":
            output = output_bw
        elif self.merge_mode == 'concat':
            output = K.concatenate([output_fw, output_bw])
        elif self.merge_mode == 'sum':
            output = output_fw + output_bw
        elif self.merge_mode == 'ave':
            output = (output_fw + output_bw) / 2
        elif self.merge_mode == 'mul':
            output = output_fw * output_bw
        elif self.merge_mode is None:
            output = [output_fw, output_bw]

        return output

```

这个的输入是`(None, seq_len, embed_dim)`， 输出是`(None, seq_len, hidden_units_num)`。

# 召回

### 向量召回

1. faiss 包

faiss工具包一般使用在推荐系统中的向量召回部分。在做向量召回的时候要么是u2u,u2i或者i2i，这里的u和i指的是user和item。**在推荐系统中**，使用两层for循环来计算海量用户/物品向量的相似度的时间复杂度是不可容量的，faiss使用了PCA和PQ(Product quantization乘积量化)两种技术进行向量压缩和编码，==faiss==就是用来加速计算某个查询向量最相似的topk个索引向量。

对于windows系统的支持不够

[faiss官方教程](https://github.com/facebookresearch/faiss/wiki/Getting-started)

```python
import numpy as np
d = 64                           # dimension
nb = 100000                      # database size
nq = 10000                       # nb of queries
np.random.seed(1234)             # make reproducible
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.


import faiss                   # make faiss available
index = faiss.IndexFlatL2(d)   # build the index， 使用内积衡量相似度
print(index.is_trained)
index.add(xb)                  # add vectors to the index
print(index.ntotal)


k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(xb[:5], k) # sanity check
print(I)
print(D)
D, I = index.search(xq, k)     # actual search
print(I[:5])                   # neighbors of the 5 first queries
print(I[-5:])                  # neighbors of the 5 last queries
```

2. annoy 包

   [spotify/annoy: Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk (github.com)](https://github.com/spotify/annoy)

# FM

### 总结：

1. FM就是使用两个特征的隐向量的内积作为交叉特征的权重，其中v_i 就是特征 x_i 的k维隐向量

1. FM支持连续特征与分类特征（分类特征一般使用one-hot处理，并且与连续特征进行concat成为新的特征向量）

1. 时间复杂度为O(kn)

1. FM的简化公式实际上是利用了对称矩阵的性质，取得对称矩阵上三角部分的和也就是等价于（全矩阵元素的和 - 主对角线元素的和）/ 2

### 推导公式：

第二种形式便于在自动求导框架中(tenosrflow)中实现，不需要自己计算负梯度方向

![img](.\figs\f1.jpg)

代码实现：

https://github.com/qianOU/recommend-tf2.0/blob/main/src/ctr/fm/model.py



# FFM

### 总结：

1. 对于每一个特征域会存取多组针对不同特征域的embedding向量

1. 通俗的来说，就是每个特征当与不同特征域进行特征交叉的时候应该各自选择适合另外一方特征域的embedding向量。举个例子来说，男生和年纪域以及男生与购物物品域的交叉应该是存在差异的，所以在男生这一特征下要存下与年纪域，购物物品域进行特征交叉的不同隐向量。

1. 时间复杂度为O(kn2)

1. FM与FFM都只具有二阶的特征交叉能力

### 迭代公式：

其中：f_j 表示的是 特征 j 所属的域

![img](.\figs\f2.jpg)

# GBDT + LR

### 总结：

1. 使用 GBDT 进行特征组合，树模型天生就是特征筛选和组合的模型，可以得到比FM特征组合更多的信息。

1. GBDT 得到的新特征呈现高维稀疏特性非常容易过拟合。

### 特征融合部分

![img](.\figs\F3.jpg)

### code

```Python
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier


gbm1 = GradientBoostingClassifier(n_estimators=50, random_state=10, subsample=0.6, max_depth=7,
                                  min_samples_split=900)
gbm1.fit(X_train, Y_train)
train_new_feature = gbm1.apply(X_train)
train_new_feature = train_new_feature.reshape(-1, 50)

enc = OneHotEncoder()

enc.fit(train_new_feature)

# # 每一个属性的最大取值数目
# print('每一个特征的最大取值数目:', enc.n_values_)
# print('所有特征的取值数目总和:', enc.n_values_.sum())

train_new_feature2 = np.array(enc.transform(train_new_feature).toarray())
```

# LS-PLM(MLR 混合逻辑回归)

### 总结：

假设特征维度为 d

1. 先对客户进行聚类为m个(注意这里不是真正的使用聚类算法产生m个群体，而是指)，阿里巴巴建议设置m为12

1. 之后在每个类别中学习一个 LR，也就是参数 w (在每一个类别中学习的参数数量为 d)

1. 对多个类别的回归结果进行加强融合（使用softmax），参数记为 u (在每一个类别中学习的参数数量为 d)

### 优点

1. 通过引入`结构化先验`，`我们使用用户特征来划分特征空间，使用广告特征来进行基分类器的训练，减小了模型的探索空间，收敛更容易`。同时，这也是符合我们认知的：不同的人群具有聚类特性，同一类人群具有类似的广告点击偏好。

### 过程

![img](.\figs\F4.jpg)

![img](.\figs\F5.jpg)

# NeuralCF

### 总结

1. 基于用户与物品的共现矩阵实现

1. GMF: 将用户的embedding与物品的embedding做点积（element wise），之后再通过一个只有一个输出单元的全连接层，选择sigmoid激活函数，表示预测的是点击率。

1. MLP 实际上是将 GMF 中的点积交互操作换成多层连接的深度结构，最后使用一个神经元作为点击率的预测输出。

1. 实质是 GMF + MLP 的混合形式，将MLP的输出与GMF的输出进行拼接之后再经过一个Dense层预测点击的概率。

1. 只是使用了用户和物品的交互信息，其他信息（物品信息，用户特征，上下文信息）都没有使用到。

### 结构

![img](.\figs\F6.jpg)

# PNN（乘积网络）

### 总结

1. https://blog.csdn.net/wuzhongqiang/article/details/108985457

1. 相比于NeuralCF引入了其他更多的特征且丰富了特征交叉的方式，让模型更容易捕获特征交叉信息

1. 无差别的特征交叉，使得模型难以获得原始特征信息。且在外积操作的时候，通过大量的简化使得原始特征中的有价值信息被遗失

1. 内积操作：就是将每个特征域的embedding向量首先进行两两内积计算得到n**2个标量，传入Prouduct层

1. 外积定义：假设a，b是n维列向量则 a与b的外积为 n*n 维矩阵(a * b.T)

1. 外积操作：就是将每个特征码的embedding向量进行两两外积计算得到n**2个矩阵（维度为m*m），为了简化复杂度通常会将n**2个矩阵做矩阵加法得到一个m*m维的矩阵传入Product层。

1. product层的输出通常由三部分通过相加组成： l_z + l_p + l_bias (三个输出具有相同的输出维度即product层的输出神经元个数)

1. l1 层会将 product层的输出与连续特征做拼接组成新的特征

### 结构

![img](.\figs\F7.jpg)

#  Wide & deep

### 总结

1. 泛化能力：挖掘出稀有特征与标签相关性的能力，技术主要有：隐向量，特征组合，特征交叉

1. 记忆能力：记忆原始特征与标签的强联系，给予更高的权重

1. 资料

1. 实现的时候有以下几个注意点
   1. 全量特征输入到deep部分
   2. deep部分的输出，需要最后整合到一个神经元的输出上（不能指定激活函数）
   3. wide部分的输出是一个只有一个神经元的dense层（不能指定激活函数）
   4. 将deep的输出与wide的输出相加传入sigmoid函数中作为ctr的预估（注意：这里如果使用Dense要将use_bias指定为False, 以及激活函数选为 sigmoid）

知乎博文：https://zhuanlan.zhihu.com/p/264499807?ivk_sa=1024320u CSDN博文：https://blog.csdn.net/wuzhongqiang/article/details/109254498

### 结构

![img](.\figs\F8.jpg)

### Wide部分

1. 这里的交互特征实质上就是第k个组合特征（可能包好多个特征）中所有特征的累乘值

1. 优化器选择的是FTRL，而不是SGD（在online learning的基础上）

1. 不足之处：`wide部分需要人工依据经验进行特征的筛选和组合`，过程繁琐

1. Wide部分也可以拓展为 FM（自动进行二阶特征交叉）

1. 一般会使用 l1 正则控制 wide 部分的参数的稀疏性，更专注于重要性大的特征

1. wide部分的特征输入是需要较大人工参与设计的

![img](.\figs\F9.jpg)

### Deep

1. 实质上是dnn结构

1. 一般会使用 l2 正则控制 deep 部分的参数的大小

1. deep 部分的输入是 稠密的embedding向量 + 原始的连续特征 的组合特征

# Deep & cross

### 总结

1. 是wide&deep的自然拓展，会使用Cross network 来代替 wide 部分在 O(n) 的时间复和空间杂度内完成特征的自动交叉组合，节约工程师进行特征组合的消耗。

1. 在cross network 完成后，将其输出与 deep 的输出进行concat拼接，之后传入逻辑回归中（单神经元的dense层），输出为 ctr 的预估。

CSDN博文：https://blog.csdn.net/wuzhongqiang/article/details/109254498

### 结构

![img](.\figs\F10.jpg)

### Deep

1. deep部分与 wide&deep 中保持一致

### Cross

![img](.\figs\F11.jpg)

![img](.\figs\F12.jpg)

#### 总结

1. 第 n 层的输出包含了所有的原始输入x0的（1—n+1）阶特征的交互，因此特征的交叉足够充分。

1. Cross网络的参数是共享的， 每一层的所有权重在特征交互之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。
   1. ![img](.\figs\F13.jpg)

1. 交叉网络的参数数量较少。 假设交叉层的数量是 L，特征的数目是 n，每一层要学习两个参数向量即 w 与 b （维度都为 n），所以总的 cross net 的参数大小为 $2*L*n$。

1. 交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有w和b， 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。

1. 从外积公式来看，每个层的输入都会与原始特征 x0 做外积操作，使得每个层的输出都不会偏离原始特征x0太远。

# DeepFM

### 总结

1. deep&wide的结构的自然拓展，使用FM结构来替代原始的wide部分中的Linear部分

1. FM 和 dnn 是共享输入特征的，即两者的输入都是稠密的embedding向量

1. FM中的一阶部分是dense_feature和embedding的线性组合（不带偏执项），二阶部分则只是对来自不同域的embedding向量做内积得到。（连续传统的FM结构，也就是将特征的embedding向量作为原FM中的特征隐向量做内积（将x_i，x_j 视为 1，可以继续套用FM的推导公式）

![img](.\figs\F14.jpg)

注：这里的weight-1 connection 也就是指的网络中连接的权重相等且都为1

### 结构  

![img](.\figs\F15.jpg)

### 实现细节

1. Addition 的输出是一个值，表示的是FM中的一阶部分，是将原始输入通过线性转化合并为一个数字（no bias, no activation）

1. FM中的二阶部分只针对sparse_feat_embedding进行，将embed视为原始特征的隐向量做内积得到二阶交互信息。二阶部分的交互信息的输出是该样本下其所有embed向量内积的和

1. deep部分最后的输出也是一个神经元的形式（no activation）

1. DeepFM的最终输出是将 FM的输出和 deep的输出相加，并且经过sigmoid的转换得到。

# XDeepFM

### 总结

1. XDeepFM 的提出也是为了==深入探索特征的高阶交叉==相关课题，实质与Deep&Cross 有着更深的联系。
1. FM中的特征交互是 vecotor wise 级别的（通过embedding向量的内积得到，即同一个embedding向量的不同维度之间不存在交叉），而在CrossNet中是 bit wise 级别（embedding向量中的维度称为bit，在 CrossNe t中由于输入是拼接的embedding向量，所以在特征组合过程中会存在来自同一个embedding的不同bit的信息交叉的现象）
1. bit wise 级别的特征交叉方式主要存在以下几个问题：
   1. 容易造成过拟合
   2. 不是常规的基于域的特征组合方式，失去特征组合的可解释性。
1. CrossNet 中的每一个隐藏层的输出都是原始输入x~0~的标量倍，因此CrossNet 的输出受到了特定形式的限制。

![image-20220306192428081](.\figs\f31.jpg)

5. CIN和DCN层的设计动机是相似的，Cross层的input也是前一层与输入层，这么做的原因就是可以实现: **有限高阶交互，自动叉乘和参数共享**。有限体现在高阶受层数的限制。

6. CIN 与 CrossNet 的不同点

![image-20220306202721652](C:\Users\Admin\Desktop\学习\figs\f39.jpg)

### 结构

![image-20220306192616287](.\figs\f32.jpg)

1. XDeepFM 由三部分组成： Linear， CIN(Compressed Interaction Network, 压缩交互网络)， DNN
2. 还是采用的Wide&Deep 的架构， Linear 强调模型的记忆性，CIN用于提取显性（具有可理解性的特征交互称为显性）的高阶特征，DNN 用于提取隐性的高阶特征，用于增强模型的泛化能力。

### CIN 结构

1. 只针对sparse feature 的 embedding 向量进行高阶的特征交叉。

#### 推导公式

![image-20220306193502697](.\figs\f34.jpg)

![image-20220306193813720](C:\Users\Admin\Desktop\学习\figs\f35.jpg)

下图展示了，计算CIN中第一个隐层的第 h 个特征的输出。每个隐层具h~k~权重矩阵$W_{h_{k-1},D}$, 其中 $h_k$ 是该层输出特征的个数, $h_{k-1}$ 是上一层输出的特征个数。

![image-20220306194545011](C:\Users\Admin\Desktop\学习\figs\f36.jpg)

1. 与PNN中内积层的计算类似，可以将W矩阵以及元素积矩阵Flatten来看待，实质就是对于每个特征embedding 的`元素积` 赋予不同的权重($W_{i,j}$)，再sum polling 为一个 D 维的 Embedding 向量，作为当前CIN层的第 h 个特征的输出。（参数为权重矩阵W）。（从CNN的角度来看，也就是在$H_k*m*D$的三维图片上做特征抽取转换，其中$H_k$ 为高，m 为宽， D 为 channel，使用$H_{k+1}$个filter，每个filter的维度为$W_{H_k},m$ ，在channel 维度进行逐个卷积操作得到H~k+1~ 个特征，每个的维度为 D）这也是CIN 中 C（compressed） 的由来，可视化参见结构图中 a, b 部分。
2. 从前向传播的公式来看，第 k 层的输出只包含 k+1 阶组合特征。所以整个CIN的输出如果想与 CrossNet 一致输出 1 to k+1 阶的特征，则需要将每一层的输出（h~k~个特征通过 sum polling得到一个h~k~维的向量）最后通过concat为一个dense输出。（有点RNN的味道）可视化部分见结构图中的c。

#### 结构图

![image-20220306201831878](C:\Users\Admin\Desktop\学习\figs\f33.jpg)

1. a, b 部分是类似CNN结构的提取信息
2. c 部分是类似RNN结构的综合各个隐藏层的输出（sum pooling）从而得到包含高阶特征交互的向量

以下展示了结构图中 c 的部分的计算逻辑。

![image-20220306202415932](C:\Users\Admin\Desktop\学习\figs\f37.jpg)

![image-20220306202457797](C:\Users\Admin\Desktop\学习\figs\f38.jpg)

### 时间复杂度分析

![image-20220306202920979](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20220306202920979.png)

### 实现细节

### 参考资料

[(22条消息) AI上推荐 之 xDeepFM模型(显隐性高阶特征交互的组合策略)_Miracle8070-CSDN博客_xdeepfm](https://blog.csdn.net/wuzhongqiang/article/details/116379857)

# NFM

### 总结

1. 是串行的设计神经网络（与DeepFM和FM的结合方式存在不同），使得神经网络和FM具有类似的运行风格，不过在NFM中会将二阶的特征交叉作用在dnn上，使得dnn能够从二阶特征交叉的基础上提取更多有价值的信息。

### 结构

![img](.\figs\F16.jpg)

![img](.\figs\F17.jpg)

`notes:` 这个结构中，忽略了一阶部分,只是可视化了从内积出发提取高阶特征信息的二阶部分。

### Bi-Interaction pooling 层

假设一共有n个特征域，这里是对每两个特征域的embedding做元素积得到 n*(n-1)/2 个embedding向量，再通过相加池化为一个embedding向量大小。而在原始的FM中，我们得到是n*(n-1)/2 个embedding的内积再通过加权融合为一个数目。（细究的话NFM中不同embedding维度的交互操作留在了DNN中实现，也就是说没有利用DNN提取信息的NFM是缺少embedding不同维度交互信息的）

![img](.\figs\F18.jpg)

### 实现细节

1. 在Input部分通过onehot之后得到embedding，与稀疏离散特征进行label encode 再获取embeding是一致的。

1. 特征交互层（Bi-interaction pooling）主要针对的是离散的类别特征进行的embedding元素积和pooling。对于连续型特征可以加入在Bi-Interaction pooling层的输出位置（concat拼接）。之后为了防止过拟合需要将拼接的向量经过BatchNormaliztion处理。

1. DNN将embeding的元素积通过多层网络的处理，最后需要经过一个线性层，得到一个输出值作为高阶信息。（no activation，no bias）

1. 和原始FM一样，对于一阶的特征使用Dense 层进行转换为一个输出。（no activation， bias是需要的）

1. 最后ctr的输出是 一阶信息 + 二阶信息 输出，经过 sigmoid 的转换得到。

# AFM

### 总结

1. 注意力机制在数学上只是将过去的平均操作或加和操作换成了`加权`和或者`加权`平均操作。

1. 在NFM的基础上加入了Attention，NFM在做特征交互层的时候将所有特征重要性视为一致，而这是不合理的，因此引入Attention层则可以有针对性的对于区分度高的交互特征赋予更高的权重。

1. 模型的输出也是由一阶部分和二阶高级特征交互部分组合，通过sigmoid得到。

1. 与NFM一致，embedding的元素积只在稀疏的列别特征中展开。

### 结构

![img](.\figs\F19.jpg)

注：图中结构表明 Attention Net 作用在 n*(n-1) / 2的embedding元素积之上，输出的元素也是 n*(n-1) / 2 个embedding 的各自权重。（实质是一个具有一个隐含层的MLP结构）

### Attention layer

![img](.\figs\F20.jpg)

### 细节

1. 所谓的Attention layer 实际上就是一个全连接层，将(n*(n-1)) / 2个embeding元素积的向量通过一个全连接层（最后的激活函数选择的是softmax）计算出每个元素积的权重。

1. Attention layer 之后也可以引入 dnn 增加模型提取高阶特征的能力，需要在进入 dnn 之前添加BN层防止过拟合，与NFM类似的这里需要连接原始的连续特征一起作为dnn的输入。

1. 在计算（n*(n-1)）/ 2 的embedding元素积的时候，原本需要一个两层的for循环进行遍历计算，在工程实现上存在trick。详细见代码

![img](.\figs\F21.jpg)

# DIN（Deep Interest Network）

### 总结

1. DIN模型对基于用户历史行为对用户的兴趣变化进行建模。

1. 这个模型的使用场景是**非常注重用户的历史行为特征（历史购买过的商品或者类别信息）**

### 结构

#### 传统MLP结构

![img](.\figs\F22.jpg)

通过上面的图也能看出来， 用户的历史行为特征和当前的候选广告特征在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，每个历史商品的信息会丢失了一部分，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。

改进思路：

1. 增加embeding维度，使得embedding向量能够蕴藏更多有关兴趣的信息，带来的计算负担是不可接受的。

1. 在用户历史行为中引入注意力机制，使得候选广告更关注于用户历史中的那些与当前候选广告产品更相关的历史记录上，这样在预测当前广告是否点击的时候，让模型更关注于与当前广告相关的那些用户历史产品，也就是说**与当前商品更加相关的历史行为更能促进用户的点击行为**。

#### DIN结构

引入了一个新的`local activation unit`， 这个东西用在了用户历史行为特征上面， **能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权****。**

与AFM的不同的是这里的注意力得分 weight 需要经过 softmax 进行归一化。

![img](.\figs\F23.jpg)

上图中传入local Attention unit 的输入还需要做元素积极或做差，之后传入MLP得到相似性度量。以下是Attention机制的数学表达形式：（实质就是对用户历史特征序列的embedding向量做加权sum池化操作）

![img](.\figs\F24.jpg)

### 训练trick

以下两个主要是对于大数据的训练难题，提出的工业上的训练tricks

#### mini batch aware regularization（小批量自适应正则化）

1. 由于模型训练中，大部分的参数都集中在embedding层，所以这种正则化的trick主要用在embedding层中。

1. 为了防止模型过拟合，我们一般会加入正则化， 而L2正则化加入的时候，是对于所有的参数都会起作用， 而像这种真实数据集中，每个mini-batch的样本特征是非常稀疏的，我们看之前那个样本编码的例子就能看到，这种情况下根本就没有必要考虑所有的参数进去，这个复杂度会非常大， 而仅仅约束那些在当前mini-batch样本中出现的特征(不为0的那些特征)embedding就可以。

1. 自适应正则化的思想：就是对系数的特征施加比较大的正则化力度，对于较为稠密的特征施加正则的力度要减弱。

1. 公式如下：

![img](.\figs\F25.jpg)

![img](.\figs\F26.jpg)

![img](.\figs\F27.jpg)

由于各个样本在每个不同的特征下取值为1的个数会不同，为了避免多次计算示信函数，因此可以通过实现统计取得每个特征的近似值。

![img](.\figs\F28.jpg)

注：上面的红箭头表示的就是施加了更严重的惩罚项的正则化，因为他们的特征值在batch里面只出现了一次。而其他两个则出现了两次，所以L2正则的系数更小。

#### Data Adaptive Activation Function

1. 解决MLP每一层的输入是不同的数据分布的解决方案。
   1. Batch-Normalization 标准化每一层的数据输入（均值为0，方差为1的正态分布）
   2. 自适应激活函数，使得激活函数能够根据数据的分布自动进行改变（尺度放缩以及左右移动）

![img](.\figs\F29.jpg)

Dice函数使得p(s)更类似一个概率的情况，同时引入了平滑曲线来代替硬分割。

![img](.\figs\F30.jpg)

notes: 这里的Var(s)是可以控制Sigmoid不饱和区域的大小的，Var(s)越大则不饱和区域越大，平滑的部分就越多。这样Dice函数就可以通过E(s)与Var(s)来自适应的调节激活函数。

### 细节

Dice自适用函数实质就是关闭了学习参数的BN层再经过sigmoid的作用。

# DIEN

### 总结

1. 适用的业务场景有以下几点：

   1. 非常注重用户的历史行为特征（历史购买过的商品或者类别信息）
   2. 用户兴趣多种多样，并且变化多端
   3. 对于推荐而言，捕获用户的兴趣点非常重要
   4. 用户的兴趣往往可以在其历史行为中进行学习

2. DIN 有以下两点不足

   1. ==直接将用户过去的历史行为当做了用户的兴趣，缺乏对具体行为背后的潜在兴趣进行专门的建模==。只计算了当前候选广告与过去历史行为的相关性，根据这个做出推荐， 但==历史行为之间的依赖关系并没有很好的模拟出来==，历史行为其实也是一个随时间排序的序列，既然是`时间相关的序列`，就一定存在或深或浅的前后依赖关系， 而这样的序列信息或者说依赖关系对推荐过程是非常有价值的(能够反映用户背后的潜在兴趣变化)，大量研究发现这种信息能够用于构建更丰富的用户模型并发现附加的行为模式，而DIN模型包括之前的MLP系列模型都无法学习到这样的序列依赖关系.

   2. ==DIN模型没法捕捉到用户的兴趣变化过程==，作者在论文中用到了一个词叫做”兴趣漂移", 即在相邻的访问中，用户的意图可能非常不同，用户的一个行为可能依赖于很久以前的行为。而一个用户对不同目标项的点击行为受到不同兴趣部分的影响，如果没法学习用户的兴趣演化，就很容易基于用户所有购买历史行为综合推荐，而不是针对“下一次购买”推荐， `DIN虽然是能够更加注重与当前候选物品相关的历史行为，但是这些行为并不能表示出用户的兴趣变化过程，所以序列信息是非常之重要的。`

      其实上面两点就说明了一个核心问题**DIN忽略了序列信息**，举个例子：

      ```te
      对一个综合电商来说，用户的兴趣迁移其实非常快，例如，上周一位用户在挑选一双篮球鞋， 这位用户上周的行为序列都会集中在篮球鞋这个品类的商品上， 但是完成购买之后，本周的购物兴趣可能变成买机械键盘。如果使用DIN则可能会更倾向于继续推荐鞋子，而DIEN对用户兴趣序列进行建模，则可能会根据兴趣之间的依赖性，推送机械键盘。
      ```

      序列信息的重要性就体现在：

      1. 加强了最近行为对下次行为预测的影响。 比如上面的例子， 用户近期购买机械键盘的概率会明显高于再买篮球鞋的概率。
      2. `序列模型能够学习到购买趋势的信息`。这个感觉就是在建模用户的兴趣演化， 在上面例子中， `序列模型能在一定程度上建立“篮球鞋”到“机械键盘”的转移概率，如果这个转移概率在全局统计意义上足够高， 那么用户购买篮球鞋时，推荐机械键盘也会是一个不错的选项`。 直观上，两者的用户群体很有可能一致。

### 结构

![img](.\figs\f50.jpg)

主要分为两部分：

1. 米黄色代表的兴趣提取层，使用了GRU序列模型对用户历史行为序列进行建模，提取出兴趣点。（PS: 工程上的trick是采用了auxiliary loss，表征t期的输出应该与第t+1期的输入计算兴趣的损失，即t期预测的兴趣反映在t+1期的点击上）

2. 粉红色代表的兴趣进化层，使用了Attention机制（AGRU单元）来提取与当前推荐产品相关的兴趣，输出到DNN中进行隐形高阶特征交叉。（PS：AGRU实质是指将当前兴趣embedding与灌高embeddin的相关性嵌入到GRU的更新单元中）

   ![image-20220312233231336](.\figs\f51.jpg)

#### GRU

GRU是RNN序列模型中的一种，可以解决RNN序列中的梯度消失问题，有着与LSTM媲美的性能，但是参数更少，训练速度更快。其中

![image-20220313150751415](C:\Users\Admin\Desktop\学习\figs\f52.jpg)

#### 兴趣提取层

![](C:\Users\Admin\Desktop\学习\figs\f53.jpg)

注意：如果使用内积来象征相关性的话，需要点击的物品embedding维度和提取出的兴趣状态embedding具有同样的维度。如果维度不一样的话，工程上会将

![](C:\Users\Admin\Desktop\学习\figs\f54.jpg)

`总结`:如果让所有的中间隐层状态的更新全部依赖了最后的那个交叉熵损失。 这时候其实是不合理的，就像作者说的， `目标商品的行为是由最后的兴趣触发， 但是前面的那些时间步或许和目标商品并没有太大的关系`，就像我上面举得那个兴趣在演化的例子，用户兴趣经过了一系列变化，这种情况显然用目标商品去更新前面时间步的中间隐藏状态就不合理了呀。所以作者在这里改进了这个损失的计算方式，因为我们都知道， 每个相邻的时间步之间的兴趣状态信息应该是非常相关的(与word2vec的思想解决。）所以这里作者提出了一个auxiliary loss， 让每个时间步都会有个输出， 而这里的label用的是下个时间步的输入x~t+1~，这样就是相当于用下个时间步的输入去监督训练当前时间步的隐藏状态h~t~ 。当然，这里具体的训练方式是采用了一种二分类的训练方式，也就是每一步的输出都在做一个二分类的任务。 除了使用真实的next行为作为正实例外，还使用从项目集采样的负实例(除了点击的项目)(PS: 采样的方式也是可以采用热门打压的策略进行）。

#### 兴趣进化层

##### 背景

​		在兴趣抽取层上面加上了兴趣进化层，就是为了==更有针对性的模拟与目标广告相关的兴趣进化路径==， 由于阿里巴巴这类综合电商的特点， 用户非常有可能同时购买多类商品，比如在购买“机械键盘”的同时还在查看“衣服”品类的商品，那么这时候注意力机制就显得格外重要。 而当目标广告是某个电子产品的时候，用户购买“机械键盘”相关的兴趣演化路径显然比购买“衣服”的演化路径重要。 这样的筛选功能在兴趣抽取层是没法做到的。

##### 注意力分数计算

注意力分数的计算与DIN类似，也是根据兴趣提取层在每个时间步的输出h~t~与当前候选广告之间的关联性大小。(PS: 由于W矩阵的存在，所以不要求兴趣embedding维度和广告embedding维度一致)
![image-20220313152508756](C:\Users\Admin\Desktop\学习\figs\f55.jpg)

几种逻辑实现：

1. AIGRU（GRU with attentional input）

   ![image-20220313153922587](C:\Users\Admin\Desktop\学习\figs\f59.jpg)

2. AGRU（Attention based GRU）

   ![image-20220313153609255](C:\Users\Admin\Desktop\学习\figs\f58.jpg)

3. AUGRU（GRU with attentional update gate）
   ![image-20220313153413532](C:\Users\Admin\Desktop\学习\figs\F57.jpg)

### 参考资料

实现主要参考以下几个部分
DynamicGRU 借鉴：
	[[阿里DIEN\] 深度兴趣进化网络源码分析 之 Keras版本 - 罗西的思考 - 博客园 (cnblogs.com)](https://www.cnblogs.com/rossiXYZ/p/14331924.html)
整体借鉴：
	[(11条消息) AI上推荐 之 DIEN模型(序列模型与推荐系统的花火碰撞)_翻滚的小@强的博客-CSDN博客_dien 数据集](https://zhongqiang.blog.csdn.net/article/details/109532438)

# DSIN

### 总结

1. 全称为Deep Session Interest Network(深度会话兴趣网络)。 这个模型依然是**研究如何更好的从用户的历史行为中捕捉到用户的动态兴趣演化规律**。而这个模型的改进动机呢？ 就是作者认为之前的序列模型，比如DIEN等，**忽视了序列的本质结构其实是由会话组成的**。

2. **用户历史行为序列本身的特点，是指在比较短的时间间隔内的商品往往会很相似，时间间隔长了之后，商品之间就会出现很大的差别**，而DIEN模型呢？ 它并没有考虑这个问题，而是会直接把这一大串行为序列放入GRU让它自己去学，由于往往用户的行为快速改变和突然终止的序列会有很多噪声点，这就导致不利于模型的学习。

3. 会话的划分：通过从序列本身的特点出发， 把一个用户的行为序列分成了多个会话，**所谓会话，其实就是按照时间间隔把序列分段，每一段的商品列表就是一个会话**，那这时候，会话里面每个商品之间的相似度就比较大了，而会话与会话之间商品相似度就可能比较小。

4. DSIN的整体逻辑

   1. **会话划分层**

      对于用户序列按照固定的时间间隔（论文中设置的是30min）进行分段，每一段里面的商品序列称为一个Session。PS：指的是如果两次行为之间的间隔如果大于等于30分钟则认为此处是Session的一个划分点。

   2. **会话兴趣提取层**

      由于用户序列已经被划分成多个Session了，现在需要学习每一个Session子序列的兴趣表示。当然我们说可以用GRU， 不过这里作者用了**多头的注意力机制**，这个东西是在**多个角度研究一个会话里面各个商品的关联关系**， 相比GRU来讲，没有啥梯度消失，并且可以并行计算，比GRU可强大多了。

   3. **会话交互层**

      上面研究了会话内各个商品之间的关联关系，接下来就是研究会话与会话之间的关系了，虽然我们说各个会话之间的关联性貌似不太大，但是可别忘了会话可是能够表示一段时间内用户兴趣的， 所以**研究会话与会话的关系其实就是在学习用户兴趣的演化规律，这里用了双向的LSTM，不仅看从现在到未来的兴趣演化，还能学习未来到现在的变化规律**。

   4. **会话兴趣局部激活层**

      既然会话内各个商品之间的关系学到了，会话与会话之间的关系也学到了，然后呢？ 当然也是**针对性的模拟与目标广告相关的兴趣进化路径了**， 所以后面是**会话兴趣局部激活层**， 这个就是**注意力机制， 每次关注与当前商品更相关的兴趣**。

   总的来说，DSIN思路与DIEN的思路是差不多的，无非是用了一些新的结构。

5. DSIN的核心创新点就是把用户的历史行为按照时间间隔进行切分，以会话为单位进行学习， 而学习的方式首先是会话之内的行为自学习，然后是会话之间的交互学习，最后是与当前候选商品相关的兴趣演进，总体上还是挺清晰的。

6. 适用背景是需要非常丰富的用户行为数据，并且用户行为数据能够充分反映用户兴趣的业务场景之中。

### 结构

![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f92.jpg)

#### Session Division Layer

这一层是将用户的行为序列进行切分，首先将用户的点击行为按照时间排序，判断两个行为之间的时间间隔，如果前后间隔大于30min，就进行切分(划一刀)， 当然30min不是定死的，具体跟着自己的业务场景来。
![img](C:\Users\Admin\Desktop\学习\figs\f93.jpg)

![image-20220320152948770](C:\Users\Admin\Desktop\学习\figs\f94.jpg)

#### Session Interest Extractor Layer

这个层是学习每个会话中各个行为之间的关系，之前也分析过，在同一个会话中的各个商品的相关性是非常大的。此外，作者这里还提到，`用户的随意的那种点击行为会偏离用户当前会话兴趣的表达`，所以为了捕获同一会话中行为之间的内在关系，同时降低这些不相关行为的影响，这里采用了multi-head self-attention。PS: 实质是Transformer block，唯一的区别是将 positional Encoding 做了改进，称为Bias Encoding。

下图给出了 transformer block 的结构图。

![img](C:\Users\Admin\Desktop\学习\figs\f95.jpg)
没有采用传统的transformer positional encoding 是因为在这里**还需要考虑各个会话之间的位置信息，毕竟这里是多个会话，并且各个会话之间也是有位置顺序的呀，所以还需要对每个会话添加一个Positional Encoding， 在DSIN中，这种对位置的处理，称为Bias Encoding。**

##### Bias Encoding

Bias Ebcoding 的结果 BE 最终会和 会话分割层的 Embedding 矩阵 Q 进行直接的 Add。

![image-20220320154303789](C:\Users\Admin\Desktop\学习\figs\f96.jpg)

##### self-Attention

经过Bias Encoding的处理之后，Q的维度为$[batch, K,T,d_{model}]$,首先我们先不考虑Batch维度，所以是$[K,T,d_{model}]$。传统的transformer block的输入为$[seq\_len, d_{model}]$ 。由此可知，DSIN的输入会多出一个会话维度K。接下来，就是**每个会话的序列**都通过Tranformer进行处理，即输入是 $[T, d_{model}]$。也就是说K个会话是共享一套Transformer block 参数的以及Bias encoding权重参数。
![image-20220320160136546](C:\Users\Admin\Desktop\学习\figs\f97.jpg)

会话兴趣提取层的输出是 K 个 $d_{model}$ 维度的embedding向量，表示的是K个会话的兴趣表示。考虑样本维度的话，输出维度是 $[batch, K, d_{model}]$ 。之后输出作为**会话交互层**的输入，对用户兴趣演化进行建模。

![image-20220320160906695](C:\Users\Admin\Desktop\学习\figs\f98.jpg)

两个注意点：

1. 这 K 个会话是走同一个Transformer网络的，也就是**在自注意力机制中不同的会话之间权重共享**
2. 最后得到的这个矩阵，**K 这个维度上是有时间先后关系的**，这为后面用LSTM学习这各个会话之间的兴趣向量奠定了基础。

#### Session interest interaction Layer

作者这里就是想通过一个双向的LSTM来学习下会话兴趣之间的关系， 从而增加用户兴趣的丰富度，或许还能学习到演化规律。
![](C:\Users\Admin\Desktop\学习\figs\f91.jpg)

#### Session interest Activation Layer

用户的会话兴趣与目标物品越相近，那么应该赋予更大的权重，这里依然使用注意力机制来刻画这种相关性，根据结构图也能看出**这里是用了两波注意力计算。一方面，相当于考察了候选物品与不同会话的相关性，另一方面，也考察了候选物品与整个兴趣演化过程的相关性。**

<img src="C:\Users\Admin\Desktop\学习\figs\f99.jpg" style="zoom:100%;" />

##### 会话兴趣提取层

![image-20220320162723478](C:\Users\Admin\Desktop\学习\figs\f100.jpg)

##### 会话兴趣交互层

![image-20220320163003822](C:\Users\Admin\Desktop\学习\figs\f101.jpg)

#### Output Layer

这个就很简单了，对上面的**用户特征， 物品特征以及求出的会话兴趣特征（有两个输出）进行拼接**，然后过一个DNN网络，就可以得到输出了。

![img](C:\Users\Admin\Desktop\学习\figs\f102.jpg)

![image-20220320163439759](C:\Users\Admin\Desktop\学习\figs\f103.jpg)

### 细节

### 参考资料

[DSIN模型(阿里DIEN之上的再探索，Transformer来了)](https://blog.csdn.net/wuzhongqiang/article/details/114500619?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164760889516782094846494%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=164760889516782094846494&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-114500619.nonecase&utm_term=DSIN&spm=1018.2226.3001.4450)





# YoutubeDNN

## 召回模块

![img](.\figs\f44.jpg)

==notes==:

1. embedding 向量是通过Word2Vec预训练得到的（包括：vedio，query，item），当然也可以通过加入embedding层来获得

2. 训练的时候是做的多分类任务，即预测下一个点击的视频的概率，但是一般全量视频维度很大，所以会采用负采样的技术，也就是说会采样 k 个负样本和正样本构成k个二分类任务之后计算loss进行反向传播。所谓的正样本是指**用户看完了的视频作为正样本**，具有时长限制。

3. 作者为了防止搜索信息对视频推荐带来的影响（即你知道用户搜索了什么就会偏向于推荐什么），因此会将搜索的序列给shuffle。（直接舍弃掉query的时序特性是不理智的，可以将query和item进行捆绑，让未来的query不出现在训练集中即可）

4. 作者为了打压高活跃客户可能会带偏模型的情况，因此对于每个用户选取的选取相同的样本数，保证用户在损失函数等权重。

5. example age：工程师发现视频推荐领域，许多用户倾向于看新视频（新视频不一定是与用户极其相关），原有模型框架中没有考虑对上述倾向的特征。**Example Age就是用训练时间的最后一刻减去样本日志产生的时间用来有效区分点击的时间影响，预测时置为0保证预测时处于训练的最后一刻，论文也说了还可以稍微负一些，虽然还没试过，但可以理解为处于训练窗口以后的时间去预测。**`example age 表征了用户的最新一次点击时间，距当前时刻进行推荐的时间间隔,属于用户特征`。

   example age 与 视频上传时间到用户点击的时间间隔(Video age)的关系

​                                       $$example \quad age\quad+ Video\quad age = Constant$$

6. expamle age 比 video age 作为特征有以下几点好处：
   1. 线上预测时example age是常数值， 所有item可以设置成统一的， 但如果是video age的话，这个根每个视频的上传时间有关， 那这样在计算用户向量的时候，就依赖每个候选item了。 而统一的这个好处就是用户向量只需要计算一次。
   2. example age的构造与视频相关的特征无关， 并且范围是训练阶段的时间间隔长度，便于归一化操作。
      

![image-20220311184050019](C:\Users\Admin\Desktop\学习\figs\f49.jpg)

### 负采样

1. 负采样技术主要是因为在多分类任务中，使用softmax的情况下计算十分耗时且复杂，不利于模型的参数更新迭代。因此，采用负采样技术能加快模型的收敛。

2. 主要有以下几种采样方式

   1. 随机负采样

   2. 热门打压

      主要是指将热门未点击的物品有更大的概率成为负样本，即带权重的随机采样

      ![img](.\figs\f45.jpg)



## 精排模块

![img](.\figs\f46.jpg)

==notes==

1. 对于连续特征的处理采用了百分位数的转换方式，即$x_{new} = Percent_{x_{old}}$ 取值为0-1，并且通过幂运算加入x^2, 以及 x^(.5)等非线性信息
2. 会获取离当前最近的n个看过的视频id，通过average polling 为watched video embedding
3. 特征工程上面主要加入了以下两个符合业务场景的新特征
   - **time since last watch**: 自上次观看同channel视频的时间，大多数用户都具有连续性
   - **previous imporessions**: 该视频已经被曝光给该用户的次数，避免无效的曝光，类似exploration 思维，对于无效曝光进行打压，使得倾向于推荐没被曝光的视频
4. 对于精排模型的训练的时候，使用的ylabel是是否点击，损失函数是加权交叉熵损失函数。而在模型serve端，主要是按$e^{WX+b}$其中的W和b都是训练的时候最后输出为1个神经元的Dense层的参数。值具有物理意义，表征的是一个待推荐视频的期望播放时长。

### 加权交叉熵损失函数

模型建模的预测对象不再是点击率，而是对视频的观看时间期望进行建模，可以避免推荐那些标题党的视频，有助于客户长期留存。

视频的观看时间的期望建模使用加权交叉熵函数实现，通过对正样本使用观看时间作为权重。

![image-20220310211818415](C:\Users\Admin\Desktop\学习\figs\f48.jpg)

![image-20220310211746244](C:\Users\Admin\Desktop\学习\figs\f47.jpg)

参考资料：

[(22条消息) AI上推荐 之 YouTubeDNN模型(工业界推荐系统的灯火阑珊)_Miracle8070-CSDN博客](https://zhongqiang.blog.csdn.net/article/details/122671511)

https://zhuanlan.zhihu.com/p/52504407?utm_source=wechat_session&utm_medium=social&utm_oi=645991503802011648

# FIBINET

### 总结

1. 背景：第一个是大部分模型没有考虑特征重要性，也就是交互完事之后，没考虑对于预测目标来讲谁更重要，一视同仁。 第二个是目前的两两特征交互，大部分依然是内积或者哈达玛积， 作者认为还不是细粒度(fine-grained way)交互。
2. FIBINET（Feature Importance and Bilinear feature Interaction）模型的着力点有以下两个：
   1. 特征的重要性，让更重要的特征有更大的权重，（SENET结构）
   2. 使用双线性特征交互方式（同时采用内积和哈达玛积）来进行特征交互，从而在更细粒度的刻画特征的交互。

### 结构

![img](C:\Users\Admin\Desktop\学习\figs\f62.jpg)

#### 前向传播过程

```pyth
梳理细节之前， 先说下前向传播的过程。

首先，我们输入的特征有离散和连续，对于连续的特征，输入完了之后，先不用管，等待后面拼起来进DNN即可，这里也没有刻意处理连续特征。

对于离散特征，过embedding转成低维稠密，一般模型的话，这样完了之后，就去考虑embedding之间交互了。 而这个模型不是， 在得到离散特征的embedding之后，分成了两路

一路保持原样， 继续往后做两两之间embedding交互，不过这里的交互方式，不是简单的内积或者哈达玛积，而是采用了非线性函数，这个后面会提到。
另一路，过一个SENET Layer， 过完了之后得到的输出是和原来embedding有着相同维度的，这个SENET的理解方式和Attention网络差不多，也是根据embedding的重要性不同计算出来个权重乘到了上面。 这样得到了SENET-like Embedding，就是加权之后的embedding。 这时候再往上两两双线性交互。
两路embedding都两两交互完事， Flatten展平，和连续特征拼在一块过DNN输出。
```

#### SENET

##### 背景

SENET本身是在CV领域中的应用，本质上是增对CNN中间的卷积核特征进行Attention操作，从而强化重要特征的表现来提高模型的准确率。

```html
SENet能否用到推荐系统？— 张俊林老师的知乎

推荐领域里面的特征有个特点，就是海量稀疏，意思是大量长尾特征是低频的，而这些低频特征，去学一个靠谱的Embedding是基本没希望的，但是你又不能把低频的特征全抛掉，因为有一些又是有效的。既然这样，如果我们把SENet的思想用在特征Embedding上，类似于做了个对特征的Attention，弱化那些不靠谱低频特征Embedding的负面影响，强化靠谱低频特征以及重要中高频特征的作用，从道理上是讲得通的
```

##### 结构

把SENet放在Embedding层之上，通过SENet网络，`动态`(动态性体现在对于不同的输入得到的特征权重参数是不同的)地学习这些特征的重要性。**对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的**。
	下图展示了一个样本在SENET的计算逻辑，总的来说分为三部分，Squeeze，excitation阶段以及re-weight阶段。 

![](C:\Users\Admin\Desktop\学习\figs\f63.jpg)
$$
假设有f个特征域，e_i表示的是特征i的embedding向量。
$$

###### Squeeze 阶段

进行embedding向量的特征压缩为一个标量

![image-20220317213443854](C:\Users\Admin\Desktop\学习\figs\f64.jpg)

###### Excitation阶段

通过一个两层的MLP自编码得到特征权重向量。

![image-20220317214341319](C:\Users\Admin\Desktop\学习\figs\f65.jpg)

整个excitation阶段从压缩的特征向量到得到同纬度的特征权重向量，结构上和自动编码器十分相像。下图展示的是自编码的结构 +1 神经单元 可以理解为bias项。`而在SENet的两层MLP结构中主要学习的是特征交互，所以不会加入偏置项。`

![img](C:\Users\Admin\Desktop\学习\figs\f66.jpg)

分析一下维度的变化：SENet的输入是E（稀疏特征的embedding输入），维度是（None, f, k）其中 f 是特征域的个数，k 是embedding的维度。之后通过Squeeze阶段，得到了（None, f）的矩阵，这就是Excitation两层MLP结构的输入了，$W_1\in{R^{f*\frac{f}{r}}},W_2\in{R^{\frac{f}{r}*f}}$,`这里 r 被称为 reduction ratio， $\frac{f}{r}$ 就是中间层神经元的个数，==r 表示了压缩的程度==（有经验值的设定方式）,r 越大信息被压缩的约厉害。

###### Re-Weight

![image-20220317220556737](C:\Users\Admin\Desktop\学习\figs\f67.jpg)

这样，就可以将SENet引入推荐系统，用来对特征重要性进行动态判断。（对于以Attention为主的特征重要性实现的一种全新的补充）

#### Bilinear-Interaction Layer

![image-20220317221132279](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20220317221132279.png)

无论是FM，PNN所采用的内积，还是NFM和AFM所采用的哈达玛积来进行特征交互都存在特征交互粒度太粗的问题，因此通过模型学习连通内积和哈达玛积的权重参数W的表达可以得到更细粒度的特征交互向量。这里的$v_i, v_j$分别代表是不同特征域下的embedding向量维度为(1, k), 则 W 的维度为 (k, k) 。计算逻辑是先让$v_i$与W进行内积，得到（1, k）的向量，体会一下这个新向量，**新向量的每个元素，相当于是原向量 $v_i$ 在每个维度上的线性组合，权重参数可以通过学习得到**，之后再与$v_j$ 进行哈达玛积。

为什么双线性被称为更细粒度的特征交互？

1. 向量内积，其实是相当于向量的各个维度先对应位置元素相乘再相加求和。这个过程中认为的是向量的各个维度信息的重要性是一致的。但这个结论往往是不成立的。**内积操作没有考虑向量各个维度的重要性**
2. 如果我们单独看哈达玛积操作， 特征交互如果是两个向量哈达玛积，这时候，是各个维度对应位置元素相乘得到一个向量， 而这个向量往往后面会进行线性或者非线性交叉的操作， 最后可能也会得到具体某个数值，但是这里经过了线性或者非线性交叉操作之后， 有没有感觉把向量各个维度信息的重要性考虑了进来？ 就类似于 $w_1v_{i1}v_{j1}+w_2v_{i2}v_{j2}+...$ 。 **如果模型认为重要性相同，那么哈达玛积还有希望退化成内积，所以哈达玛积感觉考虑的比内积就多了一些**。 — **哈达玛积操作自身也没有考虑各个维度重要性，但通过后面的线性或者非线性操作，有一定的维度重要性在里面**
3. 再看看这个双线性， 是先内积再哈达玛积。这个内积操作不是直接$v_i和v_j$内积，而是中间引入了个W矩阵，参数可学习。 那么 $v_i$ 和 W 做内积之后，虽然得到了同样大小的向量，但是这个向量是$v_i$各个维度元素的线性组合，相当于变成了$[w_{11}v_{i1}+...+w_{1k}v_{ik},...,w_{k1}v_{i1}+...+w_{kk}v_{ik}]$，紧接着再与$v_j$进行哈达玛积，就变成了$[(w_{11}v_{i1}+...+w_{1k}v_{ik})*v_{j1},...,(w_{k1}v_{i1}+...+w_{kk}v_{ik})*v_{jk}]$， **这时候，就可以看到，如果这里的W是个酷似单位矩阵的对角矩阵，那么这里就退化成了哈达玛积。 所以双线性感觉考虑的又比哈达玛积多了一些**。如果后面再走一个非线性操作的话，就会发现这里同时考虑了两个交互向量各个维度上的重要性。—**双线性操作同时可以考虑交互的向量各自的各个维度上的重要性信息（）， 这应该是作者所说的细粒度，各个维度上的重要性**

**当然思路是思路，双线性并不一定见得一定比哈达玛积有效， SENET也不一定就会比原始embedding要好，一定要辩证看问题**

##### Bilinear 中的W trick

借鉴FM和FFM的递推演进关系，这里的W设计了三种选择方式。也就意味着有三种类型的双线性交互方式。

$\cdot\;表示的是内积，\ \odot表示的是哈达玛积$

1. Filed-All Type
   $$
   p_{ij}=v_i\cdot{W}\odot{v_j}
   $$
   也就是所有的特征embedding共用一个W矩阵，这也是FIled-All的名字的来源。 $W\in{R^{k\times{k}}},\; and\; v_i，v_j\in{R^{k}}$,这种方式最简单。

2. Fileld-Each Type
   $$
   p_{ij} = v_i\cdot{W_i}\odot{v_j}
   $$
   类似于FM的思想了，每个特征域都有一个自己的W矩阵，如果有 f 个特征的话，就会有个 W 矩阵。但是在实际进行编程实现的时候可以减少一个W矩阵的参数量，所以总的参数个数为 $f-1\times{k}\times{k}$个参数，这里的$f-1$是因为两两组合之后首部元素的类型只会有$f-1$个，比如[0, 1, 2]，两两组合之后的所有情况是[0, 1]，[0, 2]，[1, 2] 用到的域只有 0 和 1。

3. Filed-interaction Type
   $$
   p_ij = v_i\cdot{W_{ij}}\odot{v_j}
   $$
   类似于FFM的思想，每组特征交互的时候，用一个W矩阵， 那么这里如果有 f 个特征的话，需要$W_{ij}$是$\frac{f\times{(f-1)}}{2}$个。参数个数是$\frac{f\times{(f-1)}}{2}\times{k}\times{k}$个。

​	我们的原始embedding和SENET-like embedding都需要过这个双线性特征交互层，那么分别得到的就是一个双线性两两组合的矩阵， 维度是$(\frac{f\times{(f-1)}}{2}, k)$的矩阵。

#### Combination Layer

![image-20220317235718954](C:\Users\Admin\Desktop\学习\figs\f69.jpg)

### 细节&经验

#### 论文相关实现细节

1. 超参数选择，主要是embedding维度以及DNN层数， embedding维度这个10-50， 不同的数据集可能表现不一样， 但尽量不要超过50了。否则在DNN之前的特征维度会很大。
2. DNN层数，作者这里建议3层，而每一层神经单元个数，也是没有定数了。
3. reduction ratio（SENET中的压缩比例）可以设置为3。
4. FIBINET主要适合用在多数高维稀疏特征的场景下，进行数据挖掘。
5. **使用itertools.combinations 来得到两两组合的各种情况比较优雅，避免了写for循环。特征组合的实现值得学习**

#### 对于实际应用的一些经验

1. **SE-FM 在实验数据效果略高于 FFM，优于FM，对于模型处于低阶的团队，升级FM、SE-FM成本比较低**

2. deepSE-FM 效果优于DCN、XDeepFM 这类模型，相当于XDeepFM这种难上线的模型来说，很值得尝试，不过大概率怀疑是**增加特征交叉的效果，特征改进比模型改进work起来更稳**

3. 实验中增加embeding 长度费力不讨好，效果增加不明显，如果只是增加长度不改变玩法边际效应递减，**不增加长度增加emmbedding 交叉方式类似模型的ensemble更容易有效果**

#### 工业应用上的经验

* 适用的数据集
  虽然模型是针对点击率预测的场景提出的，但可以尝试的数据场景也不少，**比较适合包含大量categorical feature且这些feature cardinality本身很高，或者因为encode method导致的某些feature维度很高且稀疏的情况**。推荐系统的场景因为大量的user/item属性都是符合这些要求的，所以效果格外好，但我们也可以举一反三把它推广到其他相似场景。另外，文字描述类的特征（比如人工标注的主观评价，名字，地址信息……）可以用tokenizer处理成int sequence/matrix作为embedding feature喂进模型，丰富的interaction方法可以很好的学习到这些样本中这些特征的相似之处并挖掘出一些潜在的关系。

* 回归和分类问题都可以做，无非改一下DNN最后一层的activation函数和objective，没有太大的差别。

* 如果dense feature比较多而且是分布存在很多异常值的numeric feature，尽量就不要用FiBiNET了，相比大部分NN没有优势不说，SENET那里的一个最大池化极其容易把特征权重带偏，如果一定要上，可能需要修改一下池化的方法。

* DeepCTR的实现还把指定的linear feature作为类似于WDL中的wide部分直接输入到DNN的最后一层，以及DNN部分也吸收了一部分指定的dnn feature中的dense feature直接作为输入。毫无疑问，DeepCTR作者在尽可能的保留更多的特征作为输入防止信息的丢失。

* 使用Field-Each方式能够达到最好的预测准确率，而且相比默认的Field-Interaction，参数也减少了不少，训练效率更高。当然，三种方式在准确率方面差异不是非常巨大。

* reduce ratio设置到8效果最好，这方面我的经验和不少人达成了共识，SENET用于其他学习任务也可以得到相似的结论。 – 这个试了下，确实有效果

* 使用dropout方法扔掉hidden layer里的部分unit效果会更好，系数大约在0.3时最好，原文用的是0.5,请根据具体使用的网络结构和数据集特点自己调整。-- 这个有效果

* 在双线性部分引入Layer Norm效果可能会更好些

* 尝试在DNN部分使用残差防止DNN效果过差

* 直接取出Bilinear的输出结果然后上XGBoost，也就是说不用它来训练而是作为一种特征embedding操作去使用， 这个方法可能发生leak

* 在WDL上的调优经验： 适当调整DNN hideen layer之间的unit数量的减小比例，防止梯度爆炸/消失。

### 参考资料

[FiBiNET: paper reading + 实践调优经验](https://zhuanlan.zhihu.com/p/79659557)
https://zhongqiang.blog.csdn.net/article/details/118439590（主要）

# AutoInt

### 总结

1. 全称 **Automatic Feature Interaction** Learning via Self-Attentive Neural Networks，重点也是在特征交互上。主要是将 Transformer结构引入到模型结构中，通过多头的自注意力机制来显示的构造高阶特征，有效的提升了模型的效果。

### 结构

### 细节

### 参考资料

# Transformer

### 总结

1. **提出了一种Transformer的结构，这种结构呢，是完全依赖==注意力机制==来刻画输入和输出之间的全局依赖关系，而不使用递归运算的RNN网络了。这样的好处就是第一可以有效的防止RNN存在的梯度消失的问题，第二是允许所有的字全部同时训练（RNN的训练是迭代的，一个接一个的来，当前这个字过完，才可以进下一个字），即训练并行，大大加快了计算效率。**

2. Transformer使用了**位置嵌入来理解语言的顺序，使用了多头注意力机制和全连接层等进行计算，还有跳越机制，LayerNorm机制，Encoder-Decoder架构**

3. transformer的优缺点

   ![image-20220319175835413](C:\Users\Admin\Desktop\学习\figs\f89.jpg)

### 结构

![image-20220319145449208](C:\Users\Admin\Desktop\学习\figs\f71.jpg)

#### positional encoding

由于transformer模型没有循环神经网络的迭代操作, 所以我们必须提供每个字的位置信息给transformer, 才能识别出语言中的顺序关系。
现在定义一个位置嵌入的概念，也就是现在定义一个位置嵌入的概念, 也就是𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔, 位置嵌入的维度为[𝑚𝑎𝑥 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛𝑔𝑡ℎ, 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛], **嵌入的维度同词向量的维度, 𝑚𝑎𝑥 𝑠𝑒𝑞𝑢𝑒𝑛𝑐𝑒 𝑙𝑒𝑛𝑔𝑡ℎ属于超参数, 指的是限定的最大单个句长.**
![image-20220319151004821](C:\Users\Admin\Desktop\学习\figs\f73.jpg)
用 sin 与 cos 进行处理得到位置编码信息主要有以下几个优点：

* 随着embedding dimension维度的上升，三角函数的周期约来越大，函数周期变化越来越慢，从而产生一种包含位置信息的纹理。

  ![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f74.jpg)

* 位置嵌入函数的周期从$2\pi$到$10000 * 2 \pi$变化, 而每一个位置在embedding dimension 维度上都会得到不同周期的 sin 和 cos 函数的取值组合, 从而产生独一的纹理位置信息, 模型从而学到位置之间的依赖关系和自然语言的时序特性。

* 在NLP场景下，除了单词的绝对位置， 单词的相对位置也非常重要。由三角函数定义可知$sin(a+b) = sin(a)cos(b) +cos(a)sin(b),\ cos(a+b)=cos(a)cos(b)-sin(a)sin(b)$

  所以可知离a字符 b 个单位的字符位置信息，可以由 a 字符的位置embedding通过线性组合的方式得到,为模型捕捉单词之间的相对位置关系提供了非常大的便利。

将得到的位置编码和原始字符的embedding能直接 Add作为新的Embedding，是因为不同位置得到的位置编码是不同的，而原始字符不同其embedding也是不同的，所以直接 Add 也能够区分开词语和位置信息。（PS：在具体实现的时候，可以通过赋予不同的权重来增大字符和位置embedding之间的区别，因为往往字符所在的位置对语义影响较大。transformer实现的时候会将position的embedding * sqrt(embed_dims) 再与字符编码 Add）

#### Layer Normalization

 Layer Normlization的作用是把每一个样本输入的所有特征维度归一化为标准正态分布，起到加快训练速度，加速收敛的作用。类似于Batch Normlization，但是与Batch Normlization不同的是前者是以样本为单位（每一样本减去每一样本的均值然后除以每一样本的标准差），后者是一个Batch的特征为单位（每一个特征减去Batch下特征的均值然后除以Batch下特征的标准差）。来个图感受下就是这样：
![image-20220319163323503](C:\Users\Admin\Desktop\学习\figs\f81.jpg)

使用LN而不使用BN是因为在NLP中，序列长度大多都是不定长的，所以在很多维度下会填充0，这就导致BN在特征维度下的操作不具有优势。

#### Multi-Head Attetion

多头注意力机制，是指多个自注意力机制，不同的头反映了在不同的角度下字符和全局字符串序列的关系。
**每个字经过多头注意力机制之后会得到一个R矩阵，这个R矩阵表示这个字与其他字在N个角度上（比如指代，状态…）的一个关联信息，这个角度就是用多个头的注意力矩阵体现的。这就是每个字多重语义的含义**。

![image-20220319154625132](C:\Users\Admin\Desktop\学习\figs\f75.jpg)

计算过程：

![img](C:\Users\Admin\Desktop\学习\figs\f76.jpg)
PS: 其中计算公式里面的$d_k = d_{model}/n_{head}$，也就是每个 K~i~ 矩阵的维度，其中 i 表示的是头的index。在实际处理中通常不会有显示的分头操作，一般都是以多个头共存的大矩阵来表示Q,K,V。所以这里 $d_k = d_{model}=embed\_dims$

Q，K，V这三个矩阵分别是什么意思， **Q表示Query，K表示Key，V表示Value。之所以引入了这三个矩阵，是借鉴了搜索查询的思想**，比如我们有一些信息是键值对（key->value）的形式存到了数据库，（5G->华为，4G->诺基亚）, 比如我们输入的Query是5G， 那么去搜索的时候，会对比一下Query和Key, 把与Query最相似的那个Key对应的值返回给我们。 这里是同样的思想，我们最后想要的Attention，就是V的一个线性组合，只不过根据Q和K的相似性加了一个权重并softmax了一下而已， 这里比较巧妙的是Q,K,V都是这个Xembedding_pos经过线性变化得到的而已。
上面图中有8个head， 我们这里拿一个head来看一下做了什么事情：（请注意这里**head的个数一定要能够被embedding dimension整除**才可以，上面的embedding dimension是512， head个数是8，那么每一个head的维度是(4, 512/8）。
![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f77.jpg)

其后计算Q和K的相识度的时候，采用点积相似度计算的方式进行。**两个向量越相似， 他们的点积就越大，反之就越小**。

![image-20220319160519329](C:\Users\Admin\Desktop\学习\figs\f79.jpg)

**方差越大也就说明，点积的数量级越大（以越大的概率取大值）**。当QK^T^中有的元素非常大的时候，会对softmax的性能造成很大的影响。**在数量级较大时，softmax将几乎全部的概率分布都分配给了最大值对应的标签**。

<img src="C:\Users\Admin\Desktop\学习\figs\f80.jpg" alt="img" style="text-align: center;zoom:35%" />
								  上图是softmax的函数图像

​	从这个图像中可以看出，softmax 在输入值很大的时候，会发现导数几乎为0. 因此会导致模型训练的时候出现梯度消失的情况。使用 $\sqrt{d_{k}}$ 进行注意力得分的 scale 操作可以使得标准差重新回归到 1。也就使得softmax没有落到梯度饱和区。解决了梯度消失的问题。**softmax计算attention权重的时候，需要使用mask除掉padding的影响，主要是通过将padding为0的位置设置为一个很大的负数，这样在经过softmax的时候权重就为0。**之后将得到的每个字之间的注意力分数和 V 进行加权融合，从而使每个字向量都含有当前句子的所有字向量的信息。这样就得到了新的X_attention(这个X_attention中每一个字都含有其他字的信息)。之后与原始输入进行**残差连接，训练的时候可以使得梯度直接走捷径反传到最初层，不易消失。另外，用残差还有个好处就是能够保留原始的一些信息。**

整个流程如下图所示：

![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f82.jpg)
这里多个头直接拼接的操作， 相当于默认了每个头或者说每个子空间的重要性是一样的， 在每个子空间里面学习到的相似性的重要度是一样的

#### Feed Forward

Feed Forward 实质就是一个两层的MLP结构，主要作用就是对多头得到的信息进行再一次的交叉融合，得到更深维度的表示。

![image-20220319164932403](C:\Users\Admin\Desktop\学习\figs\f83.jpg)

#### encoder模块

由 N 个 encoder-transformer block 组成 

![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f72.jpg)

计算逻辑：

1. 首先输入进来之后，经过Input Embedding层每个字进行embedding编码，然后再编入位置信息（position Encoding），形成带有位置信息的embedding编码。

2. 然后进入多头（**多头可以理解为对句子的多个角度考察**）注意力部分，这部分是**多角度**的self-attention部分，在里面每个字的信息会依据权重进行交换融合，这样每一个字会带上其他字的信息（信息多少依据权重决定），然后进入**feed-forward部分（融合多个角度的信息）**进行进一步的计算，最后就会得到输入句子的数学表示了。

   

##### transformer block

一个完整的transformer block 的运算结构如下，最终的输出是和输入同维度的，所以可以stack多个blocks。

![在这里插入图片描述](C:\Users\Admin\Desktop\学习\figs\f84.jpg)
在Attention阶段需要先经过mask处理之后，再使用softmax对权重归一化，避免padding项对模型结果的干扰。

#### decoder

编码器通过处理输入序列开启工作。**顶端编码器的输出之后会经过线性变化 WK 与 WV 转化为一个包含向量K（键向量）和V（值向量）的注意力向量集**。这些向量将被每个解码器用于自身的**“编码-解码注意力层”**。

在完成编码阶段后，则开始解码阶段。解码阶段的从一个**特殊的起始符号开始**，每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素（先输出为，为落下去，输出什， 什落下去输出么）。接下来的步骤重复了这个过程，直到到达一个**特殊的终止符号**，它表示transformer的解码器已经完成了它的输出。**每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。**

**而解码器中的自注意力层表现的模式与编码器不同主要体现在增加了look_ahead mask操作**。**在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。**在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）, 这里才是正规的mask操作， 这个mask操作的目的是不能让当前位置的词看到它之后的，只能看到它以及它之前的。这是符合直觉的，在翻译一个句子的时候，我们只能知道该单词以前已经翻译了的信息，对于还没翻译的信息是位置的，所以需要mask掉。

```python
举个例子：
假设最大允许的序列长度为10， 先令padding mask为: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]， 然后再假设当前句子一共五个单词， 在输入第三个单词的时候， 前面有一个开始标识和两个单词， 那么此刻的sequence mask为[1, 1, 1, 0, 0]， 然后两个mask相加，得[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]。

```

```python
那么这个sequence mask到底是为什么要做呢？ 难道我不mask了， 我就得不出结果了？ 其实不mask是可以得出结果的，但是仅仅是在训练集上。 什么意思？ 这里如果不用sequence mask的话，相当于我预测当前输出的时候， 是结合了所有的信息，也就是看到了后面的词语信息。 比如 我/爱/中国/共产党. 在输出爱的时候，如果不mask，会用到后面中国共产党的信息。 But， 我们做预测的时候， 预测当前输出，是看不多后面句子的呀， 毕竟后面句子还没有出来呢？ 所以如果训练的时候不mask，就会导致训练和预测出现了一个gap，所以这个sequence mask是必须要的。
```

计算逻辑如下：

![img](C:\Users\Admin\Desktop\学习\figs\f85.jpg)

### Linear & Softmax

![image-20220319174544690](C:\Users\Admin\Desktop\学习\figs\f86.jpg)

![img](C:\Users\Admin\Desktop\学习\figs\f87.jpg)

### 细节 & 技巧

#### Label Smothing（regularization）

将原理的onehot编码的label，变为平滑处理的label。这个的意思就是我们准备我们的真实标签的时候，**最好也不要完全标成非0即1的这种情况，而是用一种概率的方式标记我们的答案**。这是一种规范化的方式。

![img](C:\Users\Admin\Desktop\学习\figs\f88.jpg)

```python
label1 = [0, 0, 1, 0, 0] # 原来的label标记方式使用one-hot来标记
 label_smothing = [0.02  0.02   0.9  0.02  0.02   0.02] # 使用概率的方式进行标记
```

#### Noam learning Rate Schedule

这是一种非常重要的方式，如果不用这种学习率的话，可能训练不出一个好的Transformer。
![img](C:\Users\Admin\Desktop\学习\figs\f70.jpg)

简单的说，**就是先让学习率线性增长到某个最大的值，然后再按指数的方式衰减**。相当于先有一个热启动阶段，在这个阶段呢？ 先让学习率线性增大， 到一定的程度， 再开启冷启阶段，或者叫学习率衰减的方式。 这个方式对于训练Transformer来说，是一个比较重要的策略。

### 参考资料

[Attention is all you need](https://blog.csdn.net/wuzhongqiang/article/details/104414239?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161512240816780357259240%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=161512240816780357259240&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-1-104414239.pc_v1_rank_blog_v1&utm_term=Attention+is+all)
[transformer中的attention为什么scaled?](https://www.zhihu.com/question/339723385)
https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc
https://www.bilibili.com/video/av73631845?t=1451
https://mp.weixin.qq.com/s/lUqpCae3TPkZlgT7gUatpg
https://github.com/LaoGong-zp/Transformer.git



# 附录

### Attention机制的实现

1. 通过输出单元为1的MLP结构来学习出权重，权重可以视为Attention中的注意力。

2. 常用的几种计算Attention分数的方式

   ![image-20220313152728165](C:\Users\Admin\Desktop\学习\figs\f56.jpg)

### 排序模型评估指标

![](.\figs\f41.jpg)

### Embedding 计算逻辑

embedding层背后存的是一个D*k 的矩阵（其中D是隐向量维度，k是特征的所有可能的情况数），则特征值为 i 的embedding向量就是 矩阵中的第 i 列部分。

![](.\figs\f42.jpg)

### 数据挖掘知识杂记

1. 召回阶段
   1. 协同过滤

1. multi-hot编码

与one-hot编码类似使用0-1来表征特征，不过这里特征中1可以是多个。比如对于用户观看历史这类序列特征可以使用multi-hot编码来进行表征。

### tensorflow编程trick

1. tf.multiply

   两个相同维度的张量进行对应元素相乘，支持广播机制（element-wise）

   ```python
   a # [5, 4, 6]
   b # [5, 4, 1]
   tf.multiply(a, b) # [5, 4, 6]
   ```

   

1. tensordot

   可以实现tensor中任意维矩阵相乘，并聚合为一个数

   在PNN网络中的实现，prodcut层中的内积和外积有用

```Python
a = tf.random.normal((5, 3, 3)) 
b = tf.random.normal((3, 3, 10))

# 得到 shape为[5, 10]的外积矩阵
c = tf.tensordot(a, b, axes=[[1, 2], [0, 1]])
print(c.shape)
# c[0] = sum(a[0,:,:] * b[0,:,:]) # 这里的乘法表示的element wise
tf.reduce_sum(tf.multiply(a[0,:,:], b[:,:,0])), c[0][0]


outputs:
(5, 10) # c.shape
(<tf.Tensor: shape=(), dtype=float32, numpy=3.7304153>,
 <tf.Tensor: shape=(), dtype=float32, numpy=3.7304156>)
```

2. tf.matmul

   矩阵乘法，对张量的最后两维的矩阵进行乘法运算

```python
matmul(
    a, # 维度必须大于等于2
    b,
    transpose_a=False, # 将tensor a 的最后两个维度先转置后进行运算
    transpose_b=False, # 将 tesor b 的最后两个维度先转置后计算
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)

```

3. tf.squeeze

   当tensor维度中有None（即表示不定长的时候），对其降维需要精确知道tnesor的dim维度为 1，添加参数 axis = [dim]才能得到正确的降维结果。

4. reduce_sum

   tensorflow中使用聚合函数按某一维度进行聚合，默认会降维。所以如果要保持维度的一致需要指定keepdims=True

5. embedding层具有升为作用

   经过embeding层的左右会使得输入的tensor再增加一维即embedding的长度

```Python
a : [5,4,5]
embed = keras.layers.Embedding(10, 5) # 表示embed的长度为 5，10表示的是label encode 的类别数
embed(a).shape # [5, 4, 5, 5] # 最后一维是每个数的embeding向量
```

6. tf.gather

   tf不支持像numpy一样，使用list作为索引来获取array的多列数据。可以借助gather函数来实现获取指定tensor上某一个维度上的多个index，并组合形成新的tensor来返回。

```Python
a.shape = [6, 7, 8]
tf.gather(a, [1, 3], axis=1) # shape = [6, 2, 8] 其中 axis 1 上的两个索引是记录下来的原[1, 3] 位置
```

7. tf.tile

   将tensor x 作为一个整体，指定在各个dims 上要赋值的次数 ，注意multiples的dims必须和x一致。

```Python
a # tensor shape = [3, 2, 2]
tf.tail(a, multiples=[2, 3, 2]) # tensor shape = [6, 6, 4]
```

7. tensor切片

注意这里的索引取法使用切片读取得到的依旧是二维的tensor，如果是标量读取tensor则会降维

```Python
a # shape = [12, 3, 4]
a[:,1,:] # shape [12, 4] 标量索引会降维
a[:,1:2,:] # shape [12, 1, 4] 切片索引不会降维
```

8. fit steps_per_epoch

   在指定 `steps_per_epoch` 参数后，每轮 (`epoch`) 训练开始时， `TensorFlow` 会从上轮训练结束的 `dataset` 位置继续读取数据并进行训练直至训练流程结束。因此我们最好使用 `.repeat()` 方法来对训练数据进行复制，以保证有足够的数据来完成 `N` 轮的训练。

8. tf.reshape

​	tf.reshape默认是按行的方式排列数据，从而重新组织tensor的形状

9. tf.nn.conv1d

   一维卷积，所谓的一维卷积主要指的就是卷积的方向只有一个方向。

```python
inputs # [batch, seq_len, input_channels]
filters # [width, input_channels, n_filters]
# 经过卷积之后的输出如下
outputs # [batch, n_filters, (seq_len-width+1)//stride]
```

![image-20220310104955023](.\figs\f40.jpg)

https://blog.csdn.net/u013323018/article/details/90444952

10. tf.nn.sampled_softmax_loss

    该函数主要是为了实现对于百万级别的类别进行softmax时，计算缓慢的问题。解决思路是通过负采样来采样k个负例与一个正例形成k+1个二分类问题，将k+1个二分类的损失作为总的损失进行反向传播。

```python
tf.nn.sampled_softmax_loss(weights, # Shape (num_classes, dim)     - floatXX
                     biases,        # Shape (num_classes)          - floatXX 
                     labels,        # Shape (batch_size, num_true) - int64
                     inputs,        # Shape (batch_size, dim)      - floatXX  
                     num_sampled,   # - int
                     num_classes,   # - int
                     num_true=1,  
                     sampled_values=None,
                     remove_accidental_hits=True,
                     partition_strategy="mod",
                     name="sampled_softmax_loss")

# 参数解析
num_sampled是采样的数目
num_classes 是列别数目（如果类别中的0作为掩码则记得+1）
num_true 表示的是某个物品属于的类别数目为多少，1表示每个物品只有一个类别
inputs 是输入的物品相关特征信息
labels 必须是二维张量，且类型为int，指出正例的索引位置
```

​	其中的weights也可以作为类别的embedding向量，配合tf.nn.embedding_lookup函数可以将weights中的某一行读取出来作为 该类的embedding向量，维度为dim。（如果是这个用法，则biasses需要设置为0，且trainable=False）在youtubeDNN中的召回模型训练板块中有使用该技巧。参考资料：https://blog.csdn.net/qq_41978896/article/details/109164450

11. tf.nn.embedding_lookup

主要是用于从embedding matrix 中 读取出某些类的embedding向量，注意读出来的embedding矩阵与传入的index相比会升维。

```python
#weight 的shape为 [num_classes, dims]
tf.nn.embedding_lookup(weight, index:[int, list])
```

12. tf.sequence_mask

    在NLP以及推荐领域经常需要处理不定长的序列，使用tf.sequence_mask可以根据提供的lengths（lengths表征了真实序列的长度，类型：[int | list |嵌套list ]）,以及max_len（表示的是填充后的序列长度）生成相应的mask tensor（默认的是在不足长度的序列尾部进行填充）。可以在计算损失函数的时候mask掉这些填充引起的误差。

    ```python
    # 函数定义
    sequence_mask(
        lengths, # 所有序列的真实长度 类型：[int | list |嵌套list ]
        maxlen=None, # 填充之后的序列长度
        dtype=tf.bool, #默认返回的mask是bool类型，一般需要指定为tf.float32 
        name=None 
    )
    
    # for example
    lengths = [2, 2, 4] # [3, ]
    # 定义dtype时
    mask_data = tf.sequence_mask(lengths=lenght,maxlen=6,dtype=tf.float32)
    # 长度为maxlen，数据格式为float32
    array([[1., 1., 0., 0., 0., 0.],
           [1., 1., 0., 0., 0., 0.],
           [1., 1., 1., 1., 0., 0.]], dtype=float32) # [3, 6]
    
    # 得到的mask是array形式，且与length维度相比会升维
    ```

    [(22条消息) TensorFlow笔记之：填充使用tf.sequence_mask()函数详细说明和应用场景_xinjieyuan的博客-CSDN博客_sequence_mask](https://blog.csdn.net/xinjieyuan/article/details/95760679)

13. tf.keras.preprocessing.sequence.pad_sequences

    这个函数主要用与填充序列，当指定是尾部填充的时候，可以与 tf.sequence_mask配合使用，产生mask张量。

    ```python
    tf.keras.preprocessing.sequence.pad_sequences(
    	sequences,
    	maxlen=None,
    	dtype='int32',
    	padding='pre', # post 指定为尾部填充
    	truncating='pre',
    	value=0) # 填充的值
    
    # for example
    list_1 = [[2, 3, 4]]
    tf.keras.preprocessing.sequence.pad_sequences(list_1, maxlen=10, padding='post')
    # output
    array([[2, 3, 4, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)
    ```

14. tf.linalg.band_part

    该函数主要用于取出矩阵以主对角线为中心，规定范围的次对角线部分，其余部分有0填充。

    ```python
    tf.linalg.band_part(
        input, # 输入的张量
        num_lower, # 下三角矩阵保留的副对角线数量，从主对角线开始计算，相当于下三角的带宽。取值为负数时，则全部保留。
        num_upper,#上三角矩阵保留的副对角线数量，从主对角线开始计算，相当于上三角的带宽。取值为负数时，则全部保留。
        name=None
    )
    
    # for example
    a = tf.ones((4, 5))
    tf.linalg.band_part(a, 2, 1)
    # outputs
    <tf.Tensor: shape=(4, 5), dtype=float32, numpy=
    array([[1., 1., 0., 0., 0.],
           [1., 1., 1., 0., 0.],
           [1., 1., 1., 1., 0.],
           [0., 1., 1., 1., 1.]], dtype=float32)>
    ```

15. tf.maxium & tf.minium

    进行两个张量的element-wise的比较，来选取最大（小）值。支持广播机制，和格式转换。

    ```python
    tf.math.minimum([1,2,3], 2) # 取emenet-wise 的最小值，组成新的tensor
    Out[79]: <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 2])>
          
    ```

16. tf.rsqrt

    Computes reciprocal(倒数) of square root of x element-wise. 只支持浮点类型数据

    ```python
    a = [1, 2, 3]
    tf.rsqrt(a) == 1/tf.sqrt(a)
    ```

17. 自定义学习率调整策略
    以transformer中惯用的 Noam learning Rate Schedule 为例。这是一种非常重要的方式，如果不用这种学习率的话，可能训练不出一个好的Transformer。
    ![img](C:\Users\Admin\Desktop\学习\figs\f70.jpg)

     ```python
     # 自定义优化器学习率的调整策略
     # 需要继承 tf.keras.optimizers.schedules.LearningRateSchedule
     class CustomizedSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
         def __init__(self, d_model, warm_up_steps=4000):
             super(CustomizedSchedule, self).__init__()
             self.d_model = tf.cast(d_model, tf.float32)
             self.warm_up_steps = warm_up_steps
     
         def __call__(self, step): # 传入的参数是训练的step数
             arg1 = tf.math.rsqrt(step)
             arg2 = step * (self.warm_up_steps ** (-1.5))
             arg3 = tf.math.rsqrt(self.d_model)
             return arg3 * tf.math.minimum(arg1, arg2)
         
     # 使用
     # 4、optimizer优化器
     # 将自定义的学习率调整策略传给优化器的 learning_rate 参数
     optimizer = tf.keras.optimizers.Adam(learning_rate=CustomizedSchedule(d_model))
     
     ```

18. tf.clip_by_value

    主要是用来做数值上的截断工作

    ```python
    tf.clip_by_value([1, 12], 4, 10)
    ```
    
19. tf.math.logical_and

    element-wise的逻辑关系判断与np.logical_and原理上是相通的。支持广播运算机制。

20. get_layer

    在keras的API中，表示的是获得某一个层对象

    ```python
    model # 是训练好的keras model
    model.get_layer("flatten").output # 得到flatten层的输出tensor作为其它模型的输入
    dense = tf.keras.layers.Dense(256, activation='relu')(flatten)
    dense = tf.keras.layers.Dense(128, activation='relu')(dense)
    pred = tf.keras.layers.Dense(10, activation='softmax')(dense)
    # 将 base_model的输入 -> pred 这段网络结构看作是新的模型
    fine_tune_model = tf.keras.models.Model(inputs=base_model.input, outputs=pred)
    ```

    

21. fine-tune的时候加载预训练模型

    ```python
    # 冻结前面的卷积层
    for i, layer in enumerate(fine_tune_model.layers): # 打印各层的名字
        print(i, layer.name)
      
    # 明确哪些层需要静止训练
    for layer in fine_tune_model.layers[:6]:
        layer.trainable = False # 前6层禁止训练
    
    ```

    

18. tf.strings.substr

    字符串截取子串函数

    ```python
    path = 'dogs.jpg'
    tf.strings.substr(path, -8, 4) # dogs
    ```
    
18. tf.strings.unicode_decode

    获得字符串的unicode编码，返回的张量和输入的字符串等长

    ```python
    In [168]: tf.strings.unicode_decode('abcd', input_encoding='utf-8')
        
    Out[168]: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 97,  98,  99, 100])>
    ```
    
    
    
18. 使用 tf.GradientTape 模式训练模型

    推荐使用 model.compile 以及 model.fit 的方式，更为简洁

    ```python
    # **************************************  模型训练  ************************************
    # 1、初始化模型
    input_vocab_size = pt_tokenizer.vocab_size + 2
    target_vocab_size = en_tokenizer.vocab_size + 2
    d_model = 128
    max_len = 40
    heads_num = 8
    dff = 512
    layers_num = 4
    transformer = Transformer(input_vocab_size, target_vocab_size, d_model, max_len, heads_num, dff, layers_num) # 实例化模型，此时还没有构建计算图
    
    # 2、自定义损失函数
    def loss_func(real,pred):
        '''
        :param real: shape:(batch_size, target_seq_len)
        :param pred: shape: (batch_size, target_seq_len, target_vocab_size)
        :return:
        '''
        mask = 1-tf.cast(tf.math.equal(real, 0),tf.float32) # shape: (batch_size, target_seq_len)
        loss_function = tf.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')
        loss = loss_function(real,pred) # shape: (batch_size, target_seq_len)
        return tf.reduce_mean(tf.multiply(loss, mask))
    
    # 3、自定义学习率
    # lrate = (d_model **-0.5) * min(step_num **-0.5,step_num*warm_up_steps**-1.5) 先增后减的学习率
    class CustomizedSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
        def __init__(self, d_model, warm_up_steps=4000):
            super(CustomizedSchedule, self).__init__()
            self.d_model = tf.cast(d_model, tf.float32)
            self.warm_up_steps = warm_up_steps
    
        def __call__(self, step):
            arg1 = tf.math.rsqrt(step)
            arg2 = step * (self.warm_up_steps ** (-1.5))
            arg3 = tf.math.rsqrt(self.d_model)
            return arg3 * tf.math.minimum(arg1, arg2)
    # # 学习率曲线绘图
    # temp_learning_rate_schedule = CustomizedSchedule(d_model)
    # # 0-40000步
    # plt.plot(temp_learning_rate_schedule(tf.range(40000,dtype=tf.float32)))
    # plt.ylabel("Learning Rate")
    # plt.xlabel("train_step")
    
    
    # 4、optimizer优化器
    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomizedSchedule(d_model))
    
    # 5、定义评估指标
    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
    
    # 6、定义好 checkpoint
    checkpoint = tf.train.Checkpoint(model=transformer,
                                    optimizer=optimizer)
    
    # 7、定义输入的Type, 用于建立计算图
    train_step_signature = [
        tf.TensorSpec(shape=(None, None), dtype=tf.int64), # 表明是不定长序列，embeding维度可变
        tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    ]
    @tf.function(input_signature=train_step_signature) # !!! 使用装饰器构件计算图
    def train_step(inp, target):
        tar_inp = target[:,:-1] # 作为对于的decoder的输入
        tar_real = target[:,1:] # 作为 对于的decoder的输出
    
        with tf.GradientTape() as tape:
            predictions = transformer(inp, tar_inp, training=True)
            loss = loss_func(tar_real, predictions)
        gradients = tape.gradient(loss, transformer.trainable_variables)
        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
        train_loss(loss)
        train_accuracy(tar_real, predictions)
    
    
    # 8、循环执行训练流程
    import time
    epoches = 120
    for epoch in range(epoches):
        start = time.time()
        train_loss.reset_states()
        train_accuracy.reset_states()
        for (batch, (inp, target)) in enumerate(train_datasets):
            train_step(inp,target)
            if batch % 100 == 0:
                print('Epoch {} Batch {} Loss {} Accuracy {}'.format(epoch+1,batch,train_loss.result(),train_accuracy.result()))
                
        print('Epoch {}  Loss {} Accuracy {}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))
        print('Time take for 1 epoch:{} secs\n'.format(time.time()-start))
        
        if (epoch+1) % 12 == 0:
            path = checkpoint.save(file_prefix='./checkpoint/trf')
            print("**********************    model saved to %s     ************************" % path)
            print()
      
    
    transformer.save_weights('./saved model/transformer.h5')
    
    
    ########################加载checkpoint，实现断点续训#################################
        # 1、创建模型
    transformer = Transformer(input_vocab_size, target_vocab_size, d_model, max_len, heads_num, dff, layers_num)
        # 2、创建optimizer优化器（学习率用上面定义过的）
    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomizedSchedule(d_model))
        # 3、创建checkpoint对象，加载模型与其优化器
    checkpoint = tf.train.Checkpoint(model=transformer,
                                        optimizer=optimizer)
        # 4、将 checkpoint文件信息，恢复到model与优化器上
    checkpoint.restore(tf.train.latest_checkpoint('./checkpoint/'))
    ```

19. Transformer的预测，是将起始状态输入到decoder端，然后循环直到decoder端的输出是结束状态或者到达了序列最长长度的时候终止decoder的部分，完成解码。

    ```python
    def evalute(inp_sentence, model):
        # pt_tokenizer.vocab_size 编码器的起始状态
        # pt_tokenizer.vocab_size + 1编码器的结束状态
        input_id_sentence = [pt_tokenizer.vocab_size]+pt_tokenizer.encode(inp_sentence)+ [pt_tokenizer.vocab_size+1]
        # en_tokenizer.vocab_size # 解码器的起始状态
        # en_tokenizer.vocab_size + 1 解码器的结束状态
        encoder_input =tf.expand_dims(input_id_sentence,0) # (1,input_sentence_length)
        decoder_input = tf.expand_dims([en_tokenizer.vocab_size],0) # (1,1)
        for i in range(max_len): # 循环直到遇见退出状态或达到最长序列为止
            predictions = model(encoder_input, decoder_input, training=False)
            predictions = predictions[:,-1,:] # 单步
            predictions_id = tf.cast(tf.argmax(predictions,axis=-1),tf.int32) #预测概率最大的值
            if tf.equal(predictions_id,en_tokenizer.vocab_size+1):
                return tf.squeeze(decoder_input,axis=0)
            
            decoder_input = tf.concat([decoder_input,[predictions_id]],
                                      axis=-1)
        return tf.squeeze(decoder_input,axis=0)
    
    def translate(input_sentence, model):
        """
        将labelencoder 的句子翻译回去
        """
        result = evalute(input_sentence, model)
        predicted_sentence = en_tokenizer.decode([i for i in result if i < en_tokenizer.vocab_size]) # 防止无词出错
        print("Input: {}".format(input_sentence))
        print("Predicted translation: {}".format(predicted_sentence))
    ```

    

### pandas

1.  data.itertuples()

   当需要遍历dataframe的每一行的时候，推荐使用 data.itertuples() 方法会十分高效，返回的每一行记录都是namedtuple类型。

```Python
for ele in data.itertuples():   # 遍历行这里推荐用itertuples， 比iterrows会高效很多
    user, movie, rating = getattr(ele, 'userId'), getattr(ele, 'movieId'), getattr(ele, 'rating')
```

2. tf.embedding_lookup

   就是查某个索引（也可以是多个索引）对应的embedding 向量，背后是通过tf.gather来获取。

```python
# w shape [num_class, embedding_dims]
tf.embedding_lookup(w, [1, 2, 3]) # 获取[1, 2, 3] 的embedding
```

3. layer.Layer

   ```python
   build(self, input_shape): pass # 在示例化模型的时候不会触发，当输入 inputs 的时候即调用call方法的时候会触发。 input_shape 是有batch的维度的
   # 对于 GRUCell 这类单元序列模型来说，input_shape 记录的是输入的shape（与单元的隐藏状态无关）
   # eg ：
   inputs : shape[12, 3, 34] # [batch, seq_len, embed_dims]
   则传入build的shape就为 [12, 3, 4] 
   
   # 如果 cell 传入到 RNN 中就会形成 RNN结构
   # 此时的 inputs: 将会依次从RNN的输入tensor [batch, seq_len, embed_dims] 中沿着 axis=1 的方向抽取 tensor [batch, embed_dims] 作为输入传入 Cell 中
   # 所以 cell 的 build 中接受到的 input_shape 为 [batch, embed_dims]
   ```

   

### numpy

1. np.ascontiguousarray 

   可以将不连续存放的数组重构成为连续存放的形式（默认是按行进行连续存放数组元素），从而加快运行速度。

   ```python
   import numpy as np
   arr = np.arange(12).reshape(3,4)
   flags = arr.flags # 可以通过 flags 属性来查看数组是否连续存放
   b = arr[:, 0:3] # 切片得到的array不是连续存放的
   c_b = np.ascontiguousarray(b) #  转换成连续存放的数组
   ```

   

   