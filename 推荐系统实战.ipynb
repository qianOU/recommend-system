{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "b4366d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from ctr.fm.model import FM as NEW_FM\n",
    "\n",
    "from ctr.utils.data_process import create_criteo_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 切换工作目录\n",
    "os.chdir(r'C:\\Users\\Admin\\Desktop\\recommend-tf2.0-main\\src\\ctr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2675b441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>80e26c9b</td>\n",
       "      <td>fb936136</td>\n",
       "      <td>7b4723c4</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>de7995b8</td>\n",
       "      <td>1f89b562</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>a8cd5504</td>\n",
       "      <td>b2cb9c98</td>\n",
       "      <td>37c9c164</td>\n",
       "      <td>2824a5f6</td>\n",
       "      <td>1adce6ef</td>\n",
       "      <td>8ba8b39a</td>\n",
       "      <td>891b62e7</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>f54016b9</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>07b5194c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c5c50484</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>9727dd16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>f0cf0024</td>\n",
       "      <td>6f67f7e5</td>\n",
       "      <td>41274cd7</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>922afcc0</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>2b53e5fb</td>\n",
       "      <td>4f1b46f3</td>\n",
       "      <td>623049e6</td>\n",
       "      <td>d7020589</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>e6c5b5cd</td>\n",
       "      <td>c92f3b61</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>b04e4670</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>60f6221e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>43f13e8b</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>731c3655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>287e684f</td>\n",
       "      <td>0a519c5c</td>\n",
       "      <td>02cf9876</td>\n",
       "      <td>c18be181</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>c78204a1</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>3b08e48b</td>\n",
       "      <td>5f5e6091</td>\n",
       "      <td>8fe001f4</td>\n",
       "      <td>aa655a2f</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>6dc710ed</td>\n",
       "      <td>36103458</td>\n",
       "      <td>8efede7f</td>\n",
       "      <td>3412118d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>ad3062eb</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4392.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>2c16a946</td>\n",
       "      <td>a9a87e68</td>\n",
       "      <td>2e17d6f6</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>2e8a689b</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>efea433b</td>\n",
       "      <td>e51ddf94</td>\n",
       "      <td>a30567ca</td>\n",
       "      <td>3516f6e6</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>18231224</td>\n",
       "      <td>52b8680f</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>74ef3502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6b3a5ca6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>9117a34a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   I1   I2    I3    I4      I5    I6    I7   I8     I9  I10  I11  I12  \\\n",
       "0      0  1.0    1   5.0   0.0  1382.0   4.0  15.0  2.0  181.0  1.0  2.0  NaN   \n",
       "1      0  2.0    0  44.0   1.0   102.0   8.0   2.0  2.0    4.0  1.0  1.0  NaN   \n",
       "2      0  2.0    0   1.0  14.0   767.0  89.0   4.0  2.0  245.0  1.0  3.0  3.0   \n",
       "3      0  NaN  893   NaN   NaN  4392.0   NaN   0.0  0.0    0.0  NaN  0.0  NaN   \n",
       "\n",
       "    I13        C1        C2        C3        C4        C5        C6        C7  \\\n",
       "0   2.0  68fd1e64  80e26c9b  fb936136  7b4723c4  25c83c98  7e0ccccf  de7995b8   \n",
       "1   4.0  68fd1e64  f0cf0024  6f67f7e5  41274cd7  25c83c98  fe6b92e5  922afcc0   \n",
       "2  45.0  287e684f  0a519c5c  02cf9876  c18be181  25c83c98  7e0ccccf  c78204a1   \n",
       "3   NaN  68fd1e64  2c16a946  a9a87e68  2e17d6f6  25c83c98  fe6b92e5  2e8a689b   \n",
       "\n",
       "         C8        C9       C10       C11       C12       C13       C14  \\\n",
       "0  1f89b562  a73ee510  a8cd5504  b2cb9c98  37c9c164  2824a5f6  1adce6ef   \n",
       "1  0b153874  a73ee510  2b53e5fb  4f1b46f3  623049e6  d7020589  b28479f6   \n",
       "2  0b153874  a73ee510  3b08e48b  5f5e6091  8fe001f4  aa655a2f  07d13a8f   \n",
       "3  0b153874  a73ee510  efea433b  e51ddf94  a30567ca  3516f6e6  07d13a8f   \n",
       "\n",
       "        C15       C16       C17       C18       C19       C20       C21  \\\n",
       "0  8ba8b39a  891b62e7  e5ba7672  f54016b9  21ddcdc9  b1252a9d  07b5194c   \n",
       "1  e6c5b5cd  c92f3b61  07c540c4  b04e4670  21ddcdc9  5840adea  60f6221e   \n",
       "2  6dc710ed  36103458  8efede7f  3412118d       NaN       NaN  e587c466   \n",
       "3  18231224  52b8680f  1e88c74f  74ef3502       NaN       NaN  6b3a5ca6   \n",
       "\n",
       "        C22       C23       C24       C25       C26  \n",
       "0       NaN  3a171ecb  c5c50484  e8b83407  9727dd16  \n",
       "1       NaN  3a171ecb  43f13e8b  e8b83407  731c3655  \n",
       "2  ad3062eb  3a171ecb  3b183c5c       NaN       NaN  \n",
       "3       NaN  3a171ecb  9117a34a       NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "# =============================== GPU ==============================\n",
    "# gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# print(gpu)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n",
    "# ========================= Hyper Parameters =======================\n",
    "# you can modify your file path\n",
    "file = '../data/criteo_sampled_data.csv'\n",
    "read_part = True\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "df = pd.read_csv(file, nrows=4)\n",
    "df.head()\n",
    "\n",
    "def create_criteo_dataset(file, embed_dim=8, read_part=True, sample_num=100000, test_size=0.2):\n",
    "    \"\"\"\n",
    "    a example about creating criteo dataset\n",
    "    :param file: dataset's path\n",
    "    :param embed_dim: the embedding dimension of sparse features\n",
    "    :param read_part: whether to read part of it\n",
    "    :param sample_num: the number of instances if read_part is True\n",
    "    :param test_size: ratio of test dataset\n",
    "    :return: feature columns, train, test\n",
    "    \"\"\"\n",
    "    names = ['label', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11',\n",
    "             'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "             'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22',\n",
    "             'C23', 'C24', 'C25', 'C26']\n",
    "\n",
    "    if read_part:\n",
    "        data_df = pd.read_csv(file, iterator=True)\n",
    "        data_df = data_df.get_chunk(sample_num)\n",
    "\n",
    "    else:\n",
    "        data_df = pd.read_csv(file)\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "    data_df[sparse_features] = data_df[sparse_features].fillna('-1')\n",
    "    data_df[dense_features] = data_df[dense_features].fillna(0)\n",
    "\n",
    "    for feat in sparse_features:\n",
    "        le = LabelEncoder()\n",
    "        data_df[feat] = le.fit_transform(data_df[feat].astype(str))\n",
    "\n",
    "    # ==============Feature Engineering===================\n",
    "\n",
    "    # dense_features = [feat for feat in data_df.columns if feat not in sparse_features + ['label']]\n",
    "\n",
    "\n",
    "    for feat in dense_features:\n",
    "        mms = MinMaxScaler()\n",
    "        data_df[feat] = mms.fit_transform(data_df[dense_features].astype(int))\n",
    "\n",
    "    feature_columns = [[denseFeature(feat) for feat in dense_features]] + \\\n",
    "                      [[sparseFeature(feat, len(data_df[feat].unique()), embed_dim=embed_dim)\n",
    "                        for feat in sparse_features]]\n",
    "\n",
    "    train, test = train_test_split(data_df, test_size=test_size)\n",
    "\n",
    "    train_X = [train[dense_features].values.astype('float32'), train[sparse_features].values.astype('int32')]\n",
    "    train_y = train['label'].values.astype('int32')\n",
    "    test_X = [test[dense_features].values.astype('float32'), test[sparse_features].values.astype('int32')]\n",
    "    test_y = test['label'] .values.astype('int32')\n",
    "\n",
    "    return feature_columns, (train_X, train_y), (test_X, test_y)\n",
    "\n",
    "# ========================== Create dataset =======================\n",
    "feature_columns, train, test = create_criteo_dataset(file=file,\n",
    "                                       read_part=read_part,\n",
    "                                       sample_num=sample_num,\n",
    "                                       test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952dad36",
   "metadata": {},
   "source": [
    "# FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "2c619bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(keras.Model):\n",
    "    def __init__(self, latent_dim, feat_num):\n",
    "        super().__init__()\n",
    "        # 隐向量\n",
    "        self.v = self.add_weight(shape=(feat_num, latent_dim),\n",
    "                                initializer=keras.initializers.glorot_normal(),\n",
    "                                trainable=True)\n",
    "        self.w = self.add_weight(shape=(feat_num, 1),\n",
    "                                initializer=keras.initializers.glorot_normal()\n",
    "                                ,trainable=True)\n",
    "        self.bias = tf.Variable([0,], dtype=tf.float32, trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # x shape [batch, feat_num]\n",
    "        # 一阶交叉\n",
    "        first_order = self.bias + tf.matmul(x, self.w) # [batch, 1]\n",
    "        # 二阶交叉\n",
    "        second_order = 0.5*(tf.reduce_sum(\n",
    "            tf.pow(tf.matmul(x, self.v), 2) # [batch, latent] \n",
    "            - tf.matmul(tf.pow(x, 2), tf.pow(self.v, 2)),\n",
    "                axis=1,\n",
    "            keepdims=True\n",
    "        ))\n",
    "        return first_order + second_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "k = 8\n",
    "train_x = np.concatenate(train_X, axis=1)\n",
    "test_x = np.concatenate(test_X, axis=1)\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "\n",
    "model = FM(k, train_x.shape[1])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "metric = tf.keras.metrics.AUC()\n",
    "\n",
    "# 创建训练集\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train_data = train_data.repeat(3).batch(batch_size)\n",
    "\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    for idx, (train_x1, train_y1) in enumerate(train_data):\n",
    "#         print(train_x1.shape, train_y1.shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = model(train_x1)\n",
    "            loss = loss_fn(train_y1, tf.nn.sigmoid(y_hat))\n",
    "            gradient = tape.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(gradient, model.trainable_variables)) # 需要zip一下配对\n",
    "        if idx % 300 == 0:\n",
    "            print(f'epoch={i}, batch={idx} loss={tf.reduce_mean(loss)}')\n",
    "    auc = metric(test_y,  tf.nn.sigmoid(model(test_x)))\n",
    "    print(f'epoch={i}, auc={auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc1e83",
   "metadata": {},
   "source": [
    "# 调包实现fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "0a8b3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'w:0' shape=(241351, 1) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "加载模型权重参数！\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.7615 - auc: 0.4763\n",
      "Epoch 00001: val_auc improved from -inf to 0.46974, saving model to ./save\\fm_weight.ckpt\n",
      "141/141 [==============================] - 69s 484ms/step - loss: 0.7615 - auc: 0.4763 - val_loss: 0.7633 - val_auc: 0.4697\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.7615 - auc: 0.4763\n",
      "Epoch 00002: val_auc did not improve from 0.46974\n",
      "141/141 [==============================] - 74s 524ms/step - loss: 0.7615 - auc: 0.4763 - val_loss: 0.7633 - val_auc: 0.4697\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - ETA: 0s - loss: 0.7615 - auc: 0.4763\n",
      "Epoch 00003: val_auc did not improve from 0.46974\n",
      "141/141 [==============================] - 75s 532ms/step - loss: 0.7615 - auc: 0.4763 - val_loss: 0.7633 - val_auc: 0.4697\n",
      "40/40 [==============================] - 21s 526ms/step - loss: 0.7596 - auc: 0.4874\n",
      "test AUC: 0.487398\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "\n",
    "k = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "# ============================model checkpoint======================\n",
    "check_path = './save/fm_weight.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(check_path, save_weights_only=True,\n",
    "                                                verbose=1, save_best_only=True, monitor='val_auc',\n",
    "                                               mode='max')\n",
    "\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "# with mirrored_strategy.scope():\n",
    "fm = NEW_FM(feature_columns=feature_columns, k=k) # 包里的实现\n",
    "model = fm.build_graph()\n",
    "if os.path.exists(check_path + '.index'):\n",
    "    print('加载模型权重参数！')\n",
    "    model.load_weights(check_path)\n",
    "\n",
    "#     model.summary()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=binary_crossentropy, optimizer=Adam(learning_rate=learning_rate),\n",
    "              metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "# ==============================Fit==============================\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "              checkpoint],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a43bc",
   "metadata": {},
   "source": [
    "# GBDT + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e8bcf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=7, min_samples_split=500, n_estimators=50,\n",
       "                           random_state=10, subsample=0.6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=50, random_state=10, subsample=.6, max_depth=7,\n",
    "                               min_samples_split=500)\n",
    "gb.fit(np.concatenate(train_X, axis=1), train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "128c9ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 50)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feature = gb.apply(np.concatenate(train_X, axis=1)).reshape(-1, 50) # apply 方法放回在 50棵树中，每个训练样本落的叶子节点的索引\n",
    "new_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d491cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False).fit(new_feature)\n",
    "train_new_x = enc.transform(new_feature)\n",
    "test_new_x  = enc.transform(gb.apply(np.concatenate(test_X, axis=1)).reshape(-1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f51a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lr = LogisticRegression(C=0.5, max_iter=500)\n",
    "lr.fit(train_new_x, train_y)\n",
    "       \n",
    "print('在LR部分只使用GBDT得到的组合特征')\n",
    "print('训练集：auc= %.4f' % roc_auc_score(train_y, lr.predict_proba(train_new_x)[:, 1]))\n",
    "print('测试集：auc= %.4f' % roc_auc_score(test_y, lr.predict_proba(test_new_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.5, max_iter=500)\n",
    "lr.fit(np.concatenate((train_new_x, np.concatenate(train_X, axis=1)), axis=1), train_y)\n",
    "\n",
    "print('在LR部分使用GBDT得到的组合特征 + 原始特征')\n",
    "print('训练集：auc= %.4f' % roc_auc_score(train_y, lr.predict_proba(np.concatenate((train_new_x, np.concatenate(train_X, axis=1)), axis=1))[:, 1]))\n",
    "print('测试集：auc= %.4f' % roc_auc_score(test_y, lr.predict_proba(np.concatenate((test_new_x, np.concatenate(test_X, axis=1)), axis=1))[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c4601",
   "metadata": {},
   "source": [
    "### 分析\n",
    "从结果来看，似乎在LR部分不应该再加入原始特征部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb8268fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4302"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3671c85",
   "metadata": {},
   "source": [
    "# MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36a7a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b0be5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\recommend-tf2.0-main\\src\\ctr\n"
     ]
    }
   ],
   "source": [
    "#数据处理\n",
    "def data_concat(train,test):\n",
    "    train['type'] = 1\n",
    "    test['type'] = 2\n",
    "\n",
    "    \n",
    "    all_columns=['age','workclass','fnlwgt','education','education-num','marital-status',\n",
    "                'occupation','relationship','race','sex','capital-gain','capital-loss',\n",
    "                'hours-per-week','native-country','label','type']\n",
    "    all_data=pd.concat([train,test],axis=0)\n",
    "    all_data.columns=all_columns\n",
    "    return all_data\n",
    "\n",
    "def data_processing(train,test):\n",
    "    df=data_concat(train,test)\n",
    "    continus_columns=['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "    category_columns=['workclass','education','marital-status','occupation','relationship','race','sex','native-country']\n",
    "    #类别变量做one_hot_encoding\n",
    "    df=pd.get_dummies(df,columns=category_columns)\n",
    "    #连续数据标准化\n",
    "    for col in continus_columns:\n",
    "        ss=StandardScaler()\n",
    "        df[col]=ss.fit_transform(df[[col]])\n",
    "\n",
    "    df['label']=df['label'].apply(lambda x: 1 if  x.strip()=='>50K'  else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "train_data=pd.read_table(r'../data/adult.data',header=None,delimiter=',')\n",
    "test_data=pd.read_table(r'../data/adult.test',header=None,delimiter=',', skiprows=[0])\n",
    "test_data[14]=test_data[14].apply(lambda x: x[:-1])\n",
    "df = data_processing(train_data,test_data)\n",
    "train_data=df[df['type']==1].drop(['type', 'label'],axis=1).astype(np.float32) \n",
    "train_label = df.loc[df['type']==1, ['label']].astype(np.float32) \n",
    "test_data=df[df['type']==2].drop(['type', 'label'],axis=1).astype(np.float32) \n",
    "test_label = df.loc[df['type']==2, ['label']].astype(np.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000f3a5",
   "metadata": {},
   "source": [
    "### 建模\n",
    "1. 使用用户数据进行聚类（实际就是一个m输出问题，activation = softmax）\n",
    "2. 使用广告数据进行分类训练逻辑回归模型（实际就是一个m输出问题， activation= sigmoid）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2389da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f43df2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 12\n",
    "x = keras.Input(shape=(108,))\n",
    "w = keras.layers.Dense(m, activation='softmax', kernel_regularizer=keras.regularizers.l1_l2(0.001))(x)\n",
    "u = keras.layers.Dense(m, activation='sigmoid', kernel_regularizer=keras.regularizers.l1_l2(0.001))(x)\n",
    "output = tf.clip_by_value(tf.math.reduce_sum(tf.multiply(w, u), axis=1, keepdims=True),  0.00001, 1 - 0.00001)\n",
    "\n",
    "model = tf.keras.Model(inputs=x, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0fc42434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(ytrue, y_pred):\n",
    "    val = tf.add(tf.multiply(ytrue, tf.math.log(y_pred)),\n",
    "           tf.multiply(1-ytrue, tf.math.log(1-y_pred)))\n",
    "    return -tf.reduce_sum(val) # 损失函数是负对数似然\n",
    "\n",
    "model.compile(loss=my_loss,\n",
    "             optimizer='adam',\n",
    "             metrics=[keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5fba31c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 108)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 12)           1308        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 12)           1308        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 12)          0           ['dense_12[0][0]',               \n",
      " )                                                                'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_6 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_6[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.clip_by_value (TFOpLambda)  (None, 1)            0           ['tf.math.reduce_sum_6[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,616\n",
      "Trainable params: 2,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1018/1018 [==============================] - 1s 873us/step - loss: 10.2188 - auc_10: 0.9131\n",
      "Epoch 2/10\n",
      "1018/1018 [==============================] - 1s 798us/step - loss: 10.2006 - auc_10: 0.9133\n",
      "Epoch 3/10\n",
      "1018/1018 [==============================] - 1s 859us/step - loss: 10.1882 - auc_10: 0.9135\n",
      "Epoch 4/10\n",
      "1018/1018 [==============================] - 1s 852us/step - loss: 10.1793 - auc_10: 0.9135\n",
      "Epoch 5/10\n",
      "1018/1018 [==============================] - 1s 847us/step - loss: 10.1678 - auc_10: 0.9137\n",
      "Epoch 6/10\n",
      "1018/1018 [==============================] - 1s 868us/step - loss: 10.1560 - auc_10: 0.9138\n",
      "Epoch 7/10\n",
      "1018/1018 [==============================] - 1s 827us/step - loss: 10.1554 - auc_10: 0.9138\n",
      "Epoch 8/10\n",
      "1018/1018 [==============================] - 1s 905us/step - loss: 10.1483 - auc_10: 0.9138\n",
      "Epoch 9/10\n",
      "1018/1018 [==============================] - 1s 838us/step - loss: 10.1402 - auc_10: 0.9140\n",
      "Epoch 10/10\n",
      "1018/1018 [==============================] - 1s 896us/step - loss: 10.1365 - auc_10: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18488254400>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.fit(train_data, train_label, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "739b6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集合 0.9106644201433518\n"
     ]
    }
   ],
   "source": [
    "print('测试集合', roc_auc_score(test_label.squeeze(), model.predict(test_data).squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37000592",
   "metadata": {},
   "source": [
    "# NeuralCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dff77f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Aug 8, 2016\n",
    "Processing datasets. \n",
    "\n",
    "@author: Xiangnan He (xiangnanhe@gmail.com)\n",
    "'''\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "        \n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        '''\n",
    "        Read .rating file and Return dok matrix.\n",
    "        The first line of .rating file is: num_users\\t num_items\n",
    "        '''\n",
    "        # Get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                u, i = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, u)\n",
    "                num_items = max(num_items, i)\n",
    "                line = f.readline()\n",
    "        # Construct matrix\n",
    "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "                if (rating > 0):\n",
    "                    mat[user, item] = 1.0\n",
    "                line = f.readline()    \n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e293bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('NeuralCF-master/Data/' + 'ml-1m')\n",
    "train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d28d69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    k = 0\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(numItems)\n",
    "            while (u, j) in train: #train.has_key((u, j)):\n",
    "                j = np.random.randint(numItems)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "        if len(user_input) > 1e5: break # 小规模进行训练\n",
    "        \n",
    "    return user_input, item_input, labels   \n",
    "\n",
    "user_input, item_input, labels = get_train_instances(train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2b75b",
   "metadata": {},
   "source": [
    "## GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cc30d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1, 20)        120800      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1, 20)        74120       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 20)           0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 20)           0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 20)          0           ['flatten_5[0][0]',              \n",
      " )                                                                'flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            21          ['tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 194,941\n",
      "Trainable params: 194,941\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "numUser, numItems = train.shape\n",
    "latent_dim = 20\n",
    "feat_user = keras.Input(shape=(1,))\n",
    "embedding_user = keras.layers.Embedding(input_dim=numUser, output_dim=latent_dim)(feat_user)\n",
    "\n",
    "feat_item = keras.Input(shape=(1,))\n",
    "embedding_item = keras.layers.Embedding(input_dim=numItems, output_dim=latent_dim)(feat_item)\n",
    "# new_feat = tf.concat([embedding_user, embedding_item])\n",
    "# 可以通过Flatten展平embedding向量\n",
    "embedding_item = keras.layers.Flatten()(embedding_item)\n",
    "embedding_user = keras.layers.Flatten()(embedding_user)\n",
    "\n",
    "out_put = tf.multiply(embedding_user, embedding_item)\n",
    "out = tf.keras.layers.Dense(1, activation='sigmoid')(out_put)\n",
    "\n",
    "model = tf.keras.Model(inputs=[feat_user, feat_item], outputs=out)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.AUC()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8d78163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3126/3126 [==============================] - 4s 1ms/step - loss: 0.4600 - auc_1: 0.7127\n",
      "Epoch 2/5\n",
      "3126/3126 [==============================] - 4s 1ms/step - loss: 0.3313 - auc_1: 0.8784\n",
      "Epoch 3/5\n",
      "3126/3126 [==============================] - 4s 1ms/step - loss: 0.2822 - auc_1: 0.9169\n",
      "Epoch 4/5\n",
      "3126/3126 [==============================] - 4s 1ms/step - loss: 0.2347 - auc_1: 0.9456\n",
      "Epoch 5/5\n",
      "3126/3126 [==============================] - 4s 1ms/step - loss: 0.1883 - auc_1: 0.9669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18491fad610>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.array(user_input), np.array(item_input)], np.array(labels)[:,np.newaxis], epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eefd31",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "afdee61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numUser, numItems = train.shape\n",
    "latent_dim = 20\n",
    "feat_user = keras.Input(shape=(1,))\n",
    "embedding_user = keras.layers.Embedding(input_dim=numUser, output_dim=latent_dim)(feat_user)\n",
    "embedding_user = tf.keras.layers.Flatten()(embedding_user)\n",
    "\n",
    "feat_item = keras.Input(shape=(1,))\n",
    "embedding_item = keras.layers.Embedding(input_dim=numItems, output_dim=latent_dim)(feat_item)\n",
    "embedding_item = tf.keras.layers.Flatten()(embedding_item)\n",
    "\n",
    "x = tf.concat([embedding_user, embedding_item], axis=1)\n",
    "\n",
    "hidden_units = [128, 64, 32]\n",
    "for i in hidden_units:\n",
    "    layer = keras.layers.Dense(i, activation='relu', kernel_regularizer=keras.regularizers.l2())\n",
    "    x = layer(x)\n",
    "\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model2 = keras.Model(inputs=[feat_user, feat_item], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "439221a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3126/3126 [==============================] - 6s 2ms/step - loss: 0.4827 - auc: 0.7793\n",
      "Epoch 2/5\n",
      "3126/3126 [==============================] - 5s 2ms/step - loss: 0.3763 - auc: 0.8614\n",
      "Epoch 3/5\n",
      "3126/3126 [==============================] - 5s 2ms/step - loss: 0.3627 - auc: 0.8663\n",
      "Epoch 4/5\n",
      "3126/3126 [==============================] - 5s 2ms/step - loss: 0.3561 - auc: 0.8683\n",
      "Epoch 5/5\n",
      "3126/3126 [==============================] - 5s 2ms/step - loss: 0.3522 - auc: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18492172b50>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model2.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=[keras.metrics.AUC()])\n",
    "model2.fit([np.array(user_input), np.array(item_input)], np.array(labels)[:,np.newaxis], epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf959b07",
   "metadata": {},
   "source": [
    "## GMF + MLP \n",
    "注意 GMF 与 MLP 的隐向量需要各自独立训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "46f7bb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 30)        181200      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 30)        111180      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 30)           0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 30)           0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 60)           0           ['flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 20)        74120       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 20)        120800      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          7808        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 20)           0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 20)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 20)           0           ['flatten_1[0][0]',              \n",
      "                                                                  'flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 52)           0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            53          ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 505,497\n",
      "Trainable params: 505,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "numUser, numItems = train.shape\n",
    "\n",
    "feat_user = keras.Input(shape=(1,))\n",
    "feat_item = keras.Input(shape=(1,))\n",
    "# gmf embedding\n",
    "latent_dim = 20\n",
    "gmf_embedding_user = keras.layers.Embedding(input_dim=numUser, output_dim=latent_dim)(feat_user)\n",
    "gmf_embedding_user = tf.keras.layers.Flatten()(gmf_embedding_user)\n",
    "\n",
    "\n",
    "gmf_embedding_item = keras.layers.Embedding(input_dim=numItems, output_dim=latent_dim)(feat_item)\n",
    "gmf_embedding_item = tf.keras.layers.Flatten()(gmf_embedding_item)\n",
    "\n",
    "# # mlp embedding\n",
    "latent_dim = 30\n",
    "mlp_embedding_user = keras.layers.Embedding(input_dim=numUser, output_dim=latent_dim)(feat_user)\n",
    "mlp_embedding_user = tf.keras.layers.Flatten()(mlp_embedding_user)\n",
    "\n",
    "\n",
    "mlp_embedding_item = keras.layers.Embedding(input_dim=numItems, output_dim=latent_dim)(feat_item)\n",
    "mlp_embedding_item = tf.keras.layers.Flatten()(mlp_embedding_item)\n",
    "\n",
    "\n",
    "# GMF部分·\n",
    "y1 = tf.multiply(gmf_embedding_item, gmf_embedding_user)\n",
    "# MLP部分\n",
    "x = tf.concat([mlp_embedding_user, mlp_embedding_item], axis=1)\n",
    "hidden_units = [128, 64, 32]\n",
    "for i in hidden_units:\n",
    "    layer = keras.layers.Dense(i, activation='relu', kernel_regularizer=keras.regularizers.l2())\n",
    "    x = layer(x)\n",
    "\n",
    "x = tf.concat([y1, x], axis=1)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=[feat_user, feat_item], outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "79b9d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3126/3126 [==============================] - 9s 3ms/step - loss: 0.4778 - auc: 0.7797\n",
      "Epoch 2/5\n",
      "3126/3126 [==============================] - 8s 3ms/step - loss: 0.3222 - auc: 0.8965\n",
      "Epoch 3/5\n",
      "3126/3126 [==============================] - 8s 3ms/step - loss: 0.2473 - auc: 0.9394\n",
      "Epoch 4/5\n",
      "3126/3126 [==============================] - 9s 3ms/step - loss: 0.1937 - auc: 0.9650\n",
      "Epoch 5/5\n",
      "3126/3126 [==============================] - 8s 3ms/step - loss: 0.1504 - auc: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18497d13a90>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=[keras.metrics.AUC()])\n",
    "model.fit([np.array(user_input), np.array(item_input)], np.array(labels)[:,np.newaxis], epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97006f",
   "metadata": {},
   "source": [
    "# PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "67db13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "keras.backend.clear_session()\n",
    "# =============================== GPU ==============================\n",
    "# gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# print(gpu)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n",
    "# ========================= Hyper Parameters =======================\n",
    "# you can modify your file path\n",
    "file = '../data/criteo_sampled_data.csv'\n",
    "read_part = True\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "embed_dims = 8 # embedding 的维度\n",
    "df = pd.read_csv(file, nrows=4)\n",
    "df.head()\n",
    "# ========================== Create dataset =======================\n",
    "# train的结构是元组 ([dense_feature, sparse_feature], label_)\n",
    "\n",
    "feature_columns, train, test = create_criteo_dataset(file=file,\n",
    "                                       read_part=read_part,\n",
    "                                       sample_num=sample_num,\n",
    "                                       test_size=test_size,\n",
    "                                        embed_dim=embed_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "48dae162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(units_in_prdoucts = 256): # dnn 结构，参数为输入dnn的特征维度\n",
    "    return keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(units_in_prdoucts,)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "    *[ keras.layers.Dense(item, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)) for item in (128, 64, 32)]\n",
    "    , keras.layers.Dropout(0.2)\n",
    "    ,keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01)) # for ctr\n",
    "    ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b45bf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feat = keras.Input(shape=(13,))\n",
    "sparse_feat = keras.Input(shape=(26,))\n",
    "\n",
    "res = []\n",
    "for i, dic_ in enumerate(feature_columns[1]):\n",
    "    kinds = dic_['feat_num']\n",
    "    k = dic_['embed_dim']\n",
    "    emb_layer = keras.layers.Embedding(kinds, k)\n",
    "    res.append(tf.expand_dims(emb_layer(sparse_feat[:, i]), axis=1)) # 将 axis=1 设置为 filed域\n",
    "sparse_embeds = tf.concat(res, axis=1) # [batch, fileds, embed_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "2ba40e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None,)             0           ['input_2[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None,)             0           ['input_2[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_15 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_17 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_20 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_21 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_22 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_23 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_24 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_25 (S  (None,)             0           ['input_2[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 8)            4328        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 8)            3976        ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 8)            350960      ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 8)            201472      ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 8)            1160        ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 8)            96          ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 8)            60984       ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 8)            2056        ['tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 8)            24          ['tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 8)            87976       ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 8)            30392       ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 8)            330496      ['tf.__operators__.getitem_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 8)            22368       ['tf.__operators__.getitem_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 8)            208         ['tf.__operators__.getitem_13[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_14 (Embedding)       (None, 8)            41904       ['tf.__operators__.getitem_14[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)       (None, 8)            276936      ['tf.__operators__.getitem_15[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, 8)            80          ['tf.__operators__.getitem_16[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_17 (Embedding)       (None, 8)            20384       ['tf.__operators__.getitem_17[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_18 (Embedding)       (None, 8)            10424       ['tf.__operators__.getitem_18[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_19 (Embedding)       (None, 8)            32          ['tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 8)            308944      ['tf.__operators__.getitem_20[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_21 (Embedding)       (None, 8)            88          ['tf.__operators__.getitem_21[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_22 (Embedding)       (None, 8)            112         ['tf.__operators__.getitem_22[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_23 (Embedding)       (None, 8)            98680       ['tf.__operators__.getitem_23[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_24 (Embedding)       (None, 8)            408         ['tf.__operators__.getitem_24[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_25 (Embedding)       (None, 8)            76216       ['tf.__operators__.getitem_25[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 8)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 8)         0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1, 8)         0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 1, 8)         0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 1, 8)         0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 1, 8)         0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLambda)  (None, 1, 8)         0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_7 (TFOpLambda)  (None, 1, 8)         0           ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_8 (TFOpLambda)  (None, 1, 8)         0           ['embedding_8[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_9 (TFOpLambda)  (None, 1, 8)         0           ['embedding_9[0][0]']            \n",
      "                                                                                                  \n",
      " tf.expand_dims_10 (TFOpLambda)  (None, 1, 8)        0           ['embedding_10[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_11 (TFOpLambda)  (None, 1, 8)        0           ['embedding_11[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_12 (TFOpLambda)  (None, 1, 8)        0           ['embedding_12[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_13 (TFOpLambda)  (None, 1, 8)        0           ['embedding_13[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_14 (TFOpLambda)  (None, 1, 8)        0           ['embedding_14[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1, 8)        0           ['embedding_15[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_16 (TFOpLambda)  (None, 1, 8)        0           ['embedding_16[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_17 (TFOpLambda)  (None, 1, 8)        0           ['embedding_17[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_18 (TFOpLambda)  (None, 1, 8)        0           ['embedding_18[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_19 (TFOpLambda)  (None, 1, 8)        0           ['embedding_19[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_20 (TFOpLambda)  (None, 1, 8)        0           ['embedding_20[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_21 (TFOpLambda)  (None, 1, 8)        0           ['embedding_21[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_22 (TFOpLambda)  (None, 1, 8)        0           ['embedding_22[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_23 (TFOpLambda)  (None, 1, 8)        0           ['embedding_23[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_24 (TFOpLambda)  (None, 1, 8)        0           ['embedding_24[0][0]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_25 (TFOpLambda)  (None, 1, 8)        0           ['embedding_25[0][0]']           \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 26, 8)        0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'tf.expand_dims_1[0][0]',       \n",
      "                                                                  'tf.expand_dims_2[0][0]',       \n",
      "                                                                  'tf.expand_dims_3[0][0]',       \n",
      "                                                                  'tf.expand_dims_4[0][0]',       \n",
      "                                                                  'tf.expand_dims_5[0][0]',       \n",
      "                                                                  'tf.expand_dims_6[0][0]',       \n",
      "                                                                  'tf.expand_dims_7[0][0]',       \n",
      "                                                                  'tf.expand_dims_8[0][0]',       \n",
      "                                                                  'tf.expand_dims_9[0][0]',       \n",
      "                                                                  'tf.expand_dims_10[0][0]',      \n",
      "                                                                  'tf.expand_dims_11[0][0]',      \n",
      "                                                                  'tf.expand_dims_12[0][0]',      \n",
      "                                                                  'tf.expand_dims_13[0][0]',      \n",
      "                                                                  'tf.expand_dims_14[0][0]',      \n",
      "                                                                  'tf.expand_dims_15[0][0]',      \n",
      "                                                                  'tf.expand_dims_16[0][0]',      \n",
      "                                                                  'tf.expand_dims_17[0][0]',      \n",
      "                                                                  'tf.expand_dims_18[0][0]',      \n",
      "                                                                  'tf.expand_dims_19[0][0]',      \n",
      "                                                                  'tf.expand_dims_20[0][0]',      \n",
      "                                                                  'tf.expand_dims_21[0][0]',      \n",
      "                                                                  'tf.expand_dims_22[0][0]',      \n",
      "                                                                  'tf.expand_dims_23[0][0]',      \n",
      "                                                                  'tf.expand_dims_24[0][0]',      \n",
      "                                                                  'tf.expand_dims_25[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None, 8)           0           ['tf.concat[0][0]']              \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_27 (TFOpLambda)  (None, 1, 8)        0           ['tf.math.reduce_sum_1[0][0]']   \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_1 (TFOp  (None, 8, 1)        0           ['tf.expand_dims_27[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 208)          0           ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLambda  (None, 8, 8)        0           ['tf.compat.v1.transpose_1[0][0]'\n",
      " )                                                               , 'tf.expand_dims_27[0][0]']     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 35)           7280        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " my_product_1 (my_product)      (None, 35)           2240        ['tf.linalg.matmul_1[0][0]']     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 35)           0           ['dense_5[0][0]',                \n",
      "                                                                  'my_product_1[0][0]']           \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 48)           0           ['add_1[0][0]',                  \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 1)            16641       ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,956,865\n",
      "Trainable params: 1,956,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Product 层\n",
    "mode = 'out' \n",
    "p_d = 35 # 设定 product层的输出维度\n",
    "embed_dims = 8\n",
    "\n",
    "\n",
    "# bias维度\n",
    "l_b = tf.Variable([[0]*p_d], dtype=tf.float32) # 注意, 必须使用两维来表示，使用一维无法与 l_z 和 l_p 进行 add\n",
    "# l_z 部分\n",
    "x = tf.keras.layers.Flatten()(sparse_embeds)\n",
    "l_z = tf.keras.layers.Dense(p_d, use_bias=False, kernel_regularizer=keras.regularizers.l2(.01))(x)  #[batch, p_d]\n",
    "# l_p 部分\n",
    "# tf.matmul 三维张量里面每一个对应矩阵做矩阵乘法\n",
    "if mode == 'in':\n",
    "    # 得到内积矩阵\n",
    "    p = tf.matmul(sparse_embeds, tf.transpose(sparse_embeds, perm=[0, 2, 1])) # [batch, fileds, fileds]\n",
    "else:\n",
    "    # 外积\n",
    "    # 得到外积矩阵,采用降维算法 先池化所有的样本的特征域得到一个embedding\n",
    "    tmp = tf.expand_dims(tf.reduce_sum(sparse_embeds, axis=1), axis=1) #[batch ,1, embed_dim]\n",
    "    p = tf.matmul(tf.transpose(tmp, perm=[0, 2, 1]), tmp) #[batch, embed_dim, embed_dim]\n",
    "    \n",
    "\n",
    "class my_product(keras.layers.Layer):\n",
    "    def __init__(self, p_d, mode='in', embed_dims=embed_dims):\n",
    "        self.p_d = p_d\n",
    "        self.mode = mode\n",
    "        self.embed_dims = embed_dims\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.mode == 'in':\n",
    "            # 对于每个product的输出节点都有一个 fileds * fileds 维的权重矩阵\n",
    "            self.w = self.add_weight(\n",
    "                shape=(input_shape[-1], input_shape[-1], self.p_d),\n",
    "                initializer=keras.initializers.random_normal(),\n",
    "                trainable=True\n",
    "            ) # [fileds, fileds, p_d]\n",
    "        else:\n",
    "            self.w = self.add_weight(\n",
    "            shape=(self.embed_dims, self.embed_dims, self.p_d),\n",
    "            initializer=keras.initializers.random_normal(),\n",
    "                trainable=True  \n",
    "            ) # [embed_dim, embed_dim, p_d]\n",
    "            \n",
    "        super().build(input_shape) # 调用父类方法\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # axes 指出使用 inputs的axis=0维度（不使用1，2），self.w axis=2维度（不使用0，1）做矩阵乘法，得到 shape=(dim_inputs_0, dim_self.w_2).\n",
    "        # 主要要求 inputs的axis(1, 2) 要和 self.w 的 axis(0, 1) 一致，实际上做的是矩阵内积和\n",
    "        return tf.tensordot(inputs, self.w, axes=[(1, 2), (0, 1)]) \n",
    "\n",
    "l_p = my_product(p_d, mode)(p)\n",
    "# product的l1层 l1 层做加法\n",
    "l1 = keras.layers.Add()([l_z, l_p, l_b])\n",
    "\n",
    "out = tf.concat([l1, dense_feat], axis=-1) # 与连续特征拼接\n",
    "outputs = dnn(out.shape[-1])(out)\n",
    "# l1.out_puts\n",
    "model = keras.Model(inputs=[dense_feat, sparse_feat], outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "dbbf18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 39s 14ms/step - loss: 0.3439 - auc_1: 0.9479 - val_loss: 0.6654 - val_auc_1: 0.6865\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.2323 - auc_1: 0.9571 - val_loss: 0.6787 - val_auc_1: 0.6864\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 35s 14ms/step - loss: 0.2170 - auc_1: 0.9614 - val_loss: 0.7147 - val_auc_1: 0.6809\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 35s 14ms/step - loss: 0.2053 - auc_1: 0.9650 - val_loss: 0.7505 - val_auc_1: 0.6754\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 35s 14ms/step - loss: 0.1971 - auc_1: 0.9673 - val_loss: 0.7675 - val_auc_1: 0.6721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x184f56abd30>"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(),\n",
    "             metrics=[keras.metrics.AUC()])\n",
    "\n",
    "model.fit(train[0], train[1], epochs=5, batch_size=32, validation_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad33f8",
   "metadata": {},
   "source": [
    "# wide & deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "6c722459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "pd.set_option('max_columns', 100)\n",
    "keras.backend.clear_session()\n",
    "# =============================== GPU ==============================\n",
    "# gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# print(gpu)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n",
    "# ========================= Hyper Parameters =======================\n",
    "# you can modify your file path\n",
    "file = '../data/criteo_sampled_data.csv'\n",
    "read_part = True\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "embed_dims = 8 # embedding 的维度\n",
    "df = pd.read_csv(file, nrows=4)\n",
    "df.head()\n",
    "# ========================== Create dataset =======================\n",
    "# train的结构是元组 ([dense_feature, sparse_feature], label_)\n",
    "\n",
    "feature_columns, train, test = create_criteo_dataset(file=file,\n",
    "                                       read_part=read_part,\n",
    "                                       sample_num=sample_num,\n",
    "                                       test_size=test_size,\n",
    "                                        embed_dim=embed_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "fd7cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    将输出线性组合为一个输出单元，不需要经过激活函数处理\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, reg_l1=0.01, **kwgs):\n",
    "        super().__init__(*args, **kwgs)\n",
    "        self.linear = keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(reg_l1), activation=None) # 使用正则化可以防止过拟合\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "\n",
    "\n",
    "class DNN(keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class WideDeep(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, activation='relu', dnn_dropout=0, embed_reg=1e-4, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {\n",
    "            'embed_' + str(idx): keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "            ) for idx, feat in enumerate(sparse_feature_array)\n",
    "        }\n",
    "        self.wide = Linear() # 线性部分 wide 部分\n",
    "        self.dnn = DNN(hidden_units, activation=activation, dropout=dnn_dropout, **kwgs)\n",
    "        self.final_deep = keras.layers.Dense(1, activation=None) # deep 部分的输出综合为一个节点\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        dense_feat, sparse_feat = inputs\n",
    "        # embed category\n",
    "        res = []\n",
    "        for i in range(sparse_feat.shape[1]):\n",
    "            embed_layer = self.sparse_embed_map['embed_'+str(i)]\n",
    "            res.append(embed_layer(sparse_feat[:,i])) # 输入embed的维度是 batch, sepquence_dim\n",
    "        # dnn 部分\n",
    "        embed_cat = tf.concat(res, axis=-1)\n",
    "        dnn_input = tf.concat([embed_cat, dense_feat], axis=-1)\n",
    "        deep_out = self.dnn(dnn_input) # 多层MLP的输出\n",
    "        deep_out = self.final_deep(deep_out)\n",
    "        \n",
    "        # wide 部分\n",
    "        wide_out = self.wide(dense_feat)\n",
    "        \n",
    "        # 综合方式\n",
    "        out = 0.5*(wide_out + deep_out)\n",
    "        return tf.nn.sigmoid(out)\n",
    "    \n",
    "    \n",
    "    def build_graph(self): # 构建静态图，放回模型\n",
    "        dense_inputs = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_inputs = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = keras.Model(inputs=[dense_inputs, sparse_inputs],\n",
    "                           outputs=self.call([dense_inputs, sparse_inputs]))\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "21529e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "141/141 [==============================] - 5s 21ms/step - loss: 1.9065 - auc: 0.5211 - val_loss: 0.6531 - val_auc: 0.6851\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5836 - auc: 0.6294 - val_loss: 0.5544 - val_auc: 0.6964\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 0.5481 - auc: 0.6845 - val_loss: 0.5405 - val_auc: 0.7081\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5379 - auc: 0.7073 - val_loss: 0.5352 - val_auc: 0.7185\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5324 - auc: 0.7198 - val_loss: 0.5301 - val_auc: 0.7262\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5283 - auc: 0.7289 - val_loss: 0.5286 - val_auc: 0.7294\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5236 - auc: 0.7417 - val_loss: 0.5284 - val_auc: 0.7339\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5197 - auc: 0.7526 - val_loss: 0.5285 - val_auc: 0.7383\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5166 - auc: 0.7639 - val_loss: 0.5297 - val_auc: 0.7413\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5124 - auc: 0.7826 - val_loss: 0.5354 - val_auc: 0.7415\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5350 - auc: 0.7325\n",
      "test AUC: 0.732550\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "# ============================Build Model==========================\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy() # 分布式训练策略\n",
    "# with mirrored_strategy.scope():\n",
    "wdl = WideDeep(feature_columns, hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = wdl.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e96de",
   "metadata": {},
   "source": [
    "### 不规范的写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "56c22960",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "# wide 部分\n",
    "inputs_wide = keras.Input(shape=(len(feature_columns[0]),)) # 目前假设以dense 特征作为 wide 部分，实际上wide部分需要较多的人工干预进行选择\n",
    "# 一般会使用 l1 正则控制 wide 部分的参数的稀疏性\n",
    "out_wide = keras.layers.Dense(1, activation=None)(inputs_wide) # Wide 部分的输出不需要经过激活函数\n",
    "\n",
    "wide = keras.Model(inputs=inputs_wide, outputs=out_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "4e81ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# deep 特征提取部分，输出为一个值\n",
    "def dnn(units_in_prdoucts = 256): \n",
    "    return keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(units_in_prdoucts,)),\n",
    "    *[ keras.layers.Dense(item, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)) for item in (256, 128, 64)]\n",
    "    ,keras.layers.Dropout(0.1)\n",
    "    ,keras.layers.Dense(1, activation=None) # Deep 部分的输出不需要经过激活函数\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# deep 部分\n",
    "inputs_deep = keras.Input(shape=(len(feature_columns[1]),))\n",
    "# embeding for categories\n",
    "res = []\n",
    "for i, dic_ in enumerate(feature_columns[1]):\n",
    "    embed = keras.layers.Embedding(dic_['feat_num'], embed_dims)\n",
    "    res.append(embed(inputs_deep[:, i]))\n",
    "# concat embedding vector\n",
    "dense_embed = tf.concat(res, axis=-1)\n",
    "deep_feat = tf.concat([dense_embed, inputs_wide], axis=-1) # 拼接deep的输入， embedding + dense_feature\n",
    "input_shape = dense_embed.shape[-1]\n",
    "dnn_network = dnn(input_shape)\n",
    "out = dnn_network(dense_embed)\n",
    "\n",
    "deep = keras.Model(inputs=inputs_deep, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "b3951ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_layer = keras.layers.Dense(1, use_bias=False, activation='sigmoid') # 细节部分不适用bias\n",
    "out_finall = final_layer(0.5*(deep(inputs_deep)+wide(inputs_wide)))\n",
    "wide_deep = keras.Model(inputs=[inputs_wide,inputs_deep], outputs=out_finall) # 函数 api 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "2942383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 8s 19ms/step - loss: 1.1338 - auc: 0.5997 - val_loss: 0.5225 - val_auc: 0.7037\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.4945 - auc: 0.7777 - val_loss: 0.5075 - val_auc: 0.7307\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.4127 - auc: 0.8870 - val_loss: 0.5484 - val_auc: 0.7090\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.3360 - auc: 0.9340 - val_loss: 0.5966 - val_auc: 0.7004\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.2894 - auc: 0.9513 - val_loss: 0.6037 - val_auc: 0.6988\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2607 - auc: 0.9599 - val_loss: 0.6416 - val_auc: 0.6909\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2410 - auc: 0.9652 - val_loss: 0.6476 - val_auc: 0.6817\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.2272 - auc: 0.9685 - val_loss: 0.6693 - val_auc: 0.6854\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2159 - auc: 0.9712 - val_loss: 0.6802 - val_auc: 0.6807\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2079 - auc: 0.9729 - val_loss: 0.7065 - val_auc: 0.6800\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.2011 - auc: 0.9743 - val_loss: 0.7294 - val_auc: 0.6775\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1957 - auc: 0.9754 - val_loss: 0.7491 - val_auc: 0.6701\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1905 - auc: 0.9764 - val_loss: 0.7029 - val_auc: 0.6698\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1864 - auc: 0.9770 - val_loss: 0.7596 - val_auc: 0.6692\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1824 - auc: 0.9779 - val_loss: 0.7742 - val_auc: 0.6699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x184ffa3f490>"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wide_deep\n",
    "# model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                 optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                 metrics=[keras.metrics.AUC()])\n",
    "\n",
    "model.fit(train[0], train[1], epochs=15, batch_size=256, validation_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf71ee",
   "metadata": {},
   "source": [
    "# Deep&Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "865c2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0.1, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Cross(keras.layers.Layer):\n",
    "    def __init__(self, layer_nums, input_length):\n",
    "        \"\"\"\n",
    "        layer_nums : cross network的层数\n",
    "        input_length: cross network的输入维度 \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_nums = layer_nums\n",
    "        self.units = input_length\n",
    "    \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"设置参数共享变量\"\"\"\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "        for i in range(self.layer_nums):\n",
    "            w, b = 'w_' + str(i), 'b_' + str(i) \n",
    "            self.w[w] = self.add_weight(shape=(self.units, 1), # 维度为 [n, 1]\n",
    "                                             initializer=keras.initializers.glorot_normal(),\n",
    "                                             trainable=True)\n",
    "            self.b[b] = self.add_weight(shape=(self.units, 1),\n",
    "                                             initializer=keras.initializers.glorot_normal(),\n",
    "                                             trainable=True)\n",
    "        super().build(input_shape) # 调用父类方法\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs #[batch, n]\n",
    "        x = tf.expand_dims(x, axis=-1) # [batch, n, 1]\n",
    "        for i in range(self.layer_nums):\n",
    "            w, b = 'w_' + str(i), 'b_' + str(i)\n",
    "            pre = tf.identity(x) # 复制 x 一份\n",
    "            x_t = tf.transpose(x, perm=[0, 2, 1]) # [batch, 1, n]\n",
    "            x = tf.matmul(x, x_t) # [batch, n, n]\n",
    "            x = tf.matmul(x, self.w[w]) # [batch, n, 1] # self.w[w] 是二维的，但是matmul支持广播运行\n",
    "            x += self.b[b] + pre # [batch, n, 1]\n",
    "            \n",
    "#         print(x.shape, type(tf.squeeze(x)), tf.squeeze(x).shape)\n",
    "        # 需要注意的是 使用 squeeze 指名将最后一个维度去除， 如果不指名具体的axis的话，最后squeeze得到的shape为 unknow, 主要是因为 shape 里面存在 None 表示不定长导致的\n",
    "        # 所以当在遇见 shape 里面含有 None 的时候，降维需要指定具体的axis\n",
    "        return tf.squeeze(x, axis=-1) # [batch, n]\n",
    "        \n",
    "\n",
    "class CrossDeep(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, cross_layers=2, activation='relu', dnn_dropout=0, embed_reg=1e-4, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        self.tot_dims = len(dense_feature_array) # 计数的是dense特征 + embedding 维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {}\n",
    "        for idx, feat in enumerate(sparse_feature_array):\n",
    "            key = 'embed_' + str(idx)\n",
    "            self.sparse_embed_map[key] = \\\n",
    "                keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "                                    )\n",
    "            self.tot_dims += feat['embed_dim']\n",
    "        # 交叉网络 cross\n",
    "        self.crossnet = Cross(cross_layers, self.tot_dims)\n",
    "        # deep部分\n",
    "        self.deepnet = DNN(hidden_units, activation=activation, dropout=dnn_dropout, **kwgs)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_input, sparse_input = inputs\n",
    "        # 处理 sparse feature\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            embed_layer = self.sparse_embed_map['embed_'+str(i)]\n",
    "            res.append(embed_layer(sparse_input[:, i]))\n",
    "        embed_feat = tf.concat(res, axis=-1) # 得到稠密的embeding向量\n",
    "        feats = tf.concat([embed_feat, dense_input], axis=-1) # 得到cross net 的输入向量\n",
    "        \n",
    "        # cross\n",
    "        cross_out = self.crossnet(feats)\n",
    "        \n",
    "        # deep\n",
    "        deep_out = self.deepnet(feats)\n",
    "        \n",
    "        # 最终特征\n",
    "        x = tf.concat([cross_out, deep_out], axis=-1)\n",
    "        final_layer = keras.layers.Dense(1, activation='sigmoid') # 开启使用 bias\n",
    "        return final_layer(x)\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        dense_input = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_input = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = keras.Model(inputs=[dense_input, sparse_input],\n",
    "                           outputs=self.call([dense_input, sparse_input]))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "d3b78570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "141/141 [==============================] - 33s 219ms/step - loss: 1.8291 - auc: 0.6336 - val_loss: 0.5700 - val_auc: 0.7308\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 32s 227ms/step - loss: 0.4842 - auc: 0.7767 - val_loss: 0.4956 - val_auc: 0.7423\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 31s 223ms/step - loss: 0.4024 - auc: 0.8640 - val_loss: 0.5480 - val_auc: 0.7236\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 31s 223ms/step - loss: 0.3039 - auc: 0.9335 - val_loss: 0.6283 - val_auc: 0.7101\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 31s 222ms/step - loss: 0.2531 - auc: 0.9564 - val_loss: 0.7194 - val_auc: 0.6949\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 0.7222 - auc: 0.6854\n",
      "test AUC: 0.685433\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "cross_layers=3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "# ============================Build Model==========================\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy() # 分布式训练策略\n",
    "# with mirrored_strategy.scope():\n",
    "cdl = CrossDeep(feature_columns, cross_layers=cross_layers, hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = cdl.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066aa742",
   "metadata": {},
   "source": [
    "# XDeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14b344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "\n",
    "class DNN(keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0.1, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class CIN(keras.layers.Layer):\n",
    "    def __init__(self, layer_sizes, l2_reg=0.0001):\n",
    "        \"\"\"\n",
    "        layer_nums : cross network的层数\n",
    "        input_length: cross network的输入维度 \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.l2_reg = l2_reg\n",
    "        self.w = {}\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"设置参数共享变量\"\"\"\n",
    "        x0 = input_shape[1]\n",
    "        self.hidden_dims = [x0] + list(self.layer_sizes)\n",
    "        for i in range(1, len(self.hidden_dims)):\n",
    "            self.w['cin_w_'+str(i)] = self.add_weight(\n",
    "                name='cin_w_'+str(i),\n",
    "                shape=(1, self.hidden_dims[0]*self.hidden_dims[i-1], self.hidden_dims[i]), # shape [filter_height, filter_width, n_filters]\n",
    "                initializer=keras.initializers.random_uniform,\n",
    "                regularizer=keras.regularizers.l2(self.l2_reg),\n",
    "                trainable=True\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        输入的是稀疏的类别变量经过embedding的结果\n",
    "        inputs： [batch, fileds, embedding]\n",
    "        \"\"\"\n",
    "        embed_dims = inputs.shape[-1]\n",
    "        x = inputs\n",
    "        # 在 embedding 维度进行切割，好统计每一个embedding的子维度下，任意两个embedding的乘积情况\n",
    "        split_x0 = tf.split(x, embed_dims, 2) # 放回的是一个列表，共embed_dims个元素，每个元素是张量为[batch, fileds0, 1]\n",
    "\n",
    "        layers_out = [x] # last_output[i] 代表第 i 层CIN的输出\n",
    "        for i in range(1, len(self.hidden_dims)):\n",
    "            # 获取上一层的输出\n",
    "            split_xk = tf.split(layers_out[-1], embed_dims, 2) # embed_dims个元素，每个张量为 [batch, h_i_1, 1]\n",
    "            # tf.matmul 只作用在张量的最后两维上进行矩阵乘法\n",
    "            out_ = tf.matmul(split_x0, split_xk, transpose_b=True) # [embed_dims, batch, fileds0, h_(i-1)]\n",
    "            out_ = tf.reshape(out_, shape=(embed_dims, -1, out_.shape[-1]*out_.shape[-2])) # [embed_dims, batch, fileds0 * h_(i-1)]\n",
    "            out_ = tf.transpose(out_, perm=[1, 0, 2]) # [batch, embed_dims, fileds0 * h_(i-1)]\n",
    "            \n",
    "#             print(out_.shape, self.w['cin_w_%d'%i].shape)\n",
    "            # 由卷积公式得到第 i层的输出\n",
    "            after_conv = tf.nn.conv1d(input=out_, filters=self.w['cin_w_%d'%i], stride=1, padding='VALID') # [batch, embed_dims, n_filters]\n",
    "            #调整shape的顺序\n",
    "            out_ = tf.transpose(after_conv, perm=[0, 2, 1]) # [batch, n_filters, embed_dims]\n",
    "            layers_out.append(out_)\n",
    "        \n",
    "        \n",
    "        # 模拟 rnn 进行序列池化\n",
    "        out_ = tf.concat(layers_out[1:], axis=1) # [batch, h1+h2+..hk, embed_dims]\n",
    "        out_ = tf.reduce_sum(out_, axis=-1, keepdims=False) # [batch, h1+h2+...+hk]\n",
    "        return out_\n",
    "            \n",
    "        \n",
    "\n",
    "class XDeepFM(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, cin_layer_size=(128, 56), activation='relu', dnn_dropout=0, embed_reg=1e-4, cin_l2_reg=1e-4, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        self.tot_dims = len(dense_feature_array) # 计数的是dense特征 + embedding 维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {}\n",
    "        for idx, feat in enumerate(sparse_feature_array):\n",
    "            key = 'embed_' + str(idx)\n",
    "            self.sparse_embed_map[key] = \\\n",
    "                keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "                                    )\n",
    "            self.tot_dims += feat['embed_dim']\n",
    "        # Linear\n",
    "        self.linear = keras.layers.Dense(1, name='linear')\n",
    "       # CIN\n",
    "        self.cin = CIN(cin_layer_size, cin_l2_reg)\n",
    "        self.cin_linear = keras.layers.Dense(1, name='cin_linear')\n",
    "        # deep部分\n",
    "        self.deepnet = DNN(hidden_units, activation=activation, dropout=dnn_dropout, **kwgs)\n",
    "        self.dnn_linear = keras.layers.Dense(1, name='dnn_linear')\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_input, sparse_input = inputs\n",
    "        # 处理 sparse feature\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            embed_layer = self.sparse_embed_map['embed_'+str(i)]\n",
    "            res.append(embed_layer(sparse_input[:, i]))\n",
    "        embed_feat = tf.concat(res, axis=-1) # 得到稠密的embeding向量\n",
    "        feats = tf.concat([embed_feat, dense_input], axis=-1) # 得到cross net 的输入向量\n",
    "        \n",
    "        # Linear\n",
    "        # 目前为了简单起见我这里只考虑了 dense 特征\n",
    "        linear_logit = self.linear(dense_input)\n",
    "\n",
    "        # deep\n",
    "        deep_out = self.deepnet(feats)\n",
    "        dnn_logits = self.dnn_linear(deep_out)\n",
    "        \n",
    "        # CIN, 与 DNN 共享 embedding\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            embed_layer = self.sparse_embed_map['embed_'+str(i)]\n",
    "            res.append(embed_layer(sparse_input[:, i])) # [batch, embedding_dims]\n",
    "        \n",
    "        print(res[-1].shape)\n",
    "#         assert res[-1].ndim == 2\n",
    "        cin_input = tf.stack(res, axis=1)\n",
    "        cin_out = self.cin(cin_input)\n",
    "        cin_logits = self.cin_linear(cin_out)\n",
    "        \n",
    "        tot = keras.layers.Concatenate()([linear_logit, dnn_logits, cin_logits])\n",
    "        finlear = keras.layers.Dense(1, use_bias=False)\n",
    "        return tf.nn.sigmoid(finlear(tot))\n",
    "        \n",
    "    \n",
    "    def build_graph(self):\n",
    "        dense_input = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_input = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = keras.Model(inputs=[dense_input, sparse_input],\n",
    "                           outputs=self.call([dense_input, sparse_input]))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfe03ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8)\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 22s 141ms/step - loss: 1.8504 - auc: 0.5995 - val_loss: 0.5791 - val_auc: 0.7222\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 20s 145ms/step - loss: 0.4899 - auc: 0.7807 - val_loss: 0.5072 - val_auc: 0.7350\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 21s 147ms/step - loss: 0.4046 - auc: 0.8724 - val_loss: 0.5562 - val_auc: 0.7182\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 21s 149ms/step - loss: 0.3096 - auc: 0.9354 - val_loss: 0.6410 - val_auc: 0.6971\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 21s 149ms/step - loss: 0.2630 - auc: 0.9565 - val_loss: 0.6858 - val_auc: 0.6936\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.6700 - auc: 0.6973\n",
      "test AUC: 0.697315\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "cross_layers=3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "# ============================Build Model==========================\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy() # 分布式训练策略\n",
    "# with mirrored_strategy.scope():\n",
    "xdfm = XDeepFM(feature_columns, cin_layer_size=(128, 56), hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = xdfm.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510c567",
   "metadata": {},
   "source": [
    "# DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "68a6c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideFM(keras.Model):\n",
    "    \"\"\"\n",
    "    wide part\n",
    "    实现的是一个应用在deepFM框架中的FM部分，与传统FM最大的不同是不需要分解得到特征隐向量\n",
    "    计算二阶交互的时候直接将embedding向量作为原始特征隐向量计算内积即可\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dims):\n",
    "        # 一阶线性转换权重\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(feat_dims, 1),\n",
    "                                initializer=keras.initializers.glorot_normal(),\n",
    "                                trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs_1: dense_feature + embed_feat for calculate factor 1 #[batch, feat_dims] \n",
    "        inputs_2: embed_feat for calculate factor 2 [batch, fileds, embeding]\n",
    "        \"\"\"\n",
    "        inputs1, inputs2 = inputs\n",
    "        factor1 = tf.matmul(inputs1, self.w) # no bias [batch, 1]\n",
    "        \n",
    "        square_sum = tf.pow(tf.reduce_sum(inputs2, axis=1, keepdims=False), 2) # [batch,embedding]\n",
    "        sum_square = tf.reduce_sum(tf.pow(inputs2, 2), axis=1, keepdims=False) # [batch, embedding]\n",
    "        factor2 = 0.5*tf.reduce_sum(square_sum - sum_square, axis=1, keepdims=True) # [batch, 1]\n",
    "#         print(inputs1.shape, inputs2.shape, factor1.shape, square_sum.shape, sum_square.shape, factor2.shape)\n",
    "        return factor1 + factor2\n",
    "\n",
    "\n",
    "class DNN(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    deep 部分\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0.1, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DeepFM(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, activation='relu', dnn_dropout=0.1, reg_l2=0.01, embed_reg=0.01, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        self.tot_dims = len(dense_feature_array) # 计数的是dense特征 + embedding 维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {}\n",
    "        for idx, feat in enumerate(sparse_feature_array):\n",
    "            key = 'embed_' + str(idx)\n",
    "            self.embed_dims = feat['embed_dim'] # 稀疏特征的 embedding 维度\n",
    "            self.sparse_embed_map[key] = \\\n",
    "                keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "                                    )\n",
    "            self.tot_dims += feat['embed_dim']\n",
    "            \n",
    "        # wide部分\n",
    "        self.fm = WideFM(self.tot_dims)\n",
    "        \n",
    "        # deep部分\n",
    "        self.dnn_network = DNN(hidden_units, activation, dnn_dropout, reg_l2)\n",
    "        self.final_linear = keras.layers.Dense(1, activation=None) # 不使用偏执项\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_input, sparse_input = inputs\n",
    "        # 处理 sparse feature\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            embed_layer = self.sparse_embed_map['embed_'+str(i)]\n",
    "            res.append(embed_layer(sparse_input[:, i]))\n",
    "        embed_feat = tf.concat(res, axis=-1) # 得到稠密的embeding向量\n",
    "        feats = tf.concat([embed_feat, dense_input], axis=-1) # 得到稠密的输入向量\n",
    "        second_input = tf.reshape(embed_feat, (-1, sparse_input.shape[1], self.embed_dims)) # 将类别的embeding tensor转换为三维 [batch, fileds, embed_dims]\n",
    "        \n",
    "#         print('first shape', feats.shape, 'second shape', second_input.shape)\n",
    "        # wide\n",
    "        out_fm = self.fm([feats, second_input])\n",
    "#         print('out_fm shape', out_fm.shape)\n",
    "        # deep部分\n",
    "        out_deep = self.dnn_network(feats)\n",
    "        out_deep = self.final_linear(out_deep)\n",
    "#         print('out_deep shape', out_deep.shape)\n",
    "        return tf.nn.sigmoid(out_deep + out_fm)\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        dense_input = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_input = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = tf.keras.Model(inputs=[dense_input, sparse_input],\n",
    "                              outputs=self.call([dense_input, sparse_input]))\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "7d7e097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "141/141 [==============================] - 5s 21ms/step - loss: 3.4459 - auc: 0.5525 - val_loss: 0.6164 - val_auc: 0.6715\n",
      "Epoch 2/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5508 - auc: 0.6761 - val_loss: 0.5310 - val_auc: 0.6950\n",
      "Epoch 3/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5288 - auc: 0.6926 - val_loss: 0.5277 - val_auc: 0.7032\n",
      "Epoch 4/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5269 - auc: 0.6992 - val_loss: 0.5276 - val_auc: 0.7080\n",
      "Epoch 5/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5267 - auc: 0.7040 - val_loss: 0.5275 - val_auc: 0.7117\n",
      "Epoch 6/15\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.5265 - auc: 0.7076 - val_loss: 0.5281 - val_auc: 0.7104\n",
      "Epoch 7/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5267 - auc: 0.7091 - val_loss: 0.5273 - val_auc: 0.7153\n",
      "Epoch 8/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5272 - auc: 0.7109 - val_loss: 0.5276 - val_auc: 0.7173\n",
      "Epoch 9/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5269 - auc: 0.7139 - val_loss: 0.5276 - val_auc: 0.7200\n",
      "Epoch 10/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5270 - auc: 0.7163 - val_loss: 0.5309 - val_auc: 0.7168\n",
      "Epoch 11/15\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.5282 - auc: 0.7173 - val_loss: 0.5294 - val_auc: 0.7199\n",
      "Epoch 12/15\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.5285 - auc: 0.7180 - val_loss: 0.5311 - val_auc: 0.7184\n",
      "Epoch 13/15\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.5286 - auc: 0.7195 - val_loss: 0.5323 - val_auc: 0.7217\n",
      "Epoch 14/15\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.5285 - auc: 0.7210 - val_loss: 0.5326 - val_auc: 0.7178\n",
      "Epoch 15/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5293 - auc: 0.7214 - val_loss: 0.5300 - val_auc: 0.7263\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5301 - auc: 0.7108\n",
      "test AUC: 0.710823\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "cross_layers=3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 15\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "# ============================Build Model==========================\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy() # 分布式训练策略\n",
    "# with mirrored_strategy.scope():\n",
    "dfm = DeepFM(feature_columns, hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = dfm.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2ccfa",
   "metadata": {},
   "source": [
    "# NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "1a23b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    deep 部分\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0.1, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class BiInteractionPoolling(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x):\n",
    "        # x shape = [batch, fileds, embed_dims]\n",
    "        square_sum = tf.pow(tf.reduce_sum(x, axis=1), 2) # [batch, embed_dims]\n",
    "        sum_square = tf.reduce_sum(tf.pow(x, 2), axis=1) #[batch, embed_dims]\n",
    "        return 0.5*(square_sum - sum_square)\n",
    "\n",
    "    \n",
    "class NFM(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, activation='relu', dnn_dropout=0.1, reg_l2=0.01, embed_reg=0.01, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        self.tot_dims = len(dense_feature_array) # 计数的是dense特征 + embedding 维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {}\n",
    "        for idx, feat in enumerate(sparse_feature_array):\n",
    "            key = 'embed_' + str(idx)\n",
    "            self.embed_dims = feat['embed_dim'] # 稀疏特征的 embedding 维度\n",
    "            self.sparse_embed_map[key] = \\\n",
    "                keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "                                    )\n",
    "            self.tot_dims += feat['embed_dim']\n",
    "        \n",
    "        # 低阶的特征整合\n",
    "        self.first_linear = keras.layers.Dense(1, activation=None)\n",
    "        \n",
    "        # 高阶部分\n",
    "        # 特征交互层\n",
    "        self.bi = BiInteractionPoolling()\n",
    "        # DNN\n",
    "        self.dnn = DNN(hidden_units, activation, dnn_dropout, reg_l2)\n",
    "        # BatchNormalization\n",
    "        self.bn = keras.layers.BatchNormalization() # 作用在DNN网络的输入一端\n",
    "        # dnn linear\n",
    "        self.dnn_linear = keras.layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_input, sparse_input = inputs\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            key = 'embed_' + str(i)\n",
    "            embed = self.sparse_embed_map[key]\n",
    "            res.append(embed(sparse_input[:,i:i+1]))\n",
    "        embed_feats = tf.concat(res, axis=1)  # [batch, fileds, embed_dims]\n",
    "        \n",
    "        # 一阶信息\n",
    "        into_first = tf.reshape(embed_feats, (-1, embed_feats.shape[-1]*embed_feats.shape[1]))\n",
    "        out_first = self.first_linear(into_first)\n",
    "        \n",
    "        # 二阶信息的提取\n",
    "        # bi-interaction\n",
    "        out_bi = self.bi(embed_feats) # [batch, embed_dims]\n",
    "        # concat dense_feature\n",
    "        into_dnn = tf.concat([out_bi, dense_input], axis=-1) #[batch, feats]\n",
    "        # batchnormalize\n",
    "        into_dnn = self.bn(into_dnn)\n",
    "        out_dnn = self.dnn(into_dnn)\n",
    "        out_dnn = self.dnn_linear(out_dnn) # [batch, 1]\n",
    "        \n",
    "        return tf.nn.sigmoid(out_first + out_dnn)\n",
    "    \n",
    "    \n",
    "    def build_graph(self):\n",
    "        dense_input = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_input = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = keras.Model(inputs=[dense_input, sparse_input], outputs=self.call([dense_input, sparse_input]))\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "f2bb2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "141/141 [==============================] - 5s 22ms/step - loss: 3.0375 - auc: 0.6382 - val_loss: 0.6702 - val_auc: 0.7002\n",
      "Epoch 2/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5715 - auc: 0.7205 - val_loss: 0.5787 - val_auc: 0.7269\n",
      "Epoch 3/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5482 - auc: 0.7310 - val_loss: 0.5665 - val_auc: 0.7320\n",
      "Epoch 4/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5432 - auc: 0.7391 - val_loss: 0.5455 - val_auc: 0.7376\n",
      "Epoch 5/15\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.5390 - auc: 0.7475 - val_loss: 0.5435 - val_auc: 0.7412\n",
      "Epoch 6/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5377 - auc: 0.7534 - val_loss: 0.5460 - val_auc: 0.7347\n",
      "Epoch 7/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5346 - auc: 0.7619 - val_loss: 0.5443 - val_auc: 0.7398\n",
      "Epoch 8/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5346 - auc: 0.7677 - val_loss: 0.5409 - val_auc: 0.7417\n",
      "Epoch 9/15\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.5315 - auc: 0.7723 - val_loss: 0.5490 - val_auc: 0.7386\n",
      "Epoch 10/15\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.5319 - auc: 0.7795 - val_loss: 0.5515 - val_auc: 0.7362\n",
      "Epoch 11/15\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.5302 - auc: 0.7833 - val_loss: 0.5560 - val_auc: 0.7406\n",
      "Epoch 12/15\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.5289 - auc: 0.7885 - val_loss: 0.5592 - val_auc: 0.7349\n",
      "Epoch 13/15\n",
      "141/141 [==============================] - 4s 25ms/step - loss: 0.5285 - auc: 0.7941 - val_loss: 0.5600 - val_auc: 0.7352\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5400 - auc: 0.7325\n",
      "test AUC: 0.732507\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "cross_layers=3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 15\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "\n",
    "nfm = NFM(feature_columns, hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = nfm.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe78a3",
   "metadata": {},
   "source": [
    "# AFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "f88218d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class DNN(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    deep 部分\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='relu', dropout=0.1, reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(keras.Model):\n",
    "    def __init__(self, k, embed_dims):\n",
    "        \"\"\"\n",
    "        k 表示的是attention层的隐藏神经元个数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(embed_dims, k),\n",
    "                                initializer=keras.initializers.glorot_normal(),\n",
    "                                trainable=True)\n",
    "        self.bias = self.add_weight(shape=(k, ),\n",
    "                                initializer=keras.initializers.glorot_normal(),\n",
    "                                trainable=True)\n",
    "        self.h = self.add_weight(shape=(k, 1),\n",
    "                                initializer=keras.initializers.glorot_normal(),\n",
    "                                trainable=True)\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        输入 x 的shape为 [batch, (fileds*(fileds-1)/2), embed_dims]\n",
    "        \"\"\"\n",
    "        a = tf.matmul(x, self.w) + self.bias # [batch, (fileds*(fileds-1)/2), k]\n",
    "        a = tf.nn.relu(a) # [batch, (fileds*(fileds-1)/2), k]\n",
    "        a = tf.matmul(a, self.h) # # [batch, (fileds*(fileds-1)/2), 1] \n",
    "        \n",
    "        # 添加权重\n",
    "        x = x * a # 元素积 # [batch, (fileds*(fileds-1)/2), embed_dims]\n",
    "        # pooling\n",
    "        return tf.reduce_sum(x, axis=1) # [batch, embed_dims] 使用 add 池化\n",
    "\n",
    "\n",
    "class AFM(keras.Model):\n",
    "    def __init__(self, feature_column, hidden_units, attention_dim=20, activation='relu', dnn_dropout=0.1, reg_l2=0.01, embed_reg=0.01, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        dense_feature_array, sparse_feature_array = feature_column\n",
    "        self.dense_dim, self.sparse_dim = len(dense_feature_array), len(sparse_feature_array) # 特征维度\n",
    "        self.tot_dims = len(dense_feature_array) # 计数的是dense特征 + embedding 维度\n",
    "        # embed_map\n",
    "        self.sparse_embed_map = {}\n",
    "        for idx, feat in enumerate(sparse_feature_array):\n",
    "            key = 'embed_' + str(idx)\n",
    "            self.embed_dims = feat['embed_dim'] # 稀疏特征的 embedding 维度\n",
    "            self.sparse_embed_map[key] = \\\n",
    "                keras.layers.Embedding(\n",
    "                                         input_dim=feat['feat_num'],\n",
    "                                         input_length=1, # 指名进行 embeding 的序列长度\n",
    "                                         output_dim=feat['embed_dim'],\n",
    "                                         embeddings_initializer='random_uniform',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(embed_reg)\n",
    "                                    )\n",
    "            self.tot_dims += feat['embed_dim']\n",
    "        \n",
    "        # 低阶的特征整合\n",
    "        self.first_linear = keras.layers.Dense(1, activation=None)\n",
    "        \n",
    "        # attention layer\n",
    "        self.att = AttentionLayer(attention_dim, self.embed_dims)\n",
    "        \n",
    "        # dnn \n",
    "        self.dnn = DNN(hidden_units, activation, dnn_dropout, reg_l2)\n",
    "        # bn 层\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        # dnn_linear\n",
    "        self.dnn_linear = keras.layers.Dense(1, activation=None)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_input, sparse_input = inputs\n",
    "        res = []\n",
    "        for i in range(sparse_input.shape[1]):\n",
    "            key = 'embed_' + str(i)\n",
    "            embed = self.sparse_embed_map[key]\n",
    "            res.append(embed(sparse_input[:,i:i+1]))\n",
    "        embed_feats = tf.concat(res, axis=1)  # [batch, fileds, embed_dims]\n",
    "        \n",
    "        # 一阶信息\n",
    "        into_first = tf.reshape(embed_feats, (-1, embed_feats.shape[-1]*embed_feats.shape[1]))\n",
    "        out_first = self.first_linear(into_first)\n",
    "        \n",
    "        # 二阶需要先计算两两embedding元素积的结果\n",
    "        left = [] # 记录的是元素积的左部分embedding向量\n",
    "        right = []\n",
    "        for i, j in itertools.combinations(range(sparse_input.shape[1]), 2): # 得到元素积索引组合的情况\n",
    "            left.append(i)\n",
    "            right.append(j)\n",
    "        # 采用切片的手法，来得到新的索引\n",
    "        l = tf.gather(embed_feats, left, axis=1) # [batch, (fileds*(fileds-1)/2), embed_dims]\n",
    "        r = tf.gather(embed_feats, right, axis=1) # [batch, (fileds*(fileds-1)/2), embed_dims]\n",
    "        # 使用张量计算直接得到fileds*(fileds-1)/2个元素积\n",
    "        into_second = l * r # [batch, (fileds*(fileds-1)/2), embed_dims]\n",
    "        into_dnn = self.att(into_second) # [batch, embed_dims]\n",
    "        # 与连续特征做拼接\n",
    "        into_dnn = tf.concat([into_dnn, dense_input], axis=-1)\n",
    "        # 进入 bn 层防止过拟合\n",
    "        into_dnn = self.bn(into_dnn)\n",
    "        x = self.dnn(into_dnn)\n",
    "        out_second = self.dnn_linear(x)\n",
    "        \n",
    "        # 苹姐一阶和高阶的输出\n",
    "        return tf.nn.sigmoid(out_first + out_second)\n",
    "\n",
    "    \n",
    "    def build_graph(self):\n",
    "        dense_input = keras.Input(shape=(self.dense_dim,), dtype=tf.float32)\n",
    "        sparse_input = keras.Input(shape=(self.sparse_dim,), dtype=tf.float32)\n",
    "        model = keras.Model(inputs=[dense_input, sparse_input], outputs=self.call([dense_input, sparse_input]))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "f2cc1407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "141/141 [==============================] - 12s 63ms/step - loss: 3.0184 - auc: 0.5809 - val_loss: 0.6360 - val_auc: 0.6819\n",
      "Epoch 2/15\n",
      "141/141 [==============================] - 9s 60ms/step - loss: 0.5603 - auc: 0.7129 - val_loss: 0.5785 - val_auc: 0.7247\n",
      "Epoch 3/15\n",
      "141/141 [==============================] - 9s 62ms/step - loss: 0.5410 - auc: 0.7391 - val_loss: 0.5625 - val_auc: 0.7341\n",
      "Epoch 4/15\n",
      "141/141 [==============================] - 9s 64ms/step - loss: 0.5391 - auc: 0.7526 - val_loss: 0.5486 - val_auc: 0.7382\n",
      "Epoch 5/15\n",
      "141/141 [==============================] - 9s 67ms/step - loss: 0.5400 - auc: 0.7617 - val_loss: 0.5443 - val_auc: 0.7416\n",
      "Epoch 6/15\n",
      "141/141 [==============================] - 9s 66ms/step - loss: 0.5383 - auc: 0.7720 - val_loss: 0.5505 - val_auc: 0.7402\n",
      "Epoch 7/15\n",
      "141/141 [==============================] - 9s 66ms/step - loss: 0.5390 - auc: 0.7774 - val_loss: 0.5528 - val_auc: 0.7390\n",
      "Epoch 8/15\n",
      "141/141 [==============================] - 9s 67ms/step - loss: 0.5411 - auc: 0.7844 - val_loss: 0.5556 - val_auc: 0.7421\n",
      "Epoch 9/15\n",
      "141/141 [==============================] - 10s 71ms/step - loss: 0.5380 - auc: 0.7904 - val_loss: 0.5646 - val_auc: 0.7416\n",
      "Epoch 10/15\n",
      "141/141 [==============================] - 10s 68ms/step - loss: 0.5363 - auc: 0.7969 - val_loss: 0.5646 - val_auc: 0.7366\n",
      "Epoch 11/15\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.5348 - auc: 0.8030 - val_loss: 0.5711 - val_auc: 0.7331\n",
      "Epoch 12/15\n",
      "141/141 [==============================] - 10s 73ms/step - loss: 0.5341 - auc: 0.8070 - val_loss: 0.5742 - val_auc: 0.7352\n",
      "Epoch 13/15\n",
      "141/141 [==============================] - 10s 69ms/step - loss: 0.5294 - auc: 0.8127 - val_loss: 0.5784 - val_auc: 0.7374\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.5565 - auc: 0.7300\n",
      "test AUC: 0.729962\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "sample_num = 100000\n",
    "test_size = 0.2\n",
    "\n",
    "embed_dim = 8\n",
    "dnn_dropout = 0.5\n",
    "hidden_units = [256, 128, 64]\n",
    "cross_layers=3\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 15\n",
    "    \n",
    "train_X, train_y = train\n",
    "test_X, test_y = test\n",
    "\n",
    "afm = AFM(feature_columns, hidden_units=hidden_units, dnn_dropout=dnn_dropout)\n",
    "model = afm.build_graph()\n",
    "# ============================Compile============================\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")\n",
    "# ===========================Test==============================\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6390ccb",
   "metadata": {},
   "source": [
    "# DIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ab421",
   "metadata": {},
   "source": [
    "### DIN初探，基于伪造的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "db6d6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from keras import layers\n",
    "\n",
    "# 构建DIN模型\n",
    "\n",
    "# Dice自适应激活函数\n",
    "class Dice(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = self.add_weight(shape=(), initializer=keras.initializers.Zeros(), dtype=tf.float32)\n",
    "        self.bn = layers.BatchNormalization(center=False, scale=False, trainable=True) # 只使用BN的归一化，不学习移动和缩放参数\n",
    "\n",
    "    def call(self, x):\n",
    "        p = tf.nn.sigmoid(self.bn(x))\n",
    "        return p*x + (1-p)*self.alpha*x\n",
    "\n",
    "\n",
    "class ActivationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    注意力层，用于加权融合用户历史行为序列\n",
    "    \"\"\"\n",
    "    def __init__(self, att_hidden_units, activation='prelu'):\n",
    "        super().__init__()\n",
    "        self.att_dense = [layers.Dense(unit, activation=Dice() if ffn_activation != 'prelu' else layers.PReLU()) for unit in att_hidden_units]\n",
    "        self.att_final_dense = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        \"\"\"\n",
    "        inputs [query_items, hist_items, mask]\n",
    "        quer_items : [batch, items_embedding_dims]\n",
    "        hist_items: [batch, seq_len, items_embedding_dims]\n",
    "        mask : [batch, seq_len] 因为用户序列是不定长的所以使用mask掩掉那些填充的序列维度\n",
    "        \"\"\"\n",
    "        q, k, mask = inputs\n",
    "        val = tf.identity(k) # copy\n",
    "        q = tf.tile(q, multiples=[1, k.shape[1]])  # [batch, seq_len*items_embedding_dims]\n",
    "        q = tf.reshape(q, shape=(-1, k.shape[1], k.shape[2]))  # [batch, seq_len, items_embedding_dims]\n",
    "\n",
    "        x = tf.concat([q, k, q-k, q*k], axis=-1)  # [batch, seq_len, 4*items_embedding_dims]\n",
    "        for layer in self.att_dense:\n",
    "            x = layer(x)\n",
    "        # x shape 为 [batch, seq_len, last_hidden_units]\n",
    "        w = self.att_final_dense(x)  # [batch, seq_len, 1]\n",
    "        w = tf.squeeze(w, axis=-1)  # [batch, seq_len]\n",
    "\n",
    "        w = tf.where(tf.equal(mask, 0), -(1<<31)*1.0, w)  # [batch, seq_len] 将mask位置的权重调整为很大的负数，在经过softmax之后权重会变为0\n",
    "\n",
    "        # 使用 softmax 归一化权重\n",
    "        w = tf.nn.softmax(w, axis=-1)  # [batch, seq_len]\n",
    "        w = tf.expand_dims(w, axis=1)  # [batch, 1, seq_len]\n",
    "        # 注意力的体现加权融合池化用户历史序列特征\n",
    "        outputs = tf.matmul(w, val)  # [batch, 1, items_embedding_dims]\n",
    "        return tf.squeeze(outputs, axis=1)  # [batch, items_embedding_dims]\n",
    "\n",
    "\n",
    "class DIN(keras.layers.Layer):\n",
    "    def __init__(self, sparse_feature_dict, sparse_feature_index, att_hidden_units=(80, 40),\n",
    "                 ffn_hidden_units=(80, 40), att_activation='prelu', ffn_activation='prelu', maxlen=10, dnn_dropout=0.,\n",
    "                 embed_reg=1e-4):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        # self.sparse_feature_dict = sparse_feature_dict # 存取了所有类别特征需要 embedding 的相关信息\n",
    "        self.user_sparse_feature_index, self.item_sparse_feature_index, self.behavior_feature_index = sparse_feature_index\n",
    "        self.embed_layers = {\n",
    "            'embed_' + k: layers.Embedding(v[0], v[1], embeddings_regularizer=keras.regularizers.l2(embed_reg))\n",
    "            for k, v in sparse_feature_dict.items()\n",
    "        }\n",
    "\n",
    "        self.attention = ActivationLayer(att_hidden_units, activation=Dice() if att_activation != 'prelu' else layers.PReLU())\n",
    "        self.bn = layers.BatchNormalization()  # BN\n",
    "        # MLP 结构提取高教特征交互信息\n",
    "        self.ffn = keras.Sequential([layers.Dense(i, activation=Dice() if ffn_activation != 'prelu' else layers.PReLU())\n",
    "                                     for i in ffn_hidden_units])\n",
    "        self.dropout = layers.Dropout(dnn_dropout)\n",
    "        self.final_output = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 需要事先对behavior中不等长的序列进行填充为等长，\n",
    "        # inputs的输入是不考虑batch维度的\n",
    "        # mask [seq_len]\n",
    "        dense_user_feat, sparse_user_feat, dense_item_feat, sparse_item_feat, behavior_feat, mask = inputs\n",
    "        embed_user_feat = tf.concat([\n",
    "            self.embed_layers['embed_'+k](sparse_user_feat[:, i])\n",
    "            for k, i in self.user_sparse_feature_index.items()\n",
    "        ], axis=-1)  # [batch, user_embed_dims]\n",
    "        user_embed = tf.concat([dense_user_feat, embed_user_feat], axis=-1)  # [batch, user_tot_dims]\n",
    "\n",
    "        embed_item_feat = tf.concat([\n",
    "            self.embed_layers['embed_'+k](sparse_item_feat[:, i])\n",
    "            for k, i in self.item_sparse_feature_index.items()\n",
    "        ], axis=-1)  # [batch, item_embed_dims]\n",
    "        item_embed = tf.concat([dense_item_feat, embed_item_feat], axis=-1)  # [batch, item_tot_dims]\n",
    "\n",
    "        embed_behaviors = tf.concat([\n",
    "            # 使用物品的embedding向量来得到用户的历史embedding序列\n",
    "            self.embed_layers['embed_{v[0]}_{v[1]}_{v[3]}'.format(v=key.split('_'))](behavior_feat[:, i])\n",
    "            for key, i in self.behavior_feature_index.items()\n",
    "        ], axis=-1)  # [batch, tot_seq_embed_dims]\n",
    "\n",
    "        embed_behaviors = tf.reshape(embed_behaviors, shape=(-1, self.maxlen, embed_item_feat.shape[1]))  # [batch, seq_len, item_embed_dims]\n",
    "        mask = tf.reshape(mask, shape=[-1, self.maxlen]) # [batch, seq_len]\n",
    "        behavior_out = self.attention([embed_item_feat, embed_behaviors, mask]) # [batch, items_embedding_dims]\n",
    "        # print('behavior,', behavior_out.shape)\n",
    "        tot_feat = tf.concat([user_embed, item_embed, behavior_out], axis=-1) # [batch, feat_dims]\n",
    "        tot_feat = self.bn(tot_feat)\n",
    "        out = self.ffn(tot_feat)\n",
    "        out = self.dropout(out)\n",
    "        out = self.final_output(out)\n",
    "        print(out.shape)\n",
    "        return tf.nn.sigmoid(out) # [batch, 1]\n",
    "\n",
    "    def build_graph(self):\n",
    "        dense_user_input = keras.Input(shape=(5,), dtype=tf.float32)\n",
    "        sparse_user_input = keras.Input(shape=(3,), dtype=tf.float32)\n",
    "        dense_item_input = keras.Input(shape=(5,), dtype=tf.float32)\n",
    "        sparse_item_input = keras.Input(shape=(3,), dtype=tf.float32)\n",
    "        behavior_input = keras.Input(shape=(3*self.maxlen,), dtype=tf.float32)\n",
    "        mask = keras.Input(shape=(self.maxlen,), dtype=tf.float32)\n",
    "\n",
    "        model = keras.Model(inputs=[dense_user_input, sparse_user_input, dense_item_input, sparse_item_input, behavior_input, mask],\n",
    "                            outputs=self.call([dense_user_input, sparse_user_input, dense_item_input, sparse_item_input, behavior_input, mask]))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "0ff8e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 1 1 1 1 1 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0]], shape=(5, 10), dtype=int8)\n",
      "(10000, 30)\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]], shape=(5, 10), dtype=int8)\n",
      "(None, 1)\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n",
      "157/157 [==============================] - 17s 107ms/step - loss: 0.7002 - auc_1: 0.5047 - val_loss: 0.6935 - val_auc_1: 0.4921\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 17s 106ms/step - loss: 0.6908 - auc_1: 0.5421 - val_loss: 0.6936 - val_auc_1: 0.5045\n"
     ]
    }
   ],
   "source": [
    "# 伪造随机数据进行测试，\n",
    "if __name__ == '__main__':\n",
    "    maxlen = 10\n",
    "\n",
    "    embed_dim = 8\n",
    "    att_hidden_units = [80, 40]\n",
    "    ffn_hidden_units = [256, 128, 64]\n",
    "    dnn_dropout = 0.5\n",
    "    att_activation = 'sigmoid'\n",
    "    ffn_activation = 'prelu'\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "\n",
    "    user_dense_feature_train = pd.DataFrame(np.random.random((10000, 5)),\n",
    "                                            columns=['user_dense_{}'.format(i) for i in range(5)])\n",
    "    user_sparse_feature_train = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),\n",
    "                                             columns=['user_sparse_{}'.format(i) for i in range(3)])\n",
    "    item_dense_feature_train = pd.DataFrame(np.random.random((10000, 5)),\n",
    "                                            columns=['item_dense_{}'.format(i) for i in range(5)])\n",
    "    item_sparse_feature_train = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),\n",
    "                                             columns=['item_sparse_{}'.format(i) for i in range(3)])\n",
    "    behavior_feature_train = None\n",
    "\n",
    "\n",
    "    for ml in range(maxlen): # 序列长度\n",
    "        tmp = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),\n",
    "                           columns=['item_sparse_{}_{}'.format(ml, i) for i in range(3)])\n",
    "        if ml == 0:\n",
    "            behavior_feature_train = tmp\n",
    "        else:\n",
    "            behavior_feature_train = pd.concat([behavior_feature_train, tmp], axis=1)\n",
    "\n",
    "    # 模拟每个用户的序列是不定长的\n",
    "    mask_train = []\n",
    "    for i in range(10000):\n",
    "        tmp = [1]*maxlen\n",
    "        idx = np.random.randint(0, maxlen+1)\n",
    "        tmp[idx:] = [0]*(maxlen-idx) # 用0表示是掩码位置，Attention中应该将权重置为1\n",
    "        mask_train.append(tmp)\n",
    "\n",
    "    mask_train = tf.constant(mask_train, dtype=tf.int8)\n",
    "    print(mask_train[:5])\n",
    "\n",
    "    # 模拟序列\n",
    "    target_train = pd.DataFrame(np.random.randint(0, 2, size=10000))\n",
    "\n",
    "    # valid\n",
    "    user_dense_feature_val = pd.DataFrame(np.random.random((10000, 5)),\n",
    "                                          columns=['user_dense_{}'.format(i) for i in range(5)])\n",
    "    user_sparse_feature_val = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),\n",
    "                                           columns=['user_sparse_{}'.format(i) for i in range(3)])\n",
    "    item_dense_feature_val = pd.DataFrame(np.random.random((10000, 5)),\n",
    "                                          columns=['item_dense_{}'.format(i) for i in range(5)])\n",
    "    item_sparse_feature_val = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),\n",
    "                                           columns=['item_sparse_{}'.format(i) for i in range(3)])\n",
    "    behavior_feature_val = None\n",
    "    for ml in range(maxlen):  # 行为序列的长度\n",
    "        tmp = pd.DataFrame(np.random.randint(1, 10, size=(10000, 3)),  # 3 表征的是在序列的一个时间点上，有3个维度的特征\n",
    "                           columns=['item_sparse_{}_{}'.format(ml, i) for i in range(3)])  # ml_i 表示的是序列ml中的第 i 个类别特征\n",
    "        if ml == 0:\n",
    "            behavior_feature_val = tmp\n",
    "        else:\n",
    "            behavior_feature_val = pd.concat([behavior_feature_val, tmp], axis=1)\n",
    "\n",
    "    print(behavior_feature_val.shape)\n",
    "\n",
    "    # 模拟每个用户的序列是不定长的\n",
    "    mask_test = []\n",
    "    for i in range(10000):\n",
    "        tmp = [1] * maxlen\n",
    "        idx = np.random.randint(0, maxlen + 1)\n",
    "        tmp[idx:] = [0] * (maxlen - idx)  # 用0表示是掩码位置，Attention中应该将权重置为1\n",
    "        mask_test.append(tmp)\n",
    "\n",
    "    mask_test = tf.constant(mask_test, dtype=tf.int8)\n",
    "    print(mask_test[:5])\n",
    "\n",
    "    target_val = pd.DataFrame(np.random.randint(0, 2, size=10000))\n",
    "\n",
    "    sparse_feature_dict = {}\n",
    "    user_sparse_feature_index = {}\n",
    "    item_sparse_feature_index = {}\n",
    "    behavior_feature_index = {}\n",
    "\n",
    "    for idx, col in enumerate(user_sparse_feature_train.columns):\n",
    "        sparse_feature_dict[col] = (user_sparse_feature_train[col].max() + 1, embed_dim)  # embedding(特征种类数， embedding_dim)\n",
    "        user_sparse_feature_index[col] = idx\n",
    "    for idx, col in enumerate(item_sparse_feature_train.columns):\n",
    "        sparse_feature_dict[col] = (item_sparse_feature_train[col].max() + 1, embed_dim)\n",
    "        item_sparse_feature_index[col] = idx\n",
    "    for idx, col in enumerate(behavior_feature_train.columns):\n",
    "        behavior_feature_index[col] = idx\n",
    "\n",
    "    sparse_feature_index = [user_sparse_feature_index, item_sparse_feature_index, behavior_feature_index]\n",
    "\n",
    "    din = DIN(sparse_feature_dict, sparse_feature_index, att_hidden_units, ffn_hidden_units, att_activation,\n",
    "              ffn_activation, maxlen, dnn_dropout)\n",
    "    model = din.build_graph()\n",
    "    # model.summary()\n",
    "    # ============================model checkpoint======================\n",
    "    check_path = 'save/din_weights.epoch_{epoch:04d}.val_loss_{val_loss:.4f}.ckpt'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(check_path, save_weights_only=True,\n",
    "                                                    verbose=1, period=5)\n",
    "    # =========================Compile============================\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=[keras.metrics.AUC()])\n",
    "    model.run_eagerly = True\n",
    "    model.fit([user_dense_feature_train, user_sparse_feature_train, item_dense_feature_train, item_sparse_feature_train,\n",
    "               behavior_feature_train, mask_train],\n",
    "              target_train,\n",
    "              epochs=epochs,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True),\n",
    "                         checkpoint],  # checkpoint\n",
    "              validation_data=([user_dense_feature_val, user_sparse_feature_val, item_dense_feature_val,\n",
    "                                item_sparse_feature_val, behavior_feature_val, mask_test], target_val),\n",
    "              batch_size=batch_size,\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecab97e",
   "metadata": {},
   "source": [
    "### 真实数据建模---亚马逊电商数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "75006f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Data Preprocess Start============\n"
     ]
    }
   ],
   "source": [
    "thr = 500000 # 采样的数据数目\n",
    "\n",
    "# 数据处理部分\n",
    "def to_df(file_path, thr=float('inf')):\n",
    "    \"\"\"\n",
    "    转化为DataFrame结构\n",
    "    :param file_path: 文件路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as fin:\n",
    "        df = {}\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            df[i] = eval(line)\n",
    "            i += 1\n",
    "            if i >= thr: break #采样1w条\n",
    "        df = pd.DataFrame.from_dict(df, orient='index')\n",
    "        return df\n",
    "\n",
    "def build_map(df, col_name):\n",
    "    \"\"\"\n",
    "    制作一个映射，键为列名，值为序列数字\n",
    "    :param df: reviews_df / meta_df\n",
    "    :param col_name: 列名\n",
    "    :return: 字典，键\n",
    "    \"\"\"\n",
    "    key = sorted(df[col_name].unique().tolist())\n",
    "    m = dict(zip(key, range(1, 1+len(key)))) # 从1开始进行label_encode, 0作为特殊的padding scalar\n",
    "    df[col_name] = df[col_name].map(lambda x: m[x])\n",
    "    return m, key\n",
    "\n",
    "print('==========Data Preprocess Start============')\n",
    "reviews_df = to_df('../data/Electronics_10.json', thr) # 只需要对 review 做采样， meta的数量级较小\n",
    "meta_df = to_df('../data/meta_Electronics.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "c7132cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IDCSC6NVONIZ</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>2Cents!</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This mount is just what I needed.  It is stron...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>1367280000</td>\n",
       "      <td>04 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BMUBUC1N77U8</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>ahoffoss</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This mount works really well once you get it u...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pretty simple, but definitely good!</td>\n",
       "      <td>1385164800</td>\n",
       "      <td>11 23, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AQBLWW13U66XD</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>Benjamin Belanger \"v dbl u\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought this for a 22\" TV for my son. I mount...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High Quality/Low Price</td>\n",
       "      <td>1375574400</td>\n",
       "      <td>08 4, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3IIGCFLKVFW8M</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>Brian M. Kaplan \"Brian M. Kaplan\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Works great and is so much cheaper than the mo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Holds a lot</td>\n",
       "      <td>1393459200</td>\n",
       "      <td>02 27, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3VKO21KYDJQ2W</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>C. Aaland</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>For the price, you can't beat it. Mine didn't ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Great for the price.</td>\n",
       "      <td>1310428800</td>\n",
       "      <td>07 12, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347388</th>\n",
       "      <td>A3S3R88HA0HZG3</td>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>PT Cruiser</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Bluetooth speakers have improved a lot over th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Oooh, aahh, ROAR!</td>\n",
       "      <td>1405468800</td>\n",
       "      <td>07 16, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347389</th>\n",
       "      <td>A26VF18X91983P</td>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>Richard</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Super sonic speaker system! Can you say that 5...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Simply stellar!</td>\n",
       "      <td>1405987200</td>\n",
       "      <td>07 22, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347390</th>\n",
       "      <td>A2XRMQA6PJ5ZJ8</td>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>Roger J. Buffington</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Disclosure: I received a free sample of this i...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent Bluetooth speaker with lots of bells...</td>\n",
       "      <td>1404950400</td>\n",
       "      <td>07 10, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347391</th>\n",
       "      <td>A3A4ZAIBQWKOZS</td>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>Stephen M. Lerch</td>\n",
       "      <td>[18, 23]</td>\n",
       "      <td>My short review:If you have the money to spend...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best sounding speaker at this price range</td>\n",
       "      <td>1404691200</td>\n",
       "      <td>07 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347392</th>\n",
       "      <td>AOVTLYTHVDNUX</td>\n",
       "      <td>B00L3YHF6O</td>\n",
       "      <td>Tradecraft \"Live by the Sword\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>The Creative Sound Blaster Roar has a powerful...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Impressive Sound, Stylish, Excellent Price</td>\n",
       "      <td>1405296000</td>\n",
       "      <td>07 14, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347393 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin                       reviewerName  \\\n",
       "0       A2IDCSC6NVONIZ  0972683275                            2Cents!   \n",
       "1       A3BMUBUC1N77U8  0972683275                           ahoffoss   \n",
       "2        AQBLWW13U66XD  0972683275        Benjamin Belanger \"v dbl u\"   \n",
       "3       A3IIGCFLKVFW8M  0972683275  Brian M. Kaplan \"Brian M. Kaplan\"   \n",
       "4       A3VKO21KYDJQ2W  0972683275                          C. Aaland   \n",
       "...                ...         ...                                ...   \n",
       "347388  A3S3R88HA0HZG3  B00L3YHF6O                         PT Cruiser   \n",
       "347389  A26VF18X91983P  B00L3YHF6O                            Richard   \n",
       "347390  A2XRMQA6PJ5ZJ8  B00L3YHF6O                Roger J. Buffington   \n",
       "347391  A3A4ZAIBQWKOZS  B00L3YHF6O                   Stephen M. Lerch   \n",
       "347392   AOVTLYTHVDNUX  B00L3YHF6O     Tradecraft \"Live by the Sword\"   \n",
       "\n",
       "         helpful                                         reviewText  overall  \\\n",
       "0         [1, 1]  This mount is just what I needed.  It is stron...      5.0   \n",
       "1         [0, 0]  This mount works really well once you get it u...      4.0   \n",
       "2         [0, 0]  I bought this for a 22\" TV for my son. I mount...      5.0   \n",
       "3         [0, 0]  Works great and is so much cheaper than the mo...      5.0   \n",
       "4         [0, 0]  For the price, you can't beat it. Mine didn't ...      3.0   \n",
       "...          ...                                                ...      ...   \n",
       "347388    [0, 0]  Bluetooth speakers have improved a lot over th...      5.0   \n",
       "347389    [0, 0]  Super sonic speaker system! Can you say that 5...      5.0   \n",
       "347390    [0, 0]  Disclosure: I received a free sample of this i...      5.0   \n",
       "347391  [18, 23]  My short review:If you have the money to spend...      5.0   \n",
       "347392    [0, 0]  The Creative Sound Blaster Roar has a powerful...      5.0   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "0                                                 Perfect      1367280000   \n",
       "1                     Pretty simple, but definitely good!      1385164800   \n",
       "2                                  High Quality/Low Price      1375574400   \n",
       "3                                             Holds a lot      1393459200   \n",
       "4                                    Great for the price.      1310428800   \n",
       "...                                                   ...             ...   \n",
       "347388                                  Oooh, aahh, ROAR!      1405468800   \n",
       "347389                                    Simply stellar!      1405987200   \n",
       "347390  Excellent Bluetooth speaker with lots of bells...      1404950400   \n",
       "347391          Best sounding speaker at this price range      1404691200   \n",
       "347392         Impressive Sound, Stylish, Excellent Price      1405296000   \n",
       "\n",
       "         reviewTime  \n",
       "0       04 30, 2013  \n",
       "1       11 23, 2013  \n",
       "2        08 4, 2013  \n",
       "3       02 27, 2014  \n",
       "4       07 12, 2011  \n",
       "...             ...  \n",
       "347388  07 16, 2014  \n",
       "347389  07 22, 2014  \n",
       "347390  07 10, 2014  \n",
       "347391   07 7, 2014  \n",
       "347392  07 14, 2014  \n",
       "\n",
       "[347393 rows x 9 columns]"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "36acf05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>related</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0132793040</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31JIPhp%...</td>\n",
       "      <td>The Kelby Training DVD Mastering Blend Modes i...</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Kelby Training DVD: Mastering Blend Modes in A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0321732944</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31uogm6Y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Kelby Training DVD: Adobe Photoshop CS5 Crash ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439886341</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51k0qa8f...</td>\n",
       "      <td>Digital Organizer and Messenger</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, PDAs, ...</td>\n",
       "      <td>Digital Organizer and Messenger</td>\n",
       "      <td>8.15</td>\n",
       "      <td>{'Electronics': 144944}</td>\n",
       "      <td>{'also_viewed': ['0545016266', 'B009ECM8QY', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0511189877</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41HaAhbv...</td>\n",
       "      <td>The CLIKR-5 UR5U-8780L remote control is desig...</td>\n",
       "      <td>[[Electronics, Accessories &amp; Supplies, Audio &amp;...</td>\n",
       "      <td>CLIKR-5 Time Warner Cable Remote Control UR5U-...</td>\n",
       "      <td>23.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B001KC08A4', 'B00KUL8O0W', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0528881469</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51FnRkJq...</td>\n",
       "      <td>Like its award-winning predecessor, the Intell...</td>\n",
       "      <td>[[Electronics, GPS &amp; Navigation, Vehicle GPS, ...</td>\n",
       "      <td>Rand McNally 528881469 7-inch Intelliroute TND...</td>\n",
       "      <td>299.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B006ZOI9OY', 'B00C7FKT2A', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498191</th>\n",
       "      <td>BT008V9J9U</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/313e6SJm...</td>\n",
       "      <td>Vehicle suction cup mount (replacement) NOTICE...</td>\n",
       "      <td>[[Electronics, GPS &amp; Navigation, GPS System Ac...</td>\n",
       "      <td>Suction Cup Mount</td>\n",
       "      <td>21.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'buy_after_viewing': ['B000EPFCC2']}</td>\n",
       "      <td>Garmin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498192</th>\n",
       "      <td>BT008SXQ4C</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/31oF9oNv...</td>\n",
       "      <td>Quatech - 1 Port PCMCIA to DB-25 Parallel Adap...</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Parallel PCMCIA Card 1PORT Epp</td>\n",
       "      <td>23.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_bought': ['B000SR2H4W', 'B001Q7X0W6'], ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498193</th>\n",
       "      <td>BT008G3W52</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/21WIrX5f...</td>\n",
       "      <td>C2G - 5m Ultma USB 2.0 A Mini B Cble</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>C2G / Cables to Go 5M Ultima USB 2.0 Cable</td>\n",
       "      <td>18.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'bought_together': ['B0002D6QJO'], 'buy_after...</td>\n",
       "      <td>C2G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498194</th>\n",
       "      <td>BT008UKTMW</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41TNAVmf...</td>\n",
       "      <td>Keyboard drawer.</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>Underdesk Keyboard Drawer</td>\n",
       "      <td>25.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B0002LD0ZY', 'B0002LCZP0', '...</td>\n",
       "      <td>Fellowes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498195</th>\n",
       "      <td>BT008T2BGK</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41x-15rR...</td>\n",
       "      <td>Garmin USB to R232 Converter CableUSB to RS232...</td>\n",
       "      <td>[[Electronics, Computers &amp; Accessories, Cables...</td>\n",
       "      <td>USB To R232 Converter Cable</td>\n",
       "      <td>62.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'also_viewed': ['B0007T27H8', 'B00425S1H8', '...</td>\n",
       "      <td>Garmin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498196 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin                                              imUrl  \\\n",
       "0       0132793040  http://ecx.images-amazon.com/images/I/31JIPhp%...   \n",
       "1       0321732944  http://ecx.images-amazon.com/images/I/31uogm6Y...   \n",
       "2       0439886341  http://ecx.images-amazon.com/images/I/51k0qa8f...   \n",
       "3       0511189877  http://ecx.images-amazon.com/images/I/41HaAhbv...   \n",
       "4       0528881469  http://ecx.images-amazon.com/images/I/51FnRkJq...   \n",
       "...            ...                                                ...   \n",
       "498191  BT008V9J9U  http://ecx.images-amazon.com/images/I/313e6SJm...   \n",
       "498192  BT008SXQ4C  http://ecx.images-amazon.com/images/I/31oF9oNv...   \n",
       "498193  BT008G3W52  http://ecx.images-amazon.com/images/I/21WIrX5f...   \n",
       "498194  BT008UKTMW  http://ecx.images-amazon.com/images/I/41TNAVmf...   \n",
       "498195  BT008T2BGK  http://ecx.images-amazon.com/images/I/41x-15rR...   \n",
       "\n",
       "                                              description  \\\n",
       "0       The Kelby Training DVD Mastering Blend Modes i...   \n",
       "1                                                     NaN   \n",
       "2                         Digital Organizer and Messenger   \n",
       "3       The CLIKR-5 UR5U-8780L remote control is desig...   \n",
       "4       Like its award-winning predecessor, the Intell...   \n",
       "...                                                   ...   \n",
       "498191  Vehicle suction cup mount (replacement) NOTICE...   \n",
       "498192  Quatech - 1 Port PCMCIA to DB-25 Parallel Adap...   \n",
       "498193               C2G - 5m Ultma USB 2.0 A Mini B Cble   \n",
       "498194                                   Keyboard drawer.   \n",
       "498195  Garmin USB to R232 Converter CableUSB to RS232...   \n",
       "\n",
       "                                               categories  \\\n",
       "0       [[Electronics, Computers & Accessories, Cables...   \n",
       "1       [[Electronics, Computers & Accessories, Cables...   \n",
       "2       [[Electronics, Computers & Accessories, PDAs, ...   \n",
       "3       [[Electronics, Accessories & Supplies, Audio &...   \n",
       "4       [[Electronics, GPS & Navigation, Vehicle GPS, ...   \n",
       "...                                                   ...   \n",
       "498191  [[Electronics, GPS & Navigation, GPS System Ac...   \n",
       "498192  [[Electronics, Computers & Accessories, Cables...   \n",
       "498193  [[Electronics, Computers & Accessories, Cables...   \n",
       "498194  [[Electronics, Computers & Accessories, Cables...   \n",
       "498195  [[Electronics, Computers & Accessories, Cables...   \n",
       "\n",
       "                                                    title   price  \\\n",
       "0       Kelby Training DVD: Mastering Blend Modes in A...     NaN   \n",
       "1       Kelby Training DVD: Adobe Photoshop CS5 Crash ...     NaN   \n",
       "2                         Digital Organizer and Messenger    8.15   \n",
       "3       CLIKR-5 Time Warner Cable Remote Control UR5U-...   23.36   \n",
       "4       Rand McNally 528881469 7-inch Intelliroute TND...  299.99   \n",
       "...                                                   ...     ...   \n",
       "498191                                  Suction Cup Mount   21.99   \n",
       "498192                     Parallel PCMCIA Card 1PORT Epp   23.99   \n",
       "498193         C2G / Cables to Go 5M Ultima USB 2.0 Cable   18.91   \n",
       "498194                          Underdesk Keyboard Drawer   25.54   \n",
       "498195                        USB To R232 Converter Cable   62.31   \n",
       "\n",
       "                      salesRank  \\\n",
       "0                           NaN   \n",
       "1                           NaN   \n",
       "2       {'Electronics': 144944}   \n",
       "3                           NaN   \n",
       "4                           NaN   \n",
       "...                         ...   \n",
       "498191                      NaN   \n",
       "498192                      NaN   \n",
       "498193                      NaN   \n",
       "498194                      NaN   \n",
       "498195                      NaN   \n",
       "\n",
       "                                                  related     brand  \n",
       "0                                                     NaN       NaN  \n",
       "1                                                     NaN       NaN  \n",
       "2       {'also_viewed': ['0545016266', 'B009ECM8QY', '...       NaN  \n",
       "3       {'also_viewed': ['B001KC08A4', 'B00KUL8O0W', '...       NaN  \n",
       "4       {'also_viewed': ['B006ZOI9OY', 'B00C7FKT2A', '...       NaN  \n",
       "...                                                   ...       ...  \n",
       "498191              {'buy_after_viewing': ['B000EPFCC2']}    Garmin  \n",
       "498192  {'also_bought': ['B000SR2H4W', 'B001Q7X0W6'], ...       NaN  \n",
       "498193  {'bought_together': ['B0002D6QJO'], 'buy_after...       C2G  \n",
       "498194  {'also_viewed': ['B0002LD0ZY', 'B0002LCZP0', '...  Fellowes  \n",
       "498195  {'also_viewed': ['B0007T27H8', 'B00425S1H8', '...    Garmin  \n",
       "\n",
       "[498196 rows x 9 columns]"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "id": "eb5f28d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11589, 2), (347393, 3))"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只保留reviews文件中出现过的商品\n",
    "# 'reviewerID', 'asin', 'unixReviewTime' 分别表示用户id，物品id，购物的时间\n",
    "meta_df = meta_df[meta_df['asin'].isin(reviews_df['asin'].unique())]\n",
    "meta_df = meta_df.reset_index(drop=True)\n",
    "reviews_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
    "meta_df = meta_df[['asin', 'categories']]\n",
    "meta_df.shape, reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "fcf4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 物品类别只保留最后一个\n",
    "meta_df['categories'] = meta_df['categories'].map(lambda x: x[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "668c67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-911-26aeec91a599>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name] = df[col_name].map(lambda x: m[x])\n"
     ]
    }
   ],
   "source": [
    "# 高维类别数据的编号索引map\n",
    "# meta_df文件的物品ID映射\n",
    "asin_map, asin_key = build_map(meta_df, 'asin')\n",
    "# meta_df文件物品种类映射\n",
    "cate_map, cate_key = build_map(meta_df, 'categories') # 对meta_df 的 categories列 进行 label_encoder，并返回 map_dict 以及 encode_list \n",
    "# reviews_df文件的用户ID映射\n",
    "revi_map, revi_key = build_map(reviews_df, 'reviewerID')\n",
    "\n",
    "user_count, item_count, cate_count, example_count = \\\n",
    "    len(revi_map), len(asin_map), len(cate_map), reviews_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "aa972403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>11585</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>11587</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>11588</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11587</th>\n",
       "      <td>11586</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11588</th>\n",
       "      <td>11589</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11589 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        asin  categories\n",
       "0          1         513\n",
       "1          3         573\n",
       "2          5         572\n",
       "3          4         572\n",
       "4          6         572\n",
       "...      ...         ...\n",
       "11584  11585         340\n",
       "11585  11587         266\n",
       "11586  11588          47\n",
       "11587  11586         346\n",
       "11588  11589         486\n",
       "\n",
       "[11589 rows x 2 columns]"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "ebd794c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-918-3d35b115f969>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_df['asin'] = reviews_df['asin'].map(lambda x: asin_map[x])\n"
     ]
    }
   ],
   "source": [
    "# reviews_df文件物品id进行映射，并按照用户id、浏览时间进行排序，重置索引\n",
    "reviews_df['asin'] = reviews_df['asin'].map(lambda x: asin_map[x])\n",
    "reviews_df = reviews_df.sort_values(['reviewerID', 'unixReviewTime'])\n",
    "reviews_df = reviews_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "42789f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各个物品对应的类别\n",
    "cate_list = np.array([0]+meta_df['categories'].to_list(), dtype='int32')\n",
    "\n",
    "reviews_df.columns = ['user_id', 'item_id', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "ee935fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>885</td>\n",
       "      <td>1359936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3892</td>\n",
       "      <td>1359936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4477</td>\n",
       "      <td>1359936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4709</td>\n",
       "      <td>1359936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6541</td>\n",
       "      <td>1359936000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347388</th>\n",
       "      <td>20247</td>\n",
       "      <td>4913</td>\n",
       "      <td>1388534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347389</th>\n",
       "      <td>20247</td>\n",
       "      <td>5052</td>\n",
       "      <td>1388534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347390</th>\n",
       "      <td>20247</td>\n",
       "      <td>9047</td>\n",
       "      <td>1389744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347391</th>\n",
       "      <td>20247</td>\n",
       "      <td>10490</td>\n",
       "      <td>1389744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347392</th>\n",
       "      <td>20247</td>\n",
       "      <td>3448</td>\n",
       "      <td>1404172800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347393 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id        time\n",
       "0             1      885  1359936000\n",
       "1             1     3892  1359936000\n",
       "2             1     4477  1359936000\n",
       "3             1     4709  1359936000\n",
       "4             1     6541  1359936000\n",
       "...         ...      ...         ...\n",
       "347388    20247     4913  1388534400\n",
       "347389    20247     5052  1388534400\n",
       "347390    20247     9047  1389744000\n",
       "347391    20247    10490  1389744000\n",
       "347392    20247     3448  1404172800\n",
       "\n",
       "[347393 rows x 3 columns]"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "72b60d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20247/20247 [00:07<00:00, 2770.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def gen_neg(pos_list):\n",
    "    neg = pos_list[0]\n",
    "    while neg in pos_list:\n",
    "        neg = random.randint(0, item_count - 1) # 产生还没有点过的物品作为负例 [l, r]\n",
    "    return neg\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "for user_id, hist in tqdm(reviews_df.groupby('user_id')):\n",
    "    pos_list = hist['item_id'].tolist()\n",
    "\n",
    "    neg_list = [gen_neg(pos_list) for i in range(len(pos_list))]\n",
    "    hist = []\n",
    "    for i in range(1, len(pos_list)):\n",
    "        hist.append([pos_list[i - 1], cate_list[pos_list[i-1]]])\n",
    "        hist_i = hist.copy()\n",
    "        if i == len(pos_list) - 1:\n",
    "            test_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1]) # 历史行为，【正样本id,类别】，label\n",
    "            test_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])# 历史行为，【负样本id,类别】，label\n",
    "            # test_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # test_data.append([hist_i, [neg_list[i]], 0])\n",
    "        elif i == len(pos_list) - 2:\n",
    "            val_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1])\n",
    "            val_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])\n",
    "            # val_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # val_data.append([hist_i, [neg_list[i]], 0])\n",
    "        else:\n",
    "            train_data.append([hist_i, [pos_list[i], cate_list[pos_list[i]]], 1])\n",
    "            train_data.append([hist_i, [neg_list[i], cate_list[neg_list[i]]], 0])\n",
    "            # train_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # train_data.append([hist_i, [neg_list[i]], 0])\n",
    "\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "9d272c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Padding===================\n",
      "============Data Preprocess End=============\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 10\n",
    "embed_dims = 8\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "train = pd.DataFrame(train_data, columns=['hist', 'target_item', 'label'])\n",
    "val = pd.DataFrame(val_data, columns=['hist', 'target_item', 'label'])\n",
    "test = pd.DataFrame(test_data, columns=['hist', 'target_item', 'label'])\n",
    "\n",
    "# if no dense or sparse features, can fill with 0\n",
    "print('==================Padding===================')\n",
    "\n",
    "sparse_dict = { # 配置 embedding层的信息\n",
    "    'item_sparse_id': (item_count+1, embed_dims),\n",
    "    'item_sparse_category': (cate_count+1, embed_dims)\n",
    "}\n",
    "\n",
    "user_sparse_index = {}\n",
    "\n",
    "item_sparse_index = {\n",
    "    'item_sparse_id': 0,\n",
    "    'item_sparse_category': 1\n",
    "}\n",
    "\n",
    "behavior_index = {\n",
    "}\n",
    "for time in range(maxlen):\n",
    "    for i, k in enumerate(['id', 'category']):\n",
    "        key = f'item_sparse_{time}_{k}'\n",
    "        behavior_index[key] = time*2 + i\n",
    "\n",
    "# 如果没有dense 或者 稀疏 特征使用全 0 的tesor来代替\n",
    "train_x = [np.zeros(shape=(len(train), )), np.zeros(shape=(len(train), )),\n",
    "        np.zeros(shape=(len(train), )), np.array(list(train['target_item'].values)),\n",
    "        tf.reshape(pad_sequences(train['hist'], maxlen=maxlen, value=0), shape=[len(train), -1]) # 用-1表示是填充的项\n",
    "       ] # dense_user, sparse_user, dense_item, sparse_item, behavior_\n",
    "\n",
    "val_x = [np.zeros(shape=(len(val), )), np.zeros(shape=(len(val), )),\n",
    "        np.zeros(shape=(len(val), )), np.array(list(val['target_item'].values)),\n",
    "        tf.reshape(pad_sequences(val['hist'], maxlen=maxlen, value=0), shape=[len(val), -1]) # 用-1表示是填充的项\n",
    "       ] # dense_user, sparse_user, dense_item, sparse_item, behavior_\n",
    "\n",
    "test_x = [np.zeros(shape=(len(test), )), np.zeros(shape=(len(test), )),\n",
    "        np.zeros(shape=(len(test), )), np.array(list(test['target_item'].values)),\n",
    "        tf.reshape(pad_sequences(test['hist'], maxlen=maxlen, value=0), shape=[len(test), -1]) # 用-1表示是填充的项\n",
    "       ] # dense_user, sparse_user, dense_item, sparse_item, behavior_\n",
    "print('============Data Preprocess End=============')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "f1bb136f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([40494, 20])"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b4179",
   "metadata": {},
   "source": [
    "### 搭建模型---为了适应本数据集进行了微调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "b4ed7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from keras import layers\n",
    "\n",
    "# Dice自适应激活函数\n",
    "class Dice(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = self.add_weight(shape=(), initializer=keras.initializers.Zeros(), dtype=tf.float32)\n",
    "        self.bn = layers.BatchNormalization(center=False, scale=False, trainable=True) # 只使用BN的归一化，不学习移动和缩放参数\n",
    "\n",
    "    def call(self, x):\n",
    "        p = tf.nn.sigmoid(self.bn(x))\n",
    "        return p*x + (1-p)*self.alpha*x\n",
    "\n",
    "\n",
    "class ActivationLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    注意力层，用于加权融合用户历史行为序列\n",
    "    \"\"\"\n",
    "    def __init__(self, att_hidden_units, activation='prelu'):\n",
    "        super().__init__()\n",
    "        self.att_dense = [layers.Dense(unit, activation=Dice() if ffn_activation != 'prelu' else layers.PReLU()) for unit in att_hidden_units]\n",
    "        self.att_final_dense = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        \"\"\"\n",
    "        inputs [query_items, hist_items, mask]\n",
    "        quer_items : [batch, items_embedding_dims]\n",
    "        hist_items: [batch, seq_len, items_embedding_dims]\n",
    "        mask : [batch, seq_len] 因为用户序列是不定长的所以使用mask掩掉那些填充的序列维度\n",
    "        \"\"\"\n",
    "        q, k, mask = inputs\n",
    "        val = tf.identity(k) # copy\n",
    "        q = tf.tile(q, multiples=[1, k.shape[1]])  # [batch, seq_len*items_embedding_dims]\n",
    "        q = tf.reshape(q, shape=(-1, k.shape[1], k.shape[2]))  # [batch, seq_len, items_embedding_dims]\n",
    "\n",
    "        x = tf.concat([q, k, q-k, q*k], axis=-1)  # [batch, seq_len, 4*items_embedding_dims]\n",
    "        for layer in self.att_dense:\n",
    "            x = layer(x)\n",
    "        # x shape 为 [batch, seq_len, last_hidden_units]\n",
    "        w = self.att_final_dense(x)  # [batch, seq_len, 1]\n",
    "        w = tf.squeeze(w, axis=-1)  # [batch, seq_len]\n",
    "\n",
    "        w = tf.where(tf.equal(mask, 0), -(1<<31)*1.0, w)  # [batch, seq_len] 将mask位置的权重调整为很大的负数，在经过softmax之后权重会变为0\n",
    "\n",
    "        # 使用 softmax 归一化权重\n",
    "        w = tf.nn.softmax(w, axis=-1)  # [batch, seq_len]\n",
    "        w = tf.expand_dims(w, axis=1)  # [batch, 1, seq_len]\n",
    "        # 注意力的体现加权融合池化用户历史序列特征\n",
    "        outputs = tf.matmul(w, val)  # [batch, 1, items_embedding_dims]\n",
    "        return tf.squeeze(outputs, axis=1)  # [batch, items_embedding_dims]\n",
    "\n",
    "\n",
    "class DIN(keras.layers.Layer):\n",
    "    def __init__(self, sparse_feature_dict, sparse_feature_index, att_hidden_units=(80, 40),\n",
    "                 ffn_hidden_units=(80, 40), att_activation='prelu', ffn_activation='prelu', maxlen=10, dnn_dropout=0.,\n",
    "                 embed_reg=1e-4):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        # self.sparse_feature_dict = sparse_feature_dict # 存取了所有类别特征需要 embedding 的相关信息\n",
    "        self.user_sparse_feature_index, self.item_sparse_feature_index, self.behavior_feature_index = sparse_feature_index\n",
    "        self.embed_layers = {\n",
    "            'embed_' + k: layers.Embedding(v[0], v[1], embeddings_regularizer=keras.regularizers.l2(embed_reg))\n",
    "            for k, v in sparse_feature_dict.items()\n",
    "        }\n",
    "\n",
    "        self.attention = ActivationLayer(att_hidden_units, activation=Dice() if att_activation != 'prelu' else layers.PReLU())\n",
    "        self.bn = layers.BatchNormalization()  # BN\n",
    "        # MLP 结构提取高教特征交互信息\n",
    "        self.ffn = keras.Sequential([layers.Dense(i, activation=Dice() if ffn_activation != 'prelu' else layers.PReLU())\n",
    "                                     for i in ffn_hidden_units])\n",
    "        self.dropout = layers.Dropout(dnn_dropout)\n",
    "        self.final_output = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, inputs, missing_val=0):\n",
    "        # 将o作为类别的padding值，需要对特征使用label_enocde的时候从1开始进行\n",
    "        # 需要事先对behavior中不等长的序列进行填充为等长，\n",
    "        # inputs的输入是不考虑batch维度的\n",
    "        # mask [seq_len]\n",
    "        dense_user_feat, sparse_user_feat, dense_item_feat, sparse_item_feat, behavior_feat = inputs\n",
    "        mask = tf.ones_like(behavior_feat, dtype=tf.float32)\n",
    "        mask = tf.where(tf.equal(behavior_feat, missing_val), 0.0, mask) # 将pad的进行mask\n",
    "        print(mask.shape)\n",
    "        if self.user_sparse_feature_index:\n",
    "            embed_user_feat = tf.concat([\n",
    "                self.embed_layers['embed_'+k](sparse_user_feat[:, i])\n",
    "                for k, i in self.user_sparse_feature_index.items()\n",
    "            ], axis=-1)  # [batch, user_embed_dims]\n",
    "        else: # 如果没有用户的类别特征会传入一个全为0的特征来替代embedding向量\n",
    "            embed_user_feat = sparse_user_feat\n",
    "            \n",
    "        user_embed = tf.concat([dense_user_feat, embed_user_feat], axis=-1)  # [batch, user_tot_dims]\n",
    "\n",
    "            \n",
    "        if self.item_sparse_feature_index.items:    \n",
    "            embed_item_feat = tf.concat([\n",
    "                self.embed_layers['embed_'+k](sparse_item_feat[:, i])\n",
    "                for k, i in self.item_sparse_feature_index.items()\n",
    "            ], axis=-1)  # [batch, item_embed_dims]\n",
    "        else:\n",
    "            embed_item_feat = sparse_item_feat\n",
    "            \n",
    "        item_embed = tf.concat([dense_item_feat, embed_item_feat], axis=-1)  # [batch, item_tot_dims]\n",
    "        \n",
    "        \n",
    "        embed_behaviors = tf.concat([\n",
    "            # 使用物品的embedding向量来得到用户的历史embedding序列\n",
    "            self.embed_layers['embed_{v[0]}_{v[1]}_{v[3]}'.format(v=key.split('_'))](behavior_feat[:, i])\n",
    "            for key, i in self.behavior_feature_index.items()\n",
    "        ], axis=-1)  # [batch, tot_seq_embed_dims]\n",
    "\n",
    "        embed_behaviors = tf.reshape(embed_behaviors, shape=(-1, self.maxlen, embed_item_feat.shape[1]))  # [batch, seq_len, item_embed_dims]\n",
    "        # 原本的mask是对时间点上的两个特征都进行了padding=0，所以shape = [None, 2*seq_len] \n",
    "        # 在attention中我们需要知道的是time时间点是否是被 padding的，也就是两个特征有一个被pad=0就代表这个时间点需要被mask（即 i与i+1 都是表征 i//2 这个点的特征，i为偶数）\n",
    "        # reshape采用了按行填充的方式，所以这里可以使用间隔索引乘积的方式得到真实的mask序列 [None, seq_len]\n",
    "        # trick!\n",
    "        mask = tf.gather(mask, indices=list(range(0, mask.shape[1], 2)), axis=-1) *  tf.gather(mask, indices=list(range(1, mask.shape[1], 2)), axis=-1)# [batch, seq_len]\n",
    "        print(mask.shape, embed_behaviors.shape)\n",
    "        behavior_out = self.attention([embed_item_feat, embed_behaviors, mask]) # [batch, items_embedding_dims]\n",
    "        print('behavior,', behavior_out.shape)\n",
    "        tot_feat = tf.concat([user_embed, item_embed, behavior_out], axis=-1) # [batch, feat_dims]\n",
    "        tot_feat = self.bn(tot_feat)\n",
    "        out = self.ffn(tot_feat)\n",
    "        out = self.dropout(out)\n",
    "        out = self.final_output(out)\n",
    "        print(out.shape)\n",
    "        return tf.nn.sigmoid(out) # [batch, 1]\n",
    "\n",
    "    def build_graph(self, dims=[5, 3, 5, 3, 3]):\n",
    "        d_u, s_u, d_i, s_i, k = dims\n",
    "        # 其中 k 代表用户行为序列中一个时间点上的特征数目\n",
    "        dense_user_input = keras.Input(shape=(d_u,), dtype=tf.float32)\n",
    "        sparse_user_input = keras.Input(shape=(s_u,), dtype=tf.float32)\n",
    "        dense_item_input = keras.Input(shape=(d_i,), dtype=tf.float32)\n",
    "        sparse_item_input = keras.Input(shape=(s_i,), dtype=tf.float32)\n",
    "        behavior_input = keras.Input(shape=(k*self.maxlen,), dtype=tf.float32)\n",
    "\n",
    "        model = keras.Model(inputs=[dense_user_input, sparse_user_input, dense_item_input, sparse_item_input, behavior_input],\n",
    "                            outputs=self.call([dense_user_input, sparse_user_input, dense_item_input, sparse_item_input, behavior_input]))\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "0a2461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n",
      "(None, 10) (None, 10, 16)\n",
      "behavior, (None, 16)\n",
      "(None, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None,)             0           ['input_4[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None,)             0           ['input_4[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None,)             0           ['input_5[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_11 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_12 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_13 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_14 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_15 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_17 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_20 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_21 (S  (None,)             0           ['input_5[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.equal (TFOpLambda)     (None, 20)           0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)      (None, 20)           0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 8)            92720       ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_2[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_12[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_14[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_16[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_18[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_20[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 8)            4592        ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_11[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_13[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_15[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_17[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_21[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.where (TFOpLambda)          (None, 20)           0           ['tf.math.equal[0][0]',          \n",
      "                                                                  'tf.ones_like[0][0]']           \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 160)          0           ['embedding[1][0]',              \n",
      "                                                                  'embedding_1[1][0]',            \n",
      "                                                                  'embedding[2][0]',              \n",
      "                                                                  'embedding_1[2][0]',            \n",
      "                                                                  'embedding[3][0]',              \n",
      "                                                                  'embedding_1[3][0]',            \n",
      "                                                                  'embedding[4][0]',              \n",
      "                                                                  'embedding_1[4][0]',            \n",
      "                                                                  'embedding[5][0]',              \n",
      "                                                                  'embedding_1[5][0]',            \n",
      "                                                                  'embedding[6][0]',              \n",
      "                                                                  'embedding_1[6][0]',            \n",
      "                                                                  'embedding[7][0]',              \n",
      "                                                                  'embedding_1[7][0]',            \n",
      "                                                                  'embedding[8][0]',              \n",
      "                                                                  'embedding_1[8][0]',            \n",
      "                                                                  'embedding[9][0]',              \n",
      "                                                                  'embedding_1[9][0]',            \n",
      "                                                                  'embedding[10][0]',             \n",
      "                                                                  'embedding_1[10][0]']           \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather (TFOpLambd  (None, 10)          0           ['tf.where[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_1 (TFOpLam  (None, 10)          0           ['tf.where[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 16)           0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 10, 16)       0           ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 10)           0           ['tf.compat.v1.gather[0][0]',    \n",
      "                                                                  'tf.compat.v1.gather_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 2)            0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 17)           0           ['input_3[0][0]',                \n",
      "                                                                  'tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " activation_layer (ActivationLa  (None, 16)          9681        ['tf.concat_1[0][0]',            \n",
      " yer)                                                             'tf.reshape[0][0]',             \n",
      "                                                                  'tf.math.multiply[0][0]']       \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 35)           0           ['tf.concat[0][0]',              \n",
      "                                                                  'tf.concat_2[0][0]',            \n",
      "                                                                  'activation_layer[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 35)          140         ['tf.concat_4[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 64)           50816       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            65          ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambda)   (None, 1)            0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 158,014\n",
      "Trainable params: 157,944\n",
      "Non-trainable params: 70\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 23s 19ms/step - loss: 0.6109 - auc: 0.7308 - val_loss: 0.5588 - val_auc: 0.7969\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 0.5055 - auc: 0.8445 - val_loss: 0.5102 - val_auc: 0.8438\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 0.4701 - auc: 0.8717 - val_loss: 0.5033 - val_auc: 0.8520\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.4565 - auc: 0.8821 - val_loss: 0.5040 - val_auc: 0.8561\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 0.4469 - auc: 0.8891 - val_loss: 0.5112 - val_auc: 0.8524\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.4388 - auc: 0.8949 - val_loss: 0.5114 - val_auc: 0.8537\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.4320 - auc: 0.8996 - val_loss: 0.5155 - val_auc: 0.8549\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.4264 - auc: 0.9033 - val_loss: 0.5236 - val_auc: 0.8525\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 19s 17ms/step - loss: 0.4216 - auc: 0.9067 - val_loss: 0.5265 - val_auc: 0.8504\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.4171 - auc: 0.9096 - val_loss: 0.5408 - val_auc: 0.8505\n",
      "1266/1266 [==============================] - 2s 2ms/step - loss: 0.5711 - auc: 0.8367\n",
      "[0.5710781812667847, 0.8366610407829285]\n"
     ]
    }
   ],
   "source": [
    "att_hidden_units = [80, 40]\n",
    "ffn_hidden_units = [256, 128, 64]\n",
    "dnn_dropout = 0.5\n",
    "att_activation = 'sigmoid'\n",
    "ffn_activation = 'prelu'\n",
    "din = DIN(sparse_dict, [user_sparse_index, item_sparse_index, behavior_index], att_hidden_units, ffn_hidden_units, att_activation,\n",
    "                    ffn_activation, maxlen, dnn_dropout)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = din.build_graph([1, 1, 1, 2, 2])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(),\n",
    "             metrics=[keras.metrics.AUC()])\n",
    "\n",
    "model.fit(train_x, train['label'], epochs=10, batch_size=512, validation_data=(val_x, val['label']))\n",
    "print(model.evaluate(test_x, test['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb1c9e",
   "metadata": {},
   "source": [
    "# YoutubeDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59af441",
   "metadata": {},
   "source": [
    "### 数据规整函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f1b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def gen_data_set(data, negsample=0):\n",
    "\n",
    "    data.sort_values(\"timestamp\", inplace=True)\n",
    "    item_ids = data['movie_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['movie_id'].tolist()\n",
    "        rating_list = hist['rating'].tolist()\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]), rating_list[i]))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1]), 0)) # 负采样样本看作评分为0\n",
    "            else:\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i], 1,len(hist[::-1]),rating_list[i]))\n",
    "\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(len(train_set[0]),len(test_set[0]))\n",
    "\n",
    "    return train_set,test_set\n",
    "\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8608d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "\n",
    "unames = ['user_id','gender','age','occupation','zip']\n",
    "user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames, engine='python')\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames, engine='python')\n",
    "mnames = ['movie_id','title','genres']\n",
    "movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames, engine='python')\n",
    "\n",
    "data = pd.merge(pd.merge(ratings,movies),user).iloc[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "783d22b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>850</td>\n",
       "      <td>1029</td>\n",
       "      <td>4</td>\n",
       "      <td>975351703</td>\n",
       "      <td>Dumbo (1941)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>60640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>850</td>\n",
       "      <td>1207</td>\n",
       "      <td>5</td>\n",
       "      <td>975349928</td>\n",
       "      <td>To Kill a Mockingbird (1962)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>60640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>850</td>\n",
       "      <td>608</td>\n",
       "      <td>4</td>\n",
       "      <td>975350080</td>\n",
       "      <td>Fargo (1996)</td>\n",
       "      <td>Crime|Drama|Thriller</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>60640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>850</td>\n",
       "      <td>2194</td>\n",
       "      <td>3</td>\n",
       "      <td>975357193</td>\n",
       "      <td>Untouchables, The (1987)</td>\n",
       "      <td>Action|Crime|Drama</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>60640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>850</td>\n",
       "      <td>2628</td>\n",
       "      <td>1</td>\n",
       "      <td>975359331</td>\n",
       "      <td>Star Wars: Episode I - The Phantom Menace (1999)</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>60640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating  timestamp  \\\n",
       "0            1      1193       5  978300760   \n",
       "1            1       661       3  978302109   \n",
       "2            1       914       3  978301968   \n",
       "3            1      3408       4  978300275   \n",
       "4            1      2355       5  978824291   \n",
       "...        ...       ...     ...        ...   \n",
       "59995      850      1029       4  975351703   \n",
       "59996      850      1207       5  975349928   \n",
       "59997      850       608       4  975350080   \n",
       "59998      850      2194       3  975357193   \n",
       "59999      850      2628       1  975359331   \n",
       "\n",
       "                                                  title  \\\n",
       "0                One Flew Over the Cuckoo's Nest (1975)   \n",
       "1                      James and the Giant Peach (1996)   \n",
       "2                                   My Fair Lady (1964)   \n",
       "3                                Erin Brockovich (2000)   \n",
       "4                                  Bug's Life, A (1998)   \n",
       "...                                                 ...   \n",
       "59995                                      Dumbo (1941)   \n",
       "59996                      To Kill a Mockingbird (1962)   \n",
       "59997                                      Fargo (1996)   \n",
       "59998                          Untouchables, The (1987)   \n",
       "59999  Star Wars: Episode I - The Phantom Menace (1999)   \n",
       "\n",
       "                                genres gender  age  occupation    zip  \n",
       "0                                Drama      F    1          10  48067  \n",
       "1         Animation|Children's|Musical      F    1          10  48067  \n",
       "2                      Musical|Romance      F    1          10  48067  \n",
       "3                                Drama      F    1          10  48067  \n",
       "4          Animation|Children's|Comedy      F    1          10  48067  \n",
       "...                                ...    ...  ...         ...    ...  \n",
       "59995     Animation|Children's|Musical      M   35           0  60640  \n",
       "59996                            Drama      M   35           0  60640  \n",
       "59997             Crime|Drama|Thriller      M   35           0  60640  \n",
       "59998               Action|Crime|Drama      M   35           0  60640  \n",
       "59999  Action|Adventure|Fantasy|Sci-Fi      M   35           0  60640  \n",
       "\n",
       "[60000 rows x 10 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e3b22a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6034/6034 [00:28<00:00, 215.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                    \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
    "SEQ_LEN = 50\n",
    "negsample = 2\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "\n",
    "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip']\n",
    "feature_max_idx = {}\n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1 # 将0作为mask\n",
    "    if feature == 'user_id':\n",
    "        user_idx_2_rawid = dict(zip(lbe.transform(lbe.classes_)+1, lbe.classes_))\n",
    "    elif feature == 'movie_id':\n",
    "        doc_idx_2_rawid = dict(zip(lbe.transform(lbe.classes_)+1, lbe.classes_))\n",
    "        \n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
    "\n",
    "train_set, test_set = gen_data_set(data, negsample)\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "\n",
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7690e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "75a73f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2963796, 5) (2963796,) (2963796, 1)\n"
     ]
    }
   ],
   "source": [
    "user_feat_columns = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "item_feat_columns = ['movie_id']\n",
    "user_behavior_columns = ['hist_movie_id']\n",
    "\n",
    "\n",
    "def get_array(dict_input, label, columns):\n",
    "    user_columns = np.transpose(np.stack([dict_input[i]\n",
    "                            for i in columns\n",
    "                            ], axis=0))\n",
    "    movie_id = np.expand_dims(dict_input['movie_id'], axis=1) # 点击的商品\n",
    "    print(user_columns.shape, label.shape, movie_id.shape)\n",
    "    return user_columns, label, movie_id\n",
    "\n",
    "train_user_sparse_feat, train_label, target_movie = get_array(train_model_input, train_label, user_feat_columns)\n",
    "train_hist_movie_feat = train_model_input['hist_movie_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a424966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class DNN(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    dnn 部分\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='relu', reg_l2=0.01,**kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=activation, \n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# 用于计算 softmax 的负采样函数\n",
    "class SampleSoftMax(keras.layers.Layer):\n",
    "    def __init__(self, num_items, embed_dims, numsample):\n",
    "        self.num_items = num_items\n",
    "        self.embed_dims = embed_dims\n",
    "        self.numsample= numsample\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.items_w = self.add_weight(shape=[self.num_items, self.embed_dims],\n",
    "                          initializer=tf.keras.initializers.random_normal,\n",
    "                          trainable=True,\n",
    "                          name='item_embedding') # embed\n",
    "        self.bias_ = self.add_weight(shape=[self.num_items,],\n",
    "                                    initializer=tf.keras.initializers.zeros,\n",
    "                                    trainable=False, name='bias_0')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        out_user_embed, target_movie_id = inputs\n",
    "        loss = tf.nn.sampled_softmax_loss(\n",
    "            weights=self.items_w,\n",
    "            biases=self.bias_,\n",
    "            labels=tf.reshape(tf.cast(target_movie_id, tf.int32), shape=[-1,1]), # 真实的正例索引\n",
    "            inputs=out_user_embed,\n",
    "            num_classes=self.num_items,\n",
    "            num_sampled=self.numsample,\n",
    "            num_true=1\n",
    "        )\n",
    "        return tf.expand_dims(loss, 1)  # [batch, 1]\n",
    "        \n",
    "    \n",
    "    def get_embeding(self, indx):\n",
    "        return tf.nn.embedding_lookup(self.items_w, tf.cast(indx, tf.int32))\n",
    "    \n",
    "\n",
    "class YoutubeDNN(keras.layers.Layer):\n",
    "    def __init__(self, feature_max_idx, num_items, embed_dims, SEQ_LEN, dnn_hidden_units, dnn_activation, dnn_reg_l2, numsample=5):\n",
    "        assert dnn_hidden_units[-1] == embed_dims, '用户embedding向量要与video embedding维度一致'\n",
    "        self.user_feat_columns = user_feat_columns\n",
    "        self.seq_len = SEQ_LEN\n",
    "\n",
    "        self.embedding_layers = {\n",
    "            k: keras.layers.Embedding(v, embed_dims, name=k+'_embedding')\n",
    "            for k, v in feature_max_idx.items()\n",
    "        }\n",
    "        self.num_items = num_items\n",
    "        self.embed_dims = embed_dims\n",
    "        self.numsample = 5\n",
    "        self.dnn = DNN(dnn_hidden_units, dnn_activation, dnn_reg_l2)\n",
    "        self.sampleSoftMax = SampleSoftMax(num_items, embed_dims, numsample)\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_sparse_feat, hist_movie_feat, target_movie_id = inputs\n",
    "        embed_user = tf.concat([\n",
    "            self.embedding_layers[k](user_sparse_feat[:, i])\n",
    "            for i, k in enumerate(self.user_feat_columns)\n",
    "        ], axis=-1) # [batch, embed_dims_tot]\n",
    "        mask = tf.where(tf.equal(hist_movie_feat, 0), 1.0, 0.0) # [batch, seq_len]\n",
    "        mask_count = tf.reduce_sum(mask, axis=1, keepdims=True) # [batch,1] \n",
    "        hist = self.embedding_layers['movie_id'](hist_movie_feat) # [batch, seq_len, embed_dims]\n",
    "        mask_h = tf.expand_dims(mask, 1)# [batch, 1, seq_len]\n",
    "        video_embed = tf.matmul(mask_h, hist) # [batch, 1, embed_dims]\n",
    "        video_embed = tf.squeeze(video_embed, axis=1) # [batch, embed_dims]  sum 池化\n",
    "        video_embed = video_embed / (0.001 + mask_count) # 平均池化\n",
    "        \n",
    "        print(mask.shape, video_embed.shape, mask_count.shape)\n",
    "        x = tf.concat([embed_user, video_embed], axis=-1) #[batch, input_dims]\n",
    "\n",
    "        self.user_embedding = out_user_embed = self.dnn(x) # [batch, embed_dims]\n",
    "\n",
    "        return self.sampleSoftMax([out_user_embed, target_movie_id]) # 返回的是loss\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        user_sparse_feat = keras.Input(shape=(5,), dtype=tf.float32, name='input_user')\n",
    "        hist_movie_feat = keras.Input(shape=(self.seq_len,), dtype=tf.float32, name='input_hist_video')\n",
    "        target_movie_id = keras.Input(shape=(1,), dtype=tf.float32, name='click_movie')\n",
    "\n",
    "        inputs = [user_sparse_feat, hist_movie_feat, target_movie_id]\n",
    "        outputs = self.call(inputs)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        model.__setattr__('user_input', [user_sparse_feat, hist_movie_feat])\n",
    "        model.__setattr__('user_embedding', self.user_embedding)\n",
    "        model.__setattr__('item_input', target_movie_id)\n",
    "        model.__setattr__('item_embedding', self.sampleSoftMax.get_embeding(model.item_input))\n",
    "        \n",
    "        # 添加一些列\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a4e140ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50) (None, 60) (None, 1)\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup_8), but are not present in its tracked objects:   <tf.Variable 'sample_soft_max_19/item_embedding:0' shape=(3707, 60) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    }
   ],
   "source": [
    "num_items = feature_max_idx['movie_id']\n",
    "embed_dims = 60\n",
    "SEQ_LEN = SEQ_LEN\n",
    "dnn_hidden_units = (256, 128, embed_dims)\n",
    "dnn_activation = 'relu'\n",
    "dnn_reg_l2 = 0.0001\n",
    "\n",
    "model = YoutubeDNN(\n",
    "    feature_max_idx, num_items, embed_dims, SEQ_LEN, dnn_hidden_units, dnn_activation, dnn_reg_l2, numsample=5\n",
    ").build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "24a46616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5210/5210 [==============================] - 120s 23ms/step - loss: 0.2533 - val_loss: 0.1834\n",
      "Epoch 2/10\n",
      "5210/5210 [==============================] - 129s 25ms/step - loss: 0.1986 - val_loss: 0.1997\n",
      "Epoch 3/10\n",
      "5210/5210 [==============================] - 136s 26ms/step - loss: 0.1966 - val_loss: 0.1847\n",
      "Epoch 4/10\n",
      "5210/5210 [==============================] - 135s 26ms/step - loss: 0.1942 - val_loss: 0.1899\n",
      "Epoch 5/10\n",
      "5210/5210 [==============================] - 136s 26ms/step - loss: 0.1930 - val_loss: 0.1944\n",
      "Epoch 6/10\n",
      "5210/5210 [==============================] - 203s 39ms/step - loss: 0.1946 - val_loss: 0.1874\n",
      "Epoch 7/10\n",
      "5210/5210 [==============================] - 271s 52ms/step - loss: 0.1976 - val_loss: 0.1795\n",
      "Epoch 8/10\n",
      "5210/5210 [==============================] - 146s 28ms/step - loss: 0.1948 - val_loss: 0.1860\n",
      "Epoch 9/10\n",
      "5210/5210 [==============================] - 149s 29ms/step - loss: 0.1922 - val_loss: 0.1887\n",
      "Epoch 10/10\n",
      "5210/5210 [==============================] - 148s 28ms/step - loss: 0.1920 - val_loss: 0.1954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a79ce539d0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_obj(ytrue, y_pred): # 自定义 loss \n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=loss_obj)\n",
    "model.fit([train_user_sparse_feat, train_hist_movie_feat, train_label], train_label, epochs=10, batch_size=512, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bcfaf",
   "metadata": {},
   "source": [
    "### 效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9d1ac957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_item_embded = keras.Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "model_user_embed = keras.Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "\n",
    "items_input = np.arange(1, num_items) # 0 是掩码位置不进行考虑\n",
    "doc_embs = tf.squeeze(model_item_embded(items_input)) \n",
    "doc_embs = doc_embs / tf.linalg.norm(doc_embs, axis=1, keepdims=True) # 所有item物品的embedding 进行归一化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7cec8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6034, 5) (6034,) (6034, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([6034, 60])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_sparse_feat, test_label, test_click_movie_id = get_array(test_model_input, test_label, user_feat_columns)\n",
    "test_hist_movie_feat = test_model_input['hist_movie_id']\n",
    "user_embs = model_user_embed([test_user_sparse_feat, test_hist_movie_feat])\n",
    "user_embs = user_embs / tf.linalg.norm(user_embs, axis=1, keepdims=True) # 测试集所有用户的embedding\n",
    "user_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2af998a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟server的阶段，使用LSH来找到和用户embedding最近的movie进行推荐\n",
    "from annoy import AnnoyIndex\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "def get_youtube_recall_res(user_embs, doc_embs, user_idx_2_rawid, doc_idx_2_rawid, topk):\n",
    "    \"\"\"近邻检索，这里用annoy tree\"\"\"\n",
    "    # 把doc_embs构建成索引树\n",
    "    f = user_embs.shape[1]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, v in enumerate(doc_embs):\n",
    "        t.add_item(i, v)\n",
    "    t.build(10)\n",
    "    # 可以保存该索引树 t.save('annoy.ann')\n",
    "    \n",
    "    # 每个用户向量， 返回最近的TopK个item\n",
    "    user_recall_items_dict = collections.defaultdict(dict)\n",
    "    for i, u in enumerate(user_embs):\n",
    "        recall_doc_scores = t.get_nns_by_vector(u, topk, include_distances=True)\n",
    "        # recall_doc_scores是(([doc_idx], [scores]))， 这里需要转成原始doc的id\n",
    "        raw_doc_scores = list(recall_doc_scores)\n",
    "        raw_doc_scores[0] = [doc_idx_2_rawid[i+1] for i in raw_doc_scores[0]] # 此处 doc_idx + 1来忽略 mask 的位置的影响\n",
    "        # 转换成实际用户id\n",
    "        try:\n",
    "            user_recall_items_dict[user_idx_2_rawid[i]] = dict(zip(*raw_doc_scores))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 默认是分数从小到大排的序， 这里要从大到小\n",
    "    user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in user_recall_items_dict.items()}\n",
    "    \n",
    "    # 保存一份\n",
    "    pickle.dump(user_recall_items_dict, open('youtube_u2i_dict.pkl', 'wb'))\n",
    "    \n",
    "    return user_recall_items_dict\n",
    "\n",
    "\n",
    "user_recall_items_dict = get_youtube_recall_res(user_embs, doc_embs, user_idx_2_rawid, doc_idx_2_rawid, topk=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7aed9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_click_item_dict = {\n",
    "    user_idx_2_rawid[i]: doc_idx_2_rawid[j]\n",
    "    for i, j in zip(test_user_sparse_feat[:, 0], np.squeeze(test_click_movie_id))\n",
    "} # 记录用户最后点击的真实movie_id 是多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c6167b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " topk:  20  :  hit_num:  23 hit_rate:  0.00381 user_num :  6033\n",
      " topk:  40  :  hit_num:  94 hit_rate:  0.01558 user_num :  6033\n",
      " topk:  60  :  hit_num:  137 hit_rate:  0.02271 user_num :  6033\n",
      " topk:  80  :  hit_num:  226 hit_rate:  0.03746 user_num :  6033\n",
      " topk:  100  :  hit_num:  295 hit_rate:  0.0489 user_num :  6033\n"
     ]
    }
   ],
   "source": [
    "# 依次评估召回的前10, 20, 30, 40, 50个文章中的击中率\n",
    "def metrics_recall(user_recall_items_dict, last_click_item_dict, topk=100):\n",
    "    user_num = len(user_recall_items_dict)\n",
    "    \n",
    "    for k in range(20, topk+1, 20):\n",
    "        hit_num = 0\n",
    "        for user, item_list in user_recall_items_dict.items():\n",
    "            if user in last_click_item_dict:\n",
    "                # 获取前k个召回的结果\n",
    "                tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "                if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                    hit_num += 1\n",
    "        \n",
    "        hit_rate = round(hit_num * 1.0 / user_num, 5)\n",
    "        print(' topk: ', k, ' : ', 'hit_num: ', hit_num, 'hit_rate: ', hit_rate, 'user_num : ', user_num)\n",
    "\n",
    "\n",
    "metrics_recall(user_recall_items_dict, last_click_item_dict, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04455236",
   "metadata": {},
   "source": [
    "# DIEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e229c3",
   "metadata": {},
   "source": [
    "### 实现AUGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e28563df",
   "metadata": {},
   "outputs": [],
   "source": [
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "\n",
    "class AUGRUCell(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    实现AUGRU\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_units,\n",
    "                 activation=tf.math.tanh,\n",
    "                 reuse=None, # 控制variable_scope的名字是否可以复用的，在同一个scope下的变量可以通过tf.get_variable来获取，从而实现共享变量\n",
    "                 kernel_initializer=keras.initializers.random_normal,\n",
    "                 bias_initializer=keras.initializers.Zeros):\n",
    "\n",
    "        super(AUGRUCell, self).__init__()\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation or math_ops.tanh\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._bias_initializer = bias_initializer\n",
    "        self._gate_linear = None\n",
    "        self._candidate_linear = None\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        embed_dims = input_shape[-1] - 1 # trick: -1 是因为最后一个维度作为了储存注意力分数\n",
    "        with tf.name_scope('gates'):    \n",
    "            self.w1 = self.add_weight(shape=(embed_dims+self._num_units, 2*self._num_units),\n",
    "                                     initializer=self._kernel_initializer,\n",
    "                                     trainable=True,\n",
    "                                     name=_WEIGHTS_VARIABLE_NAME)\n",
    "            self.bias1 = self.add_weight(shape=(2*self._num_units,),\n",
    "                                        initializer=self._bias_initializer,\n",
    "                                        trainable=True,\n",
    "                                        name=_BIAS_VARIABLE_NAME\n",
    "                                        )\n",
    "            \n",
    "        with tf.name_scope('candidate'):\n",
    "            self.w2 = self.add_weight(shape=(embed_dims+self._num_units, self._num_units),\n",
    "                                     initializer=self._kernel_initializer,\n",
    "                                     trainable=True,\n",
    "                                     name=_WEIGHTS_VARIABLE_NAME)\n",
    "            self.bias2 = self.add_weight(shape=(self._num_units,),\n",
    "                                        initializer=self._bias_initializer,\n",
    "                                        trainable=True,\n",
    "                                        name=_BIAS_VARIABLE_NAME\n",
    "                                        )\n",
    "            \n",
    "        \n",
    "    def __call__(self, inputs, state):\n",
    "        return self.call(inputs, state)\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Gated recurrent unit (GRU) with nunits cells.\n",
    "        trick: 将score merge 到inputs 的最后一列，表示该时期的注意力得分\n",
    "        params:\n",
    "            inputs: [batch, embedims + 1] # 第 t 时期的输入， 其中1是注意力得分\n",
    "            state: ([batch, num_units], )  传入的是元组，第 t-1 期的cell状态\n",
    "  Call arguments:\n",
    "    inputs: A 2D tensor.\n",
    "    states: List of state tensors corresponding to the previous timestep.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode or in inference mode. Only relevant when `dropout` or\n",
    "      `recurrent_dropout` is used.\n",
    "        \"\"\"\n",
    "        state = state[0] # status 是元组类型，要通过索引才能拿道具体的state\n",
    "        item_embed_dims = inputs.shape[1] - 1\n",
    "        inputs, att_score = tf.split(inputs, [item_embed_dims, 1], axis=-1) # [batch, embed_dims], [batch, 1]\n",
    "        x = tf.concat([inputs, state], axis=-1) # [batch, embed_dims + num_units]   \n",
    "        value = tf.sigmoid(tf.matmul(x, self.w1) + self.bias1) # [batch, 2*num_units]\n",
    "        \n",
    "        r, u = tf.split(value, num_or_size_splits=2, axis=1) # 2*[batch, num_units]\n",
    "\n",
    "        r_state = r * state # [batch, num_units] \n",
    "        c = self._activation(tf.matmul(x, self.w2) + self.bias2)  # [batch, num_units] \n",
    "        print(u.shape, c.shape, att_score.shape)\n",
    "        u = (1.0 - att_score) * u\n",
    "        new_h = u * state + (1 - u) * c\n",
    "        \n",
    "        return new_h, new_h\n",
    "    \n",
    "### for test\n",
    "# batch_size = 10\n",
    "# embed_dims = 129\n",
    "# output_dim = 120\n",
    "# num_units = 20\n",
    "\n",
    "# inputs = tf.Variable(tf.random.normal([batch_size, 2, embed_dims]))\n",
    "\n",
    "# gruCell = tf.keras.layers.RNN(AUGRUCell(num_units), return_sequences=True, return_state=True)\n",
    "# print('model after build!', inputs.shape)\n",
    "# output, state = gruCell(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3a6f1",
   "metadata": {},
   "source": [
    "### AttentionPoolingLayer\n",
    "#### 复用DIN的注意得分单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "ec6610da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续使用了DIN中的注意力机制来得到注意力分数\n",
    "# Dice自适应激活函数\n",
    "from keras import layers\n",
    "\n",
    "class Dice(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = self.add_weight(shape=(), initializer=keras.initializers.Zeros(), dtype=tf.float32)\n",
    "        self.bn = layers.BatchNormalization(center=False, scale=False, trainable=True) # 只使用BN的归一化，不学习移动和缩放参数\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        flag = False # 是否是由于负采样引起的升维情况\n",
    "        if len(x.shape) == 4:\n",
    "            flag = True\n",
    "            seq_len, neg_sample, embed_dims = x.shape[1:]\n",
    "            x = tf.reshape(x, shape=[-1, neg_sample, embed_dims])\n",
    "        p = tf.nn.sigmoid(self.bn(x))\n",
    "        if flag == True:\n",
    "            x = tf.reshape(x, shape=[-1, seq_len, neg_sample, embed_dims])\n",
    "        return p*x + (1-p)*self.alpha*x\n",
    "\n",
    "\n",
    "class AttentionPoolingLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    注意力层，用于加权融合用户历史行为序列\n",
    "    \"\"\"\n",
    "    def __init__(self, att_hidden_units, activation='prelu', return_score=False, **kwgs):\n",
    "        super().__init__(**kwgs)\n",
    "        self.att_dense = [layers.Dense(unit, activation=Dice() if activation != 'prelu' else layers.PReLU()) for unit in att_hidden_units]\n",
    "        self.att_final_dense = layers.Dense(1)\n",
    "        self.return_score = return_score\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        \"\"\"\n",
    "        inputs [query_items, hist_items, mask]\n",
    "        quer_items (q): [batch, seq_len, items_embedding_dims]\n",
    "        hist_items (k): [batch, seq_len, items_embedding_dims]\n",
    "        mask : [batch, 1] 传入每个序列的真实长度\n",
    "        \"\"\"\n",
    "        q, k, mask = inputs\n",
    "        mask = tf.sequence_mask(mask, k.shape[1], dtype=tf.float32) # [batch, 1, seq_len]\n",
    "        mask = tf.squeeze(mask, axis=1) # [batch, seq_len]\n",
    "        val = tf.identity(k) # copy\n",
    "        print('1', q.shape, k.shape, mask.shape)\n",
    "        \n",
    "        x = tf.concat([q, k, q-k, q*k], axis=-1)  # [batch, seq_len, 4*items_embedding_dims]\n",
    "        for layer in self.att_dense:\n",
    "            x = layer(x)\n",
    "        # x shape 为 [batch, seq_len, last_hidden_units]\n",
    "        w = self.att_final_dense(x)  # [batch, seq_len, 1]\n",
    "        w = tf.squeeze(w, axis=-1)  # [batch, seq_len]\n",
    "\n",
    "        w = tf.where(tf.equal(mask, 0), -(1<<31)*1.0, w)  # [batch, seq_len] 将mask位置的权重调整为很大的负数，在经过softmax之后权重会变为0\n",
    "\n",
    "        # 使用 softmax 归一化权重\n",
    "        w = tf.nn.softmax(w, axis=-1)  # [batch, seq_len]\n",
    "        if self.return_score: return w # 返会注意力得分，后续融入AUGRU中\n",
    "        \n",
    "        w = tf.expand_dims(w, axis=1)  # [batch, 1, seq_len] 用户行为序列与目标query的 score\n",
    "        # 不需要返回score 直接进行DIN中的加权池化\n",
    "        outputs = tf.matmul(w, val)  # [batch, 1, items_embedding_dims]\n",
    "        outputs = tf.squeeze(outputs, axis=1)  # [batch, items_embedding_dims] # 历史行为序列的池化embedding\n",
    "            \n",
    "        return outputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608c132",
   "metadata": {},
   "source": [
    "### 构建兴趣进化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "3223050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    dnn 部分\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units, activation='prelu', reg_l2=0.01, **kwgs):   \n",
    "        self.dnn_network = [keras.layers.Dense(i, activation=layers.PReLU() if activation == 'prelu' \n",
    "                                               else (Dice() if isinstance(activation, Dice) else activation),  # 对每一层确定一个激活函数\n",
    "                                               kernel_regularizer=keras.regularizers.l2(reg_l2),  \n",
    "                                               bias_regularizer=keras.regularizers.l2(reg_l2)) for i in hidden_units] # 使用正则化可以防止过拟合\n",
    "        super().__init__(**kwgs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.dnn_network:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DIEN(keras.layers.Layer):\n",
    "    def __init__(self, sparse_feature_dict, sparse_feat_index_list, num_units=20, max_len=20, neg_sample=2,\n",
    "                 aux_hidden_units=(128, 64, 1), aux_activation='sigmoid', att_hidden_units=(200, 60), att_activation=Dice(),\n",
    "                 return_score=True, dnn_hidden_units=(200, 80), dnn_activation='prelu', embed_reg=0.001,\n",
    "                dropout=0):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            sparse_feature_dict: 记录了需要embedding的特征信息\n",
    "            sparse_feat_index_lost : 记录各种类型的叙述特征和输入的索引之间的映射关系\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.neg_sample = neg_sample\n",
    "        self.aux_dnn = DNN(aux_hidden_units, activation=aux_activation, name='for_aux') # 用于学习计算兴趣和目标广告相似度的dnn结构\n",
    "        self.last_dnn = DNN(dnn_hidden_units, dnn_activation, name='last_dnn_interaction') # 最后的输出dnn\n",
    "        self.att = AttentionPoolingLayer(att_hidden_units, att_activation, return_score=return_score, name='attention_layer')\n",
    "        self.extract_int = layers.GRU(num_units, return_sequences=True)\n",
    "        self.evolution_int = tf.keras.layers.RNN(AUGRUCell(num_units))\n",
    "        self.embed_layers = {\n",
    "            k: layers.Embedding(v[0], v[1], embeddings_regularizer=keras.regularizers.l2(embed_reg), name=k)\n",
    "            for k, v in sparse_feature_dict.items()\n",
    "        }\n",
    "        self.user_sparse_feat, self.hist_sparse_feat, self.item_sparse_feat = sparse_feat_index_list\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.ctr = layers.Dense(1, activation='sigmoid', name='ctr')\n",
    "        self.bn = layers.BatchNormalization()  # 使用 DICE 函数就不需要BN了\n",
    "        \n",
    "    \n",
    "    def auxiliary_loss(self, is_pos, state_seq, click_seq, no_click_seq, mask):\n",
    "        \"\"\"\n",
    "        is_pos 代表抽取出来的是否是正例 [None, 1]\n",
    "        注意：在序列模型中使用 click_t+1 来作为label，计算和h_t的损失，表明有损失值序列长度为原seq_length-1，因为最后一个时间步的输出就没法计算了\n",
    "        k = seq_len - 1\n",
    "        dnn: 用来计算相似度的dnn结构\n",
    "        state_seq:  [None, k, num_units]\n",
    "        click_seq: [None, k, embed_dims]\n",
    "        no_click_seq: 负采样的点 [None, k, neg_sample, embed_dims]\n",
    "        mask: 真实的序列长度 [None, 1]\n",
    "        \"\"\"\n",
    "        max_len = click_seq.shape[1]\n",
    "        neg_sample = no_click_seq.shape[-2]\n",
    "        num_units = state_seq.shape[-1]\n",
    "        mask = tf.sequence_mask(mask, max_len, dtype=tf.float32)[:, 0, :] # [None, k]\n",
    "\n",
    "        x_pos = tf.concat([state_seq, click_seq], axis=-1) # [None, k, num_units + embed_dims]\n",
    "        # 只对将来点击的正例进行学习，对于负例不计算损失\n",
    "#         print('p_pos', p_pos.shape)\n",
    "        p_pos = self.aux_dnn(x_pos) # [None, k, 1]\n",
    "        is_ok_mask = tf.reshape(tf.tile(is_pos, [1, x_pos.shape[1]]), shape=(-1, p_pos.shape[1], p_pos.shape[2]))\n",
    "        print(is_ok_mask.shape)\n",
    "        p_pos = tf.where(tf.math.greater(is_ok_mask, 0), p_pos, 1) # loss 只计算兴趣状态与真实点击的物品embedding之间的loss\n",
    "        loss_pos = -tf.math.log(p_pos)\n",
    "\n",
    "        status_neg = tf.tile(state_seq, [1, 1, neg_sample]) # [None, k, neg_samples * num_units]\n",
    "        status_neg = tf.reshape(status_neg, shape=(-1, status_neg.shape[1], neg_sample, num_units)) # [None, k, neg_samples, num_units]\n",
    "        x_neg = tf.concat([status_neg, no_click_seq], axis=-1) # [None, k, neg_samples, num_units + embed_dims]\n",
    "        p_neg = tf.squeeze(self.aux_dnn(x_neg), axis=-1) # [None, k, neg_samples]\n",
    "        loss_neg = -tf.math.log(1 - p_neg) # [None, k, neg_samples]\n",
    "        loss_neg = tf.reduce_mean(loss_neg, axis=-1, keepdims=True) # [None, k, 1]\n",
    "        \n",
    "        res = tf.reduce_mean(loss_pos + loss_neg) \n",
    "        return tf.reduce_mean(loss_pos + loss_neg) # 标量\n",
    "\n",
    "    \n",
    "    def interest_evolution(self, is_pos, hist_seq, pos_query, neg_query, seq_len):\n",
    "        \"\"\"\n",
    "        整合构建DIEN中的兴趣进化网络\n",
    "        params:\n",
    "            is_pos [None, 1] # pos_query 是否是点击的物品，还是采样出来的负例\n",
    "            hist_seq [None, seq_len, embed_dims]\n",
    "            pos_query: [None, embed_dims]\n",
    "            neg_query: [None, neg_sample, embed_dims]\n",
    "            seq_len: [None, 1]\n",
    "        \"\"\"\n",
    "        print(hist_seq.shape, pos_query.shape, neg_query.shape, seq_len.shape)\n",
    "        aux_loss = 0\n",
    "        embed_dims, length = pos_query.shape[-1], hist_seq.shape[1]\n",
    "        pos_query = tf.tile(pos_query, [1, hist_seq.shape[1]]) # [None, seq_len*embed_dims]\n",
    "        pos_query = tf.reshape(pos_query, [-1, hist_seq.shape[1], embed_dims]) # [None, seq_len, embed_dims]\n",
    "        \n",
    "        neg_query = tf.tile(neg_query, [1, 1, length]) #[None, neg_sample, seq_len*embed_dims]\n",
    "        neg_query = tf.reshape(neg_query, [-1, neg_query.shape[1], length, embed_dims]) # [None, neg_sample, seq_len, embed_dims]\n",
    "        neg_query = tf.transpose(neg_query, [0, 2, 1, 3]) # [None, seq_len, neg_sample, embed_dims]\n",
    "        \n",
    "        # 兴趣提取层\n",
    "        state_seq = self.extract_int(hist_seq) # 抽取出兴趣embedding [None, seq_len, num_units]\n",
    "        aux_loss = self.auxiliary_loss(is_pos,\n",
    "                                      state_seq[:,:-1,:], # 抽取出 seq_len - 1个兴趣状态\n",
    "                                      hist_seq[:,1:,:],\n",
    "                                      neg_query[:,1:,:,:],\n",
    "                                      seq_len - 1.0 # 序列长度为 seq_len - 1\n",
    "                                      )\n",
    "        # 兴趣进化层\n",
    "        score = self.att([pos_query, state_seq, seq_len]) # [None, seq_len]\n",
    "        # 拼接到RNN的输入中作为输入\n",
    "        score = tf.expand_dims(score, axis=-1)# [None, seq_len, 1]\n",
    "        state_seq_with_score = tf.concat([state_seq, score], axis=-1) # [None, seq_len ,num_units+1]\n",
    "        out = self.evolution_int(state_seq_with_score) # [None, num_units]\n",
    "        return out, aux_loss\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs： [user_sparse_feat, user_hist_sparse_feat, pos_query_item, neg_query_item, seq_len]\n",
    "        user_sparse_feat: [None, feat_kinds]\n",
    "        user_hist_sparse_feat: [None, max_seq, k]  # k 表示的是一个时间点有k个特征\n",
    "        pos_query_item: [None, j] # 表示的是目标广告的类别有 j 个\n",
    "        neg_query_item: [None, m, j] # 表示的是负采样目标广告的类别有 j 个，1 pos vs m neg\n",
    "        seq_len: [None, 1] # 序列长度\n",
    "        is_pos: 点击的物品是否是正例\n",
    "        \"\"\"\n",
    "        user_sparse_feat, user_hist_sparse_feat, pos_query_item, neg_query_item, seq_len, is_pos = inputs\n",
    "        embed_user_feat = tf.concat([\n",
    "            self.embed_layers[k](user_sparse_feat[:, v])\n",
    "            for k, v in self.user_sparse_feat.items()\n",
    "        ], axis=-1) # [None, tot_user_embeds]\n",
    "        embed_hist = tf.concat([\n",
    "            self.embed_layers[k](user_hist_sparse_feat[:,:,i])\n",
    "            for k, i in self.hist_sparse_feat.items()\n",
    "        ], axis=-1) # [None, max_seq, k*embedding]\n",
    "        pos_embed_items = tf.concat([\n",
    "            self.embed_layers[k](pos_query_item[:, i])\n",
    "            for k, i in self.item_sparse_feat.items()\n",
    "        ], axis=-1) # [None, j*item_embeddings]\n",
    "        neg_embed_items = tf.concat([\n",
    "            self.embed_layers[k](neg_query_item[:, :, i])\n",
    "            for k, i in self.item_sparse_feat.items()\n",
    "        ], axis=-1) # [None, m, j*item_embeddings]\n",
    "        \n",
    "        # x [None, num_units]\n",
    "        # 传入is_pos 和 label 是对应的，主要用于兴趣提取层去学习兴趣的表达\n",
    "        x, aux_loss = self.interest_evolution(is_pos, embed_hist, pos_embed_items, neg_embed_items, seq_len)\n",
    "        print('112', aux_loss.shape, x.shape) #建立静态图\n",
    "        # 如果有连续特征可以加在这里\n",
    "        x = tf.concat([x, embed_user_feat, pos_embed_items], axis=-1) # [None, tot_embeddings]\n",
    "        x = self.bn(x)\n",
    "        print('111', x.shape, aux_loss.shape)\n",
    "        output = self.last_dnn(x)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        #添加 aux_loss\n",
    "        self.add_loss(aux_loss*2)\n",
    "        return self.ctr(output)\n",
    "    \n",
    "def build_graph(sparse_dict, sparse_feat_index_list, max_len, neg_sample=2):\n",
    "    dien = DIEN(sparse_dict, sparse_feat_index_list, max_len=maxlen, neg_sample=neg_sample)\n",
    "    user_sparse_feat = keras.Input(shape=(1,), dtype=tf.float32, name='user_sparse_input')\n",
    "    user_hist_sparse_feat = keras.Input(shape=(max_len, 2), dtype=tf.float32, name='user_list_input')\n",
    "    pos_query_item = keras.Input(shape=(2,), dtype=tf.float32, name='pos_item_sparse_input')\n",
    "    neg_query_item = keras.Input(shape=(neg_sample, 2), dtype=tf.float32, name='neg_item_sparse_input')\n",
    "    seq_len = keras.Input(shape=(1,), dtype=tf.float32, name='seq_len')\n",
    "    is_pos =  keras.Input(shape=(1,), dtype=tf.float32, name='is_pos')\n",
    "    inputs = [user_sparse_feat, user_hist_sparse_feat, pos_query_item, neg_query_item, seq_len, is_pos]\n",
    "    outputs = dien(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbe32c",
   "metadata": {},
   "source": [
    "### 加载亚马逊数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1f9d0645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Data Preprocess Start============\n"
     ]
    }
   ],
   "source": [
    "thr = 500000 # 采样的数据数目\n",
    "\n",
    "# 数据处理部分\n",
    "def to_df(file_path, thr=float('inf')):\n",
    "    \"\"\"\n",
    "    转化为DataFrame结构\n",
    "    :param file_path: 文件路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as fin:\n",
    "        df = {}\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            df[i] = eval(line)\n",
    "            i += 1\n",
    "            if i >= thr: break #采样1w条\n",
    "        df = pd.DataFrame.from_dict(df, orient='index')\n",
    "        return df\n",
    "\n",
    "def build_map(df, col_name):\n",
    "    \"\"\"\n",
    "    制作一个映射，键为列名，值为序列数字\n",
    "    :param df: reviews_df / meta_df\n",
    "    :param col_name: 列名\n",
    "    :return: 字典，键\n",
    "    \"\"\"\n",
    "    key = sorted(df[col_name].unique().tolist())\n",
    "    m = dict(zip(key, range(1, 1+len(key)))) # 从1开始进行label_encode, 0作为特殊的padding scalar\n",
    "    df[col_name] = df[col_name].map(lambda x: m[x])\n",
    "    return m, key\n",
    "\n",
    "print('==========Data Preprocess Start============')\n",
    "reviews_df = to_df('../data/Electronics_10.json', thr) # 只需要对 review 做采样， meta的数量级较小\n",
    "meta_df = to_df('../data/meta_Electronics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "fd3680c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保留reviews文件中出现过的商品\n",
    "# 'reviewerID', 'asin', 'unixReviewTime' 分别表示用户id，物品id，购物的时间\n",
    "meta_df = meta_df[meta_df['asin'].isin(reviews_df['asin'].unique())]\n",
    "meta_df = meta_df.reset_index(drop=True)\n",
    "reviews_df = reviews_df[['reviewerID', 'asin', 'unixReviewTime']]\n",
    "meta_df = meta_df[['asin', 'categories']]\n",
    "meta_df.shape, reviews_df.shape\n",
    "# 物品类别只保留最后一个\n",
    "meta_df['categories'] = meta_df['categories'].map(lambda x: x[-1][-1])\n",
    "# 高维类别数据的编号索引map\n",
    "# meta_df文件的物品ID映射\n",
    "asin_map, asin_key = build_map(meta_df, 'asin')\n",
    "# meta_df文件物品种类映射\n",
    "cate_map, cate_key = build_map(meta_df, 'categories') # 对meta_df 的 categories列 进行 label_encoder，并返回 map_dict 以及 encode_list \n",
    "# reviews_df文件的用户ID映射\n",
    "revi_map, revi_key = build_map(reviews_df, 'reviewerID')\n",
    "\n",
    "user_count, item_count, cate_count, example_count = \\\n",
    "    len(revi_map), len(asin_map), len(cate_map), reviews_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "017f301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reviews_df文件物品id进行映射，并按照用户id、浏览时间进行排序，重置索引\n",
    "reviews_df['asin'] = reviews_df['asin'].map(lambda x: asin_map[x])\n",
    "reviews_df = reviews_df.sort_values(['reviewerID', 'unixReviewTime'])\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "\n",
    "# 各个物品对应的类别\n",
    "cate_list = np.array([0]+meta_df['categories'].to_list(), dtype='int32')\n",
    "\n",
    "reviews_df.columns = ['user_id', 'item_id', 'time']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a78c8bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20247/20247 [03:37<00:00, 92.99it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "tot_items = set(meta_df['asin'].unique())\n",
    "cate_dict = dict(meta_df.values)\n",
    "def gen_neg(pos_list, neg_sample=2):\n",
    "    temp = list(set(tot_items) - set(pos_list))\n",
    "    res = random.choices(range(len(temp)), k=neg_sample)\n",
    "    return [[temp[i], cate_dict[temp[i]]] for i in res] # [neg_sample, 2]\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "\n",
    "neg_list = list(tot_items - set(pos_list))\n",
    "for user_id, hist in tqdm(reviews_df.groupby('user_id')):\n",
    "    pos_list = hist['item_id'].tolist()\n",
    "\n",
    "    \n",
    "    hist = []\n",
    "    for i in range(1, len(pos_list)):\n",
    "        hist.append([pos_list[i - 1], cate_list[pos_list[i-1]]])\n",
    "        hist_i = hist.copy()\n",
    "        if i == len(pos_list) - 1:\n",
    "            test_data.append([1, hist_i, [pos_list[i], cate_list[pos_list[i]]], gen_neg(pos_list[:i+1]), len(hist_i), 1]) # 历史行为，【正样本id,类别】，label\n",
    "            neg_item = random.choice(neg_list)\n",
    "            test_data.append([1, hist_i, [neg_item, cate_dict[neg_item]], gen_neg(pos_list[:i+1]), len(hist_i), 0])\n",
    "            # test_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # test_data.append([hist_i, [neg_list[i]], 0])\n",
    "        elif i == len(pos_list) - 2:\n",
    "            val_data.append([1, hist_i, [pos_list[i], cate_list[pos_list[i]]], gen_neg(pos_list[:i+1]), len(hist_i), 1]) # 历史行为，【正样本id,类别】，label\n",
    "            neg_item = random.choice(neg_list)\n",
    "            val_data.append([1, hist_i, [neg_item, cate_dict[neg_item]], gen_neg(pos_list[:i+1]), len(hist_i), 0])\n",
    "            # val_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # val_data.append([hist_i, [neg_list[i]], 0])\n",
    "        else:\n",
    "            train_data.append([1, hist_i, [pos_list[i], cate_list[pos_list[i]]], gen_neg(pos_list[:i+1]), len(hist_i), 1]) # 历史行为，【正样本id,类别】，label\n",
    "            neg_item = random.choice(neg_list)\n",
    "            train_data.append([1, hist_i, [neg_item, cate_dict[neg_item]], gen_neg(pos_list[:i+1]), len(hist_i), 0])\n",
    "            # train_data.append([hist_i, [pos_list[i]], 1])\n",
    "            # train_data.append([hist_i, [neg_list[i]], 0])\n",
    "\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "f951f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Padding===================\n",
      "============Data Preprocess End=============\n",
      "(40494, 1)\n",
      "(40494, 10, 2)\n",
      "(40494, 2)\n",
      "(40494, 2, 2)\n",
      "(40494, 1)\n",
      "(40494, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 10\n",
    "embed_dims = 10\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "columns = ['user_id', 'hist', 'pos_query', 'neg_query', 'seq_len', 'label']\n",
    "train = pd.DataFrame(train_data, columns=columns)\n",
    "val = pd.DataFrame(val_data, columns=columns)\n",
    "test = pd.DataFrame(test_data, columns=columns)\n",
    "\n",
    "# if no dense or sparse features, can fill with 0\n",
    "print('==================Padding===================')\n",
    "\n",
    "sparse_dict = { # 配置 embedding层的信息\n",
    "    'user_id': (user_count+1, embed_dims),\n",
    "    'item_id': (item_count+1, embed_dims),\n",
    "    'item_category': (cate_count+1, embed_dims)\n",
    "}\n",
    "\n",
    "user_sparse_index = {\n",
    "    'user_id': 0\n",
    "}\n",
    "\n",
    "item_sparse_index = {\n",
    "    'item_id': 0,\n",
    "    'item_category': 1\n",
    "}\n",
    "\n",
    "behavior_index = {\n",
    "    'item_id': 0,\n",
    "    \"item_category\": 1\n",
    "}\n",
    "\n",
    "train_y = train['label']\n",
    "val_y = val['label']\n",
    "test_y = test['label']\n",
    "\n",
    "\n",
    "# 如果没有dense 或者 稀疏 特征使用全 0 的tesor来代替\n",
    "train_x = [np.array(train['user_id'].values.reshape(-1, 1)),\n",
    "        tf.reshape(pad_sequences(train['hist'], maxlen=maxlen, value=0, padding='post', truncating='post'), \n",
    "                   shape=[len(train), maxlen, -1]), # 用0表示是填充的项,\n",
    "        np.array(list(train['pos_query'])), #[batch, 2]\n",
    "        np.array(list(train['neg_query'])), # [batch, 2, 2]\n",
    "        np.array(train['seq_len'].values.reshape(-1, 1)), # [batch, 1],\n",
    "        np.array(train['label'].values.reshape(-1, 1)) # [batch, 1],\n",
    "       ]\n",
    "\n",
    "val_x = [np.array(val['user_id'].values.reshape(-1, 1)),\n",
    "        tf.reshape(pad_sequences(val['hist'], maxlen=maxlen, value=0, padding='post', truncating='post'), \n",
    "                   shape=[len(val), maxlen, -1]), # 用0表示是填充的项,\n",
    "        np.array(list(val['pos_query'])), #[batch, 2]\n",
    "        np.array(list(val['neg_query'])), # [batch, 2, 2]\n",
    "        np.array(val['seq_len'].values.reshape(-1, 1)), # [batch, 1]\n",
    "         np.array(val['label'].values.reshape(-1, 1)) # [batch, 1],\n",
    "       ]\n",
    "\n",
    "test_x = [np.array(test['user_id'].values.reshape(-1, 1)), \n",
    "        tf.reshape(pad_sequences(test['hist'], maxlen=maxlen, value=0, padding='post', truncating='post'), \n",
    "                   shape=[len(test), maxlen, -1]), # 用0表示是填充的项, #[batch, maxlen, 2]\n",
    "        np.array(list(test['pos_query'])), #[batch, 2]\n",
    "        np.array(list(test['neg_query'])), # [batch, 2, 2]\n",
    "        np.array(test['seq_len'].values.reshape(-1, 1)), # [batch, 1]\n",
    "        np.array(test['label'].values.reshape(-1, 1)) # [batch, 1],\n",
    "       ]\n",
    "print('============Data Preprocess End=============')\n",
    "\n",
    "for i in test_x: print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724e5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "0a492844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 20) (None, 20) (None, 2, 20) (None, 1)\n",
      "(None, 9, 1)\n",
      "1 (None, 10, 20) (None, 10, 20) (None, 10)\n",
      "(None, 20) (None, 20) (None, 1)\n",
      "(None, 20) (None, 20) (None, 1)\n",
      "112 () (None, 20)\n",
      "111 (None, 50) ()\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " user_sparse_input (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " user_list_input (InputLayer)   [(None, 10, 2)]      0           []                               \n",
      "                                                                                                  \n",
      " pos_item_sparse_input (InputLa  [(None, 2)]         0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " neg_item_sparse_input (InputLa  [(None, 2, 2)]      0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " seq_len (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " is_pos (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dien (DIEN)                    (None, 1)            398353      ['user_sparse_input[0][0]',      \n",
      "                                                                  'user_list_input[0][0]',        \n",
      "                                                                  'pos_item_sparse_input[0][0]',  \n",
      "                                                                  'neg_item_sparse_input[0][0]',  \n",
      "                                                                  'seq_len[0][0]',                \n",
      "                                                                  'is_pos[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 398,353\n",
      "Trainable params: 397,733\n",
      "Non-trainable params: 620\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model = build_graph(sparse_dict, sparse_feat_index_list, max_len=maxlen)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "            loss=keras.losses.binary_crossentropy, \n",
    "              metrics=[keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "15a30e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 191s 170ms/step - loss: 2.3947 - precision: 0.7908 - recall: 0.8010 - auc: 0.8866 - val_loss: 2.4208 - val_precision: 0.7921 - val_recall: 0.7504 - val_auc: 0.8705\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 224s 200ms/step - loss: 2.3908 - precision: 0.7927 - recall: 0.8036 - auc: 0.8886 - val_loss: 2.4156 - val_precision: 0.7804 - val_recall: 0.7840 - val_auc: 0.8732\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 203s 181ms/step - loss: 2.3881 - precision: 0.7941 - recall: 0.8051 - auc: 0.8897 - val_loss: 2.4148 - val_precision: 0.7964 - val_recall: 0.7578 - val_auc: 0.8743\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 206s 184ms/step - loss: 2.3865 - precision: 0.7939 - recall: 0.8077 - auc: 0.8907 - val_loss: 2.4123 - val_precision: 0.7878 - val_recall: 0.7740 - val_auc: 0.8759\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 215s 192ms/step - loss: 2.3852 - precision: 0.7951 - recall: 0.8070 - auc: 0.8913 - val_loss: 2.4157 - val_precision: 0.7882 - val_recall: 0.7644 - val_auc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf458c0a30>"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到 y 的标签\n",
    "model.fit(train_x, train_y, epochs=5, batch_size=512, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "0c9c0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266/1266 [==============================] - 15s 12ms/step - loss: 2.4467 - precision: 0.7846 - recall: 0.7373 - auc: 0.8569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.446676015853882, 0.7846105098724365, 0.7372944355010986, 0.8568750619888306]"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a89630",
   "metadata": {},
   "source": [
    "# FIBINET\n",
    "## 1. SENET用于动态得到特征重要性\n",
    "## 2. 双线性用于更细粒度的特征交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "5a4f0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "import itertools\n",
    "\n",
    "\n",
    "class DNN(layers.Layer):\n",
    "    def __init__(self, units, activation='relu', dropout=0.3, reg_l2=0.01):\n",
    "        super().__init__()\n",
    "        self.layers = [layers.Dense(i, activation=activation, kernel_regularizer=keras.regularizers.l2(reg_l2))\n",
    "                      for i in units]\n",
    "        self.dropout = keras.layers.Dropout(rate=dropout)\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SENET(layers.Layer):\n",
    "    \"\"\"\n",
    "    用于计算特征重要性的网络\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction_ratio=8, activation='relu', reg_l2=0.01):\n",
    "        self.r = reduction_ratio\n",
    "        self.act = activation\n",
    "        self.reg_l2 = reg_l2\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        units = [input_shape[-1] // self.r, input_shape[1]]\n",
    "        self.layers = [keras.layers.Dense(i, activation=self.act, use_bias=False, kernel_regularizer=keras.regularizers.l2(self.reg_l2))\n",
    "                      for i in units]\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x：[None, f, k] \n",
    "        \"\"\"\n",
    "        tmp = tf.identity(x) # copy\n",
    "        # squeeze\n",
    "        x = tf.reduce_mean(x, axis=-1) # [None, f]\n",
    "        print(x.shape)\n",
    "        # excitation\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # [None f]\n",
    "        print(x.shape)\n",
    "        # re-weight\n",
    "        x = tf.expand_dims(x, axis=-1) # [None, f, 1]\n",
    "        #\n",
    "        out = tf.multiply(x, tmp) # [None, f, k]\n",
    "        return out\n",
    "\n",
    "class Binlinear(layers.Layer):\n",
    "    \"\"\"\n",
    "    该层主要实现了一个稀疏特征embedding的组合交叉功能\n",
    "    值得学习的是特征组合的实现方式，使用了组合公式\n",
    "    \"\"\"\n",
    "    def __init__(self, type_='all'):\n",
    "        assert type_ in ('all', 'each', 'interaction'), '双线性交互层的模式必须是（all, each, interaction）中的一种'\n",
    "        self.type_ = type_\n",
    "        self.concat_layer = layers.Concatenate(axis=-1, name='binlinear_concat_layer')\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        input_shape : [None, f, k]\n",
    "        \"\"\"\n",
    "        shape = list(input_shape)\n",
    "        initializer = keras.initializers.glorot_normal\n",
    "        if self.type_ == 'all':\n",
    "            self.wight1 = self.add_weight(shape=(1, shape[-1], shape[-1]),\n",
    "                                         initializer=initializer,\n",
    "                                         trainable=True) # [k, k]\n",
    "        elif self.type_ == 'each':\n",
    "            self.wight1 = [self.add_weight(shape=(shape[-1], shape[-1]),\n",
    "                                         initializer=initializer,\n",
    "                                         trainable=True)\n",
    "                          for _ in range(shape[1]-1)\n",
    "                          ]# f-1 * [k, k]\n",
    "        else:\n",
    "            self.wight1 = [self.add_weight(shape=(shape[-1], shape[-1]),\n",
    "                                         initializer=initializer,\n",
    "                                         trainable=True)\n",
    "                          for _ in itertools.combinations(range(shape[1]), 2)\n",
    "                          ] # f*(f-1)/2* [k, k]\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x: [None, f, k]\n",
    "        output: [None, f*(f-1)/2, k]\n",
    "        \"\"\"\n",
    "        # 先做内积\n",
    "        _, f, k = x.shape\n",
    "        tmp = tf.split(x, f, axis=1) # [(None, 1, k), ...]\n",
    "        tmp = [tf.squeeze(i, axis=1) for i in tmp] # [(None, k), ...]\n",
    "        if self.type_ == 'all':\n",
    "            cdots = [tf.tensordot(item, self.wight1, axes=[-1, 0]) for item in tmp] # f*[(None, k)]\n",
    "            res = [tf.multiply(\n",
    "                            cdots[i],\n",
    "                            tmp[j]) # (None, k)\n",
    "                  for i, j in itertools.combinations(f, 2) # 得到组合情况\n",
    "                  ] #f*(f-1)/2* [None, k]\n",
    "        elif self.type_ == 'each':\n",
    "            cdots = [tf.tensordot(tmp[i], self.wight1[i], axes=[-1, 0]) for i in range(f-1)] # f-1*[(None, k)]\n",
    "            res = [tf.multiply(\n",
    "                            cdots[i],\n",
    "                            tmp[j])\n",
    "                  for i, j in itertools.combinations(range(f), 2) # 得到组合情况\n",
    "                  ] #f*(f-1)/2* [None, k]\n",
    "        else:\n",
    "            cdots = [tf.tensordot(tmp[i], self.wight1[idx], axes=[-1, 0]) \n",
    "                     for idx, (i, _) in enumerate(itertools.combinations(range(f), 2))] \n",
    "            res = [tf.multiply(\n",
    "                    cdots[idx],\n",
    "                    tmp[j]\n",
    "            ) for idx, (_, j) in enumerate(itertools.combinations(range(f), 2))\n",
    "            ] #f*(f-1)/2* [None, k]\n",
    "        print('res', tmp[0].shape, cdots[0].shape, res[0].shape)\n",
    "        out = self.concat_layer(res) #[None, 1, f*(f-1)*k/2]\n",
    "        return keras.layers.Flatten()(out)\n",
    "\n",
    "    \n",
    "class FIBINET(keras.layers.Layer):\n",
    "    def __init__(self, sparse_feat_dict, feat_columns,\n",
    "                 dnn_units=(256, 126, 64), dnn_activation='relu', dnn_dropout=0.3,\n",
    "                senet_reduaction_ratio=0.3,\n",
    "                 type_='each',\n",
    "                 embed_reg=0.01,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dnn = DNN(dnn_units, dnn_activation, dnn_dropout)\n",
    "        self.senet = SENET(senet_reduaction_ratio)\n",
    "        self.org_binlinear = Binlinear(type_=type_)\n",
    "        self.senet_binlinear = Binlinear(type_=type_)\n",
    "        self.concat_layer = keras.layers.Concatenate(name='to_dnn')\n",
    "        self.last_linear = keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.bn = keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.embed_layers = {\n",
    "            k: keras.layers.Embedding(v[0], v[1], embeddings_regularizer=keras.regularizers.l2(embed_reg))\n",
    "            for k, v in sparse_feat_dict.items()\n",
    "                            }\n",
    "        self.sparse_user_index, self.sparse_items_index = feat_columns\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        dense_user, sparse_user, dense_items, sparse_items = inputs\n",
    "        \n",
    "        dense_input = tf.concat([dense_user, dense_items], axis=-1) # [None, k]\n",
    "        \n",
    "        print(sparse_user.shape, self.sparse_user_index)\n",
    "        embed_user = tf.stack(\n",
    "            [\n",
    "                self.embed_layers[k](sparse_user[:,idx])\n",
    "                for k, idx in self.sparse_user_index.items()\n",
    "            ], axis=1)\n",
    "         # [None, f_user, user_embeds]\n",
    "        embed_items = tf.stack(\n",
    "            [\n",
    "                self.embed_layers[k](sparse_items[:, idx])\n",
    "                for k,idx in self.sparse_items_index.items()\n",
    "            ], axis=1\n",
    "        ) # [None, f_items, items_embeds]\n",
    "        sparse_input = tf.concat([embed_user, embed_items], axis=1) #[None, f, embed_dims]\n",
    "        \n",
    "        out1 = self.org_binlinear(sparse_input) # [None, f*(f-1)/2*embed_dims]\n",
    "        out_senet = self.senet(sparse_input) #[None, f, embed_dims]\n",
    "        out2 = self.senet_binlinear(out_senet) # [None, f*(f-1)/2*embed_dims]\n",
    "        \n",
    "        x = self.concat_layer([dense_input, out1, out2]) # [None, tot]\n",
    "        x = self.bn(x)\n",
    "        x = self.dnn(x)\n",
    "        ctr = self.last_linear(x)\n",
    "        return ctr        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "93ec1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "# =============================== GPU ==============================\n",
    "# gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# print(gpu)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2, 3'\n",
    "# ========================= Hyper Parameters =======================\n",
    "# you can modify your file path\n",
    "file = '../data/criteo_sampled_data.csv'\n",
    "read_part = True\n",
    "sample_num = 100000\n",
    "embed_dim = 10\n",
    "test_size = 0.2\n",
    "df = pd.read_csv(file, nrows=4)\n",
    "df.head()\n",
    "\n",
    "# ========================== Create dataset =======================\n",
    "feature_columns, train, test = create_criteo_dataset(\n",
    "                                    embed_dim=embed_dim, file=file,\n",
    "                                       read_part=read_part,\n",
    "                                       sample_num=sample_num,\n",
    "                                       test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "88568496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feat_dict = {\n",
    "    item['feat']: (item['feat_num'], item['embed_dim'])\n",
    "    for item in feature_columns[1]\n",
    "    if item['feat'].startswith('C')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "id": "4fbed246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 13, 7, 13]\n",
      "[6, 13, 7, 13]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(data):\n",
    "    (dense, sparse), label = data\n",
    "    # 模拟数据\n",
    "    thr = 0.5\n",
    "    length_dense, length_sparse = dense.shape[1], sparse.shape[1]\n",
    "    idx_dense, idx_sparse = int(length_dense*thr), int(length_sparse*thr)\n",
    "    dense_user, dense_items = dense[:, :idx_dense], dense[:,idx_dense:]\n",
    "    sparse_user, sparse_items = sparse[:,:idx_sparse], sparse[:,idx_sparse:]\n",
    "    inputs = [dense_user, sparse_user, dense_items, sparse_items]\n",
    "    print([i.shape[1] for i in inputs])\n",
    "    sparse_user_index = {f'C{i+1}': i for i in range(idx_sparse)}\n",
    "    sparse_items_index = {f'C{i+1}': i-idx_sparse for i in range(idx_sparse, length_sparse)}\n",
    "\n",
    "    return (inputs, label), (sparse_user_index, sparse_items_index)\n",
    "\n",
    "(train_x, train_y), feature_column_list = get_dataset(train)\n",
    "(test_x, test_y), _ = get_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cadd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "9637c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 13) {'C1': 0, 'C2': 1, 'C3': 2, 'C4': 3, 'C5': 4, 'C6': 5, 'C7': 6, 'C8': 7, 'C9': 8, 'C10': 9, 'C11': 10, 'C12': 11, 'C13': 12}\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "(None, 26)\n",
      "(None, 26)\n",
      "res (None, 10) (None, 10) (None, 10)\n"
     ]
    }
   ],
   "source": [
    "dense_user_dims = 6\n",
    "sparse_user_dims = 13\n",
    "dense_items_dims = 7\n",
    "sparse_items_dims = 13\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    fibinet = FIBINET(sparse_feat_dict, feature_column_list, type_='interaction')\n",
    "    dense_user_input = keras.Input(shape=(dense_user_dims,), dtype=tf.float32, name='dense_user')\n",
    "    sparse_user_input = keras.Input(shape=(sparse_user_dims,), dtype=tf.float32, name='sparse_user')\n",
    "    dense_item_input = keras.Input(shape=(dense_items_dims,), dtype=tf.float32, name='dense_items')\n",
    "    sparse_items_input = keras.Input(shape=(sparse_items_dims,), dtype=tf.float32, name='sparse_items')\n",
    "    inputs = [dense_user_input, sparse_user_input, dense_item_input, sparse_items_input]\n",
    "    outputs = fibinet(inputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "             metrics=[keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "b0fe6399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " dense_user (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " sparse_user (InputLayer)       [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_items (InputLayer)       [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " sparse_items (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " fibinet (FIBINET)              (None, 1)            12944551    ['dense_user[0][0]',             \n",
      "                                                                  'sparse_user[0][0]',            \n",
      "                                                                  'dense_items[0][0]',            \n",
      "                                                                  'sparse_items[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,944,551\n",
      "Trainable params: 12,905,525\n",
      "Non-trainable params: 39,026\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "2ddf0c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "(None, 13) {'C1': 0, 'C2': 1, 'C3': 2, 'C4': 3, 'C5': 4, 'C6': 5, 'C7': 6, 'C8': 7, 'C9': 8, 'C10': 9, 'C11': 10, 'C12': 11, 'C13': 12}\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "(None, 26)\n",
      "(None, 26)\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "(None, 13) {'C1': 0, 'C2': 1, 'C3': 2, 'C4': 3, 'C5': 4, 'C6': 5, 'C7': 6, 'C8': 7, 'C9': 8, 'C10': 9, 'C11': 10, 'C12': 11, 'C13': 12}\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "(None, 26)\n",
      "(None, 26)\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.0958 - auc: 0.6514(None, 13) {'C1': 0, 'C2': 1, 'C3': 2, 'C4': 3, 'C5': 4, 'C6': 5, 'C7': 6, 'C8': 7, 'C9': 8, 'C10': 9, 'C11': 10, 'C12': 11, 'C13': 12}\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "(None, 26)\n",
      "(None, 26)\n",
      "res (None, 10) (None, 10) (None, 10)\n",
      "313/313 [==============================] - 154s 344ms/step - loss: 2.0958 - auc: 0.6514 - val_loss: 0.5831 - val_auc: 0.7112\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 86s 274ms/step - loss: 0.5392 - auc: 0.7092 - val_loss: 0.5395 - val_auc: 0.7239\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 85s 270ms/step - loss: 0.5326 - auc: 0.7242 - val_loss: 0.5280 - val_auc: 0.7323\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 97s 310ms/step - loss: 0.5298 - auc: 0.7347 - val_loss: 0.5279 - val_auc: 0.7339\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 83s 265ms/step - loss: 0.5287 - auc: 0.7440 - val_loss: 0.5262 - val_auc: 0.7405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bfb2f0f370>"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=5, batch_size=256, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "7782271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " dense_user (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " sparse_user (InputLayer)       [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_items (InputLayer)       [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " sparse_items (InputLayer)      [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " fibinet (FIBINET)              (None, 1)            4214307     ['dense_user[0][0]',             \n",
      "                                                                  'sparse_user[0][0]',            \n",
      "                                                                  'dense_items[0][0]',            \n",
      "                                                                  'sparse_items[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,214,307\n",
      "Trainable params: 4,201,281\n",
      "Non-trainable params: 13,026\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce3130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
